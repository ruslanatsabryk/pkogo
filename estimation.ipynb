{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75541273-d8a0-40fe-b676-e024f18f6b41",
   "metadata": {},
   "source": [
    "#### Импорт необходимых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d31a67-e1f4-4f73-b386-b54e9da7cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.interpolate import splrep, splev\n",
    "import pickle\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import Lars, LarsCV\n",
    "from sklearn.linear_model import LassoLars, LassoLarsCV\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, LassoLarsCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c8264-7ddd-469d-9581-8eefb3f13322",
   "metadata": {},
   "source": [
    "#### Функция чтения набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cff566a-8886-47b1-a651-f3df2c864e39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_river_dataset(fname, pr_list=None, y_name='H_max'):\n",
    "    pr_arr = []\n",
    "    y_arr = []\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            pr_arr_row = []\n",
    "            for pr in pr_list:\n",
    "                pr_arr_row.append(row[pr])\n",
    "\n",
    "            pr_arr.append(pr_arr_row)\n",
    "            y_arr.append(row[y_name])\n",
    "    X = np.asarray(pr_arr, dtype=np.float64)\n",
    "    y = np.asarray(y_arr, dtype=np.float64)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a96a1-c37d-47ba-b369-2a015ec5706f",
   "metadata": {},
   "source": [
    "#### Сумма, средний, высший, низший уровни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa78832-4724-42e1-8e4f-8513406d88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(h_max):\n",
    "    return np.sum(h_max)\n",
    "    \n",
    "def get_avg(h_max):\n",
    "    return np.mean(h_max)\n",
    "    \n",
    "def get_max(h_max):\n",
    "    return np.amax(h_max)\n",
    "    \n",
    "def get_min(h_max):\n",
    "    return np.amin(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dafa4-e1f7-4c26-b28d-3dfd6a2b1b14",
   "metadata": {},
   "source": [
    "#### Среднеквадратическая погрешность прогноза S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad85fc4-c2cd-4a9f-8a0a-bb4fe4cdfe0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_s(h_max, h_forecast):\n",
    "    # Среднеквадратическая погрешность прогноза\n",
    "    n = h_max.shape[0]\n",
    "    sqr_diff = np.sum((h_max - h_forecast) ** 2) / (n - 1)\n",
    "    std = sqr_diff ** 0.5\n",
    "    return std    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e98c4-4841-4b3f-b61b-284bc99af53d",
   "metadata": {},
   "source": [
    "#### Среднеквадратическое отклонение sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5079b8f9-3ae5-47a1-a6f3-214bdf69ad77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_sigma(h_max):\n",
    "    # Среднеквадратическая погрешность климатическая.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.std(h_max, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ebdd8-9291-4612-9573-b5e6464fc07e",
   "metadata": {},
   "source": [
    "#### Среднее значение максимальных уровней воды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fe5ef3-810d-40e8-87f0-9432f139319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hmax_avg(h_max):\n",
    "    # Среднее значение h_max.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.mean(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10fa8d-d7cd-44f4-b4f2-4f652409383f",
   "metadata": {},
   "source": [
    "#### Допустимая погрешность прогноза delta_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d4648d-8e14-4c63-bcf8-6a3c2cb7a829",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_delta_dop(sigma):\n",
    "    return 0.674 * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1c7ff-c4e1-4639-a877-d3b78d944dea",
   "metadata": {},
   "source": [
    "#### Критерий критерий применимости и качества методики S/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92739a9-3b12-44bd-b51e-4ffd73e828a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_criterion(s, sigma):\n",
    "    return s / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec5970-ba12-4121-8403-cd13b812b2de",
   "metadata": {},
   "source": [
    "#### Климатическая обеспеченность Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e1b890-8fb7-4fdd-8196-610544acee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pk(h_max, h_max_avg, delta_dop):\n",
    "    diff = np.abs(h_max - h_max_avg) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4c686-2946-4ea0-980a-172b2ff4dd64",
   "metadata": {},
   "source": [
    "#### Обеспеченность метода (оправдываемость) Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ca5adf-fbf3-49a4-8458-06d103bfacdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_pm(h_max, h_forecast, delta_dop):\n",
    "    diff = np.abs(h_max - h_forecast) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d93f0-e243-4107-b007-cb24efbdc6f6",
   "metadata": {},
   "source": [
    "#### Корреляционное отношение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a501afb2-3736-4023-a488-a19747e54910",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_correlation_ratio(criterion):\n",
    "    c_1 = (1 - criterion ** 2)\n",
    "    ro = c_1 ** 0.5 if c_1 > 0 else 0\n",
    "    return ro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059ec96-bf22-4b2b-83f9-dc2112e0c942",
   "metadata": {},
   "source": [
    "#### Вероятная ошибка прогноза S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e88b65-02b2-4bee-9262-b84a7a1ea70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_error(s):\n",
    "    return 0.674 * s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0abe2-cca4-4ebc-b6bf-f27c01c6b1bd",
   "metadata": {},
   "source": [
    "#### Ошибки климатического/природного прогноза для каждого года delta50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b00b24-686d-449b-9920-aa4733a0bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta50(h_max, delta_dop, h_max_avg=None, h_max_forecast=None):\n",
    "    if h_max_forecast is None:\n",
    "        # delta50 климатическая\n",
    "        return (h_max - h_max_avg) / delta_dop\n",
    "    else:\n",
    "        # delta50 прогноза\n",
    "        return (h_max - h_max_forecast) / delta_dop\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0cc72-ec19-437b-b6c5-8010c2e3a5bf",
   "metadata": {},
   "source": [
    "#### Функция записи списка моделей с их характеристиками в csv файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c76ad0-d75c-4746-90dc-0d8465218463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset_csv(year, dataset, dataset_name, fieldnames, pr_group, mode='training'):\n",
    "    if mode == 'estimation':\n",
    "        dir_path = f'results/Estimation/{year}/{dataset_name}/group-{pr_group}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Оценка.csv'\n",
    "    elif mode == 'training':\n",
    "        dir_path = f'results/Models/{year}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Обучение.csv'\n",
    "    elif mode == 'forecast':\n",
    "        dir_path = f'results/Forecast/{year}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Прогноз.csv'\n",
    "    else:\n",
    "        ...\n",
    "    \n",
    "    with open(\n",
    "        f'{dir_path}'\n",
    "        f'{file_name}', \n",
    "        'w', newline='', encoding='utf-8'\n",
    "    ) as csvfile:\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, \n",
    "                                delimiter=';', extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72dc311-4aa6-4775-bd80-e4182ba30c96",
   "metadata": {},
   "source": [
    "#### Функция записи результатов экспериментов в csv файл для процесса выбора моделей и параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f282d3fa-80e7-479a-94c7-c24f2d2d5b5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def write_experiment_csv(year, dataset, fieldnames, filename):\n",
    "    fpath = f'results/Estimation/{year}/{filename}.csv'\n",
    "    with open(fpath, 'a', newline='') as csvfile: #, encoding='cp1251'\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, \n",
    "                                delimiter=';', extrasaction='ignore')\n",
    "        fsize = os.stat(fpath).st_size\n",
    "        if not fsize:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2d2a0-f9da-46e5-a0ae-45e8fe0c2482",
   "metadata": {},
   "source": [
    "#### Функция разделения набора данных на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82db9e50-fadc-4c7f-9151-58c2b74df8c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y, n_test, split=True):\n",
    "    if split:   \n",
    "        X_train = X[:-n_test].copy()\n",
    "        y_train = y[:-n_test].copy()\n",
    "        X_test = X[-n_test:].copy()\n",
    "        y_test = y[-n_test:].copy()\n",
    "    else:\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "        X_test = X.copy()\n",
    "        y_test = y.copy()\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4df04-1723-4b33-b5e6-65008696e7e9",
   "metadata": {},
   "source": [
    "#### Функция перемешивания данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f3cd404-1a3a-4938-a78d-224bb5328f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_xy(X, y, shuffle=True):\n",
    "    if shuffle:\n",
    "        # Перемешивание данных\n",
    "        Xy = np.column_stack((X, y))\n",
    "        rng = np.random.default_rng(42)\n",
    "        rng.shuffle(Xy)\n",
    "        y_sh = Xy[:, -1]\n",
    "        X_sh = Xy[:,:-1]\n",
    "    else:\n",
    "        y_sh = y.copy()\n",
    "        X_sh = X.copy()\n",
    "    return X_sh, y_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719a5f9-176e-473d-8ce7-b7744d9c6e19",
   "metadata": {},
   "source": [
    "#### Функция формирования тестового набора данных с подстановкой нормированных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b464d45-843c-4792-ab02-2819edddee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_norm(x, pr_list, norms):\n",
    "    x_norm = np.copy(x)\n",
    "    for col, pr in enumerate(pr_list):\n",
    "        if pr in norms:\n",
    "            x_norm[:, col:col+1] = norms[pr]\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3a942-95d0-4f7f-aa39-82f61624b070",
   "metadata": {},
   "source": [
    "#### Функция получения датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7628a977-9d54-4129-928c-625b4e80d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    datasets = {\n",
    "        # 'Неман-Белица': 'Неман',\n",
    "        # 'Неман-Гродно': 'Неман',\n",
    "        # 'Неман-Мосты': 'Неман',\n",
    "        # 'Неман-Столбцы': 'Неман',\n",
    "\n",
    "        # 'Вилия-Стешицы': 'Вилия',\n",
    "        # 'Вилия-Михалишки': 'Вилия',\n",
    "        'ЗападнаяДвина-Сураж': 'ЗападнаяДвина-А',\n",
    "        'ЗападнаяДвина-Верхнедвинск': 'ЗападнаяДвина-А',\n",
    "        'ЗападнаяДвина-Витебск': 'ЗападнаяДвина-Б',\n",
    "        'ЗападнаяДвина-Полоцк': 'ЗападнаяДвина-Б',\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83168b0-64e2-4e58-a47b-f49f677a9eb4",
   "metadata": {},
   "source": [
    "#### Функция получения списка предикторов по названию датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bca1eea5-a699-4b0e-91fa-9fe6f4451ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictors(dataset_name, group=None):\n",
    "\n",
    "    datasets = get_datasets()   \n",
    "    predictors_lists = {\n",
    "        'Неман': (\n",
    "            ['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'X3', 'Xs'],\n",
    "            ['Smax', 'H_2802', 'X', 'X1', 'X3'],\n",
    "            ['S_2802', 'H_2802', 'X2', 'X3', 'Xs'],\n",
    "        ),\n",
    "        'Вилия': (\n",
    "            ['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'X3', 'Xs', 'L_max', 'L_2802', 'Q12', 'Q01', 'Q02', 'Y_sum'],\n",
    "            ['Smax', 'H_2802', 'X', 'X1', 'X3', 'L_max', 'Y_sum'],\n",
    "            ['S_2802', 'H_2802', 'X2', 'X3', 'Xs', 'L_2802', 'Y_sum'],\n",
    "        ),\n",
    "        'ЗападнаяДвина-А': (\n",
    "            ['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'Xs'],\n",
    "            ['Smax', 'H_2802', 'X', 'X1'],\n",
    "            ['S_2802', 'H_2802', 'X2', 'Xs'],\n",
    "        ),\n",
    "        'ЗападнаяДвина-Б': (\n",
    "            ['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'Xs', 'Q12', 'Q01', 'Q02', 'Y_sum'],\n",
    "            ['Smax', 'H_2802', 'X', 'X1', 'Y_sum'],\n",
    "            ['S_2802', 'H_2802', 'X2', 'Xs', 'Y_sum'],\n",
    "        ),\n",
    "    }\n",
    "    result = predictors_lists[datasets[dataset_name]] if group is None else \\\n",
    "             predictors_lists[datasets[dataset_name]][group]\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790ff38-4843-40b6-ac40-a9efec555315",
   "metadata": {},
   "source": [
    "#### Функция получения нормированных значений предикторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a741f9dd-31c9-4ea6-a1a8-b2b684b05013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norms(dataset_name):\n",
    "    norms_list = {\n",
    "        'Неман-Белица': {'S_max': 59.89, 'X':96.16, 'X1': 46.0, 'X2':35.0},\n",
    "        'Неман-Гродно': {'S_max': 51.70, 'X':80.27, 'X1': 36.0, 'X2':26.0},\n",
    "        'Неман-Мосты': {'S_max': 53.62, 'X':88.51, 'X1': 40.0, 'X2':31.0},\n",
    "        'Неман-Столбцы': {'S_max': 73.68, 'X':101.68, 'X1': 43.0, 'X2':34.0},\n",
    "\n",
    "        'Вилия-Стешицы': {'S_max': 67.0, 'X': 112.0, 'X1': 40.0, 'X2': 33.0, 'L_max': 60.0},\n",
    "        'Вилия-Михалишки': {'S_max': 60.0, 'X': 116.0, 'X1': 46.0, 'X2': 37.0, 'L_max': 57.0},\n",
    "\n",
    "        'ЗападнаяДвина-Сураж': {'S_max': 89.0, 'X':126.0, 'X1': 50.0, 'X2':43.0},\n",
    "        'ЗападнаяДвина-Верхнедвинск': {'S_max': 62.0, 'X':122.0, 'X1': 68.0, 'X2':56.0},\n",
    "        'ЗападнаяДвина-Витебск': {'S_max': 80.0, 'X': 134.0, 'X1': 65.0, 'X2': 53.0, 'Y_sum': 46.4},\n",
    "        'ЗападнаяДвина-Полоцк': {'S_max': 66.0, 'X': 122.0, 'X1': 61.0, 'X2': 53.0, 'Y_sum': 46.5},\n",
    "    }\n",
    "    return norms_list[dataset_name]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83f925-d3c5-46ea-bbba-cb3cf9605113",
   "metadata": {},
   "source": [
    "#### Функция получения аугментированных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc8e198a-5d6a-4815-bbbc-94ae2d2c11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(x_data, y_data, aug_n, aug_pow=2, mirror=True, s=None):\n",
    "    #print(x_data)\n",
    "    data_len = len(y_data)\n",
    "    \n",
    "    x_points = np.linspace(0, data_len, data_len)\n",
    "    \n",
    "    x_splitted = np.hsplit(x_data, x_data.shape[1])\n",
    "    #print(x_splitted)\n",
    "\n",
    "    x_list = []\n",
    "    for arr in x_splitted:\n",
    "        x_spl = splrep(x_points, arr, k=aug_pow, s=s)\n",
    "        x_points_n = np.linspace(0, data_len, aug_n)\n",
    "        x_col_augmented = splev(x_points_n, x_spl)\n",
    "        x_list.append(x_col_augmented)\n",
    "    x_augmented = np.array(x_list).T\n",
    "\n",
    "    y_points = np.linspace(0, data_len, data_len)\n",
    "    y_spl = splrep(y_points, y_data, k=aug_pow, s=s)\n",
    "    y_points_n = np.linspace(0, data_len, aug_n)\n",
    "    y_augmented = splev(y_points_n, y_spl)\n",
    "        \n",
    "    # # --------- Умножение на коэфф, без mirror!\n",
    "    # x_augmented_dif = (x_augmented - np.mean(x_augmented)) * 0.25\n",
    "    # y_augmented_dif = (y_augmented - np.mean(y_augmented)) * 0.25\n",
    "    \n",
    "    # x_augmented_up = x_augmented + x_augmented_dif\n",
    "    # x_augmented_dn = x_augmented - x_augmented_dif\n",
    "    # y_augmented_up = y_augmented + y_augmented_dif\n",
    "    # y_augmented_dn = y_augmented - y_augmented_dif\n",
    "\n",
    "    # x_augmented = np.vstack((x_augmented_up, x_augmented_dn))\n",
    "    # y_augmented = np.hstack((y_augmented_up, y_augmented_dn))\n",
    "\n",
    "    # plt.plot(y_points, y_data, 'o', y_points_n, y_augmented_up, y_points_n, y_augmented_dn)\n",
    "    # plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_augmented_up[:, 0], x_points_n, x_augmented_dn[:, 0])\n",
    "    # # -----------\n",
    "    \n",
    "    x_aug_round = np.round(x_augmented, decimals=-1)\n",
    "    y_aug_round = np.round(y_augmented, decimals=1)\n",
    "\n",
    "    x_data_round = np.round(x_data, decimals=-1)\n",
    "    y_data_round = np.round(y_data, decimals=1)   \n",
    "\n",
    "    # x_aug_round = np.trunc(x_augmented)\n",
    "    # y_aug_round = np.trunc(y_augmented)\n",
    "\n",
    "    # x_data_round = np.trunc(x_data)\n",
    "    # y_data_round = np.trunc(y_data)   \n",
    "\n",
    "    \n",
    "    \n",
    "    mx = (x_aug_round[:, None] == x_data_round).all(-1).any(1)\n",
    "    x_aug_clear = x_augmented[~mx].copy()\n",
    "    y_aug_clear = y_augmented[~mx].copy()\n",
    "    points_aug_clear = y_points_n[~mx]\n",
    "    \n",
    "    # my = np.in1d(y_aug_round, y_data_round)\n",
    "    # x_aug_clear = x_aug_clear[~my]\n",
    "    # y_aug_clear = y_aug_clear[~my]\n",
    "    \n",
    "    print('x_aug_clear.shape', x_aug_clear.shape)\n",
    "    print('x_augmented.shape', x_augmented.shape)\n",
    "\n",
    "    if mirror:\n",
    "        x_mirror = np.mean(x_augmented) - x_augmented + np.mean(x_augmented)\n",
    "        y_mirror = np.mean(y_augmented) - y_augmented + np.mean(y_augmented)\n",
    "    \n",
    "        x_result = np.vstack((x_aug_clear, x_mirror))\n",
    "        y_result = np.hstack((y_aug_clear, y_mirror))\n",
    "\n",
    "    else:\n",
    "        x_result = x_aug_clear\n",
    "        y_result = y_aug_clear\n",
    "    \n",
    "    if mirror:\n",
    "        ...\n",
    "        plt.plot(y_points, y_data, 'o', y_points_n, y_mirror)\n",
    "        plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_mirror[:, 0])\n",
    "\n",
    "    # plt.plot(y_points, y_data, 'o', y_points_n, y_augmented)\n",
    "    # plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_augmented[:, 0])       \n",
    "    # plt.show()   \n",
    "    \n",
    "    # points_aug_clear = np.linspace(0, y_points, len(y_aug_clear))\n",
    "   \n",
    "    plt.plot(y_points, y_data, 'o', points_aug_clear, y_aug_clear)\n",
    "    plt.plot(x_points, x_data[:, 0], 'x', points_aug_clear, x_aug_clear[:, 0])\n",
    "    plt.show()\n",
    "\n",
    "    x_result[x_result < 0] = 0\n",
    "    y_result[y_result < 0] = 0\n",
    "    \n",
    "    return x_result, y_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ea47511-f2ef-4795-b8eb-db922c11f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_2(x_data, y_data, aug_mpl, aug_pow=2, mirror=True, s=None):\n",
    "    #print(x_data)\n",
    "    data_len = len(y_data)\n",
    "    \n",
    "    x_points = np.linspace(0, data_len, data_len)\n",
    "    \n",
    "    x_splitted = np.hsplit(x_data, x_data.shape[1])\n",
    "    #print(x_splitted)\n",
    "\n",
    "    aug_n = round(data_len * (aug_mpl - 1) / (data_len - 1)) * (data_len - 1) + data_len\n",
    "    \n",
    "    x_list = []\n",
    "    for arr in x_splitted:\n",
    "        x_spl = splrep(x_points, arr, k=aug_pow, s=s)\n",
    "        x_points_n = np.linspace(0, data_len, aug_n)\n",
    "        x_col_augmented = splev(x_points_n, x_spl)\n",
    "        x_list.append(x_col_augmented)\n",
    "    x_augmented = np.array(x_list).T\n",
    "\n",
    "    y_points = np.linspace(0, data_len, data_len)\n",
    "    y_spl = splrep(y_points, y_data, k=aug_pow, s=s)\n",
    "    y_points_n = np.linspace(0, data_len, aug_n)\n",
    "    y_augmented = splev(y_points_n, y_spl)\n",
    "        \n",
    "    # # --------- Умножение на коэфф, без mirror!\n",
    "    # x_augmented_dif = (x_augmented - np.mean(x_augmented)) * 0.25\n",
    "    # y_augmented_dif = (y_augmented - np.mean(y_augmented)) * 0.25\n",
    "    \n",
    "    # x_augmented_up = x_augmented + x_augmented_dif\n",
    "    # x_augmented_dn = x_augmented - x_augmented_dif\n",
    "    # y_augmented_up = y_augmented + y_augmented_dif\n",
    "    # y_augmented_dn = y_augmented - y_augmented_dif\n",
    "\n",
    "    # x_augmented = np.vstack((x_augmented_up, x_augmented_dn))\n",
    "    # y_augmented = np.hstack((y_augmented_up, y_augmented_dn))\n",
    "\n",
    "    # plt.plot(y_points, y_data, 'o', y_points_n, y_augmented_up, y_points_n, y_augmented_dn)\n",
    "    # plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_augmented_up[:, 0], x_points_n, x_augmented_dn[:, 0])\n",
    "    # # -----------\n",
    "    \n",
    "    x_aug_round = np.round(x_augmented, decimals=-1)\n",
    "    y_aug_round = np.round(y_augmented, decimals=1)\n",
    "\n",
    "    x_data_round = np.round(x_data, decimals=-1)\n",
    "    y_data_round = np.round(y_data, decimals=1)   \n",
    "\n",
    "    # x_aug_round = np.trunc(x_augmented)\n",
    "    # y_aug_round = np.trunc(y_augmented)\n",
    "\n",
    "    # x_data_round = np.trunc(x_data)\n",
    "    # y_data_round = np.trunc(y_data)   \n",
    "\n",
    "    \n",
    "    \n",
    "    mx = (x_aug_round[:, None] == x_data_round).all(-1).any(1)\n",
    "    x_aug_clear = x_augmented[~mx].copy()\n",
    "    y_aug_clear = y_augmented[~mx].copy()\n",
    "    points_aug_clear = y_points_n[~mx]\n",
    "    \n",
    "    # my = np.in1d(y_aug_round, y_data_round)\n",
    "    # x_aug_clear = x_aug_clear[~my]\n",
    "    # y_aug_clear = y_aug_clear[~my]\n",
    "    \n",
    "    print('x_aug_clear.shape', x_aug_clear.shape)\n",
    "    print('x_augmented.shape', x_augmented.shape)\n",
    "\n",
    "    if mirror:\n",
    "        x_mirror = np.mean(x_augmented) - x_augmented + np.mean(x_augmented)\n",
    "        y_mirror = np.mean(y_augmented) - y_augmented + np.mean(y_augmented)\n",
    "    \n",
    "        x_result = np.vstack((x_aug_clear, x_mirror))\n",
    "        y_result = np.hstack((y_aug_clear, y_mirror))\n",
    "\n",
    "    else:\n",
    "        x_result = x_aug_clear\n",
    "        y_result = y_aug_clear\n",
    "    \n",
    "    if mirror:\n",
    "        ...\n",
    "        plt.plot(y_points, y_data, 'o', y_points_n, y_mirror)\n",
    "        plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_mirror[:, 0])\n",
    "\n",
    "    # plt.plot(y_points, y_data, 'o', y_points_n, y_augmented)\n",
    "    # plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_augmented[:, 0])       \n",
    "    # plt.show()   \n",
    "    \n",
    "    # points_aug_clear = np.linspace(0, y_points, len(y_aug_clear))\n",
    "   \n",
    "    plt.plot(y_points, y_data, 'o', points_aug_clear, y_aug_clear)\n",
    "    plt.plot(x_points, x_data[:, 0], 'x', points_aug_clear, x_aug_clear[:, 0])\n",
    "    plt.show()\n",
    "\n",
    "    x_result[x_result < 0] = 0\n",
    "    y_result[y_result < 0] = 0\n",
    "    \n",
    "    return x_result, y_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038d244-bc1b-405e-af41-14f21d5cddee",
   "metadata": {},
   "source": [
    "#### Функция получения трансформеров входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "108a4350-76eb-4d0e-bbec-3ca892851cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer(transformer, n_samples=10_000):\n",
    "    scaler = (\n",
    "        StandardScaler() if transformer == 'standard' else \\\n",
    "        MinMaxScaler() if transformer == 'minmax' else \\\n",
    "        MaxAbsScaler() if transformer == 'maxabs' else \\\n",
    "        RobustScaler() if transformer == 'robust' else \\\n",
    "        QuantileTransformer(output_distribution='uniform', n_quantiles=min(10_000, n_samples), random_state=0) if transformer == 'uniform' else \\\n",
    "        QuantileTransformer(output_distribution='normal', n_quantiles=min(10_000, n_samples), random_state=0) if transformer == 'normal' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=False) if transformer == 'normal-bc' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=False) if transformer == 'normal-yj' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=True) if transformer == 'normal-bc-st' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=True) if transformer == 'normal-yj-st' else \\\n",
    "        None\n",
    "    )\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c879a5e-a136-4264-a975-1d8081a80250",
   "metadata": {},
   "source": [
    "#### Функция получения списка моделей регрессоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e12029aa-6232-4e5a-8017-2ed96fcf2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressors_list():\n",
    "    names = [\n",
    "        'LinearRegression',\n",
    "        \n",
    "        # 'Ridge',\n",
    "        'RidgeCV',\n",
    "        \n",
    "        'ElasticNetCV',\n",
    "        \n",
    "        'LassoCV',\n",
    "\n",
    "        'LarsCV',\n",
    "        \n",
    "        'Lars1',\n",
    "        'Lars2',\n",
    "        'Lars3',\n",
    "        'Lars4',\n",
    "        'Lars5',\n",
    "        'Lars6',\n",
    "        'Lars7',\n",
    "        'Lars8',\n",
    "        'Lars9',\n",
    "        'Lars10',\n",
    "        'Lars11',\n",
    "        'Lars12',\n",
    "        'Lars13',\n",
    "        'Lars14',\n",
    "\n",
    "        'LassoLarsCV',\n",
    "        \n",
    "        'OMPCV',\n",
    "        \n",
    "        'OMP1',\n",
    "        'OMP2',\n",
    "        'OMP3',\n",
    "        'OMP4',\n",
    "        'OMP5',\n",
    "        'OMP6',\n",
    "        'OMP7',\n",
    "        'OMP8',\n",
    "        'OMP9',\n",
    "        'OMP10',\n",
    "        'OMP11',\n",
    "        'OMP12',\n",
    "        'OMP13',\n",
    "        'OMP14',\n",
    "                \n",
    "        'BayesianRidge',\n",
    "        # 'BayesianRidgeCV',\n",
    "        ## 'ARDRegression',\n",
    "        # 'ARDRegressionCV',\n",
    "        # 'SGDRegressor', \n",
    "        # 'PassiveAggressiveRegressor',\n",
    "        # # 'HuberRegressor',\n",
    "        # 'HuberRegressorCV',\n",
    "        ## 'TheilSenRegressor',\n",
    "        # # 'TheilSenRegressorCV',\n",
    "        'QuantileRegressor',\n",
    "        # # 'QuantileRegressorCV',\n",
    "        \n",
    "        \n",
    "        'KNeighborsRegressor',\n",
    "        # 'NuSVR',\n",
    "        # 'SVR',\n",
    "        # 'MLPRegressor',\n",
    "        \n",
    "        'RandomForestRegressor',\n",
    "        'ExtraTreesRegressor',\n",
    "        'HistGradientBoostingRegressor',\n",
    "        'BaggingRegressor',\n",
    "        'VotingRegressor',\n",
    "        'StackingRegressorRidge',\n",
    "        'AdaBoostRegressor',\n",
    "    ]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb31417-4c5b-4efe-8b29-8e7ae8008cd7",
   "metadata": {},
   "source": [
    "#### Функция получения объектов моделей регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1ec8c3b-4b03-4044-84f9-15426ee44e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressors_objects():\n",
    "    # Инициализация генератора случайных чисел для\n",
    "    # для обеспечения воспроизводимости результатов\n",
    "    rng = np.random.RandomState(0)\n",
    "\n",
    "    # Наборы гиперпараметров моделей для алгоритма кроссвалидации\n",
    "    # Гиперпараметры для Ridge, Lasso, ElasticNet, LassoLars, HuberRegressor\n",
    "    alphas = np.logspace(-4, 3, num=100)\n",
    "    \n",
    "    # Гиперпараметры для ElasticNet\n",
    "    l1_ratio = np.linspace(0.01, 1.0, num=50)\n",
    "    \n",
    "    # Гиперпараметры для BayesianRidge\n",
    "    alphas_init = np.linspace(0.5, 2, 5)\n",
    "    lambdas_init = np.logspace(-3, 1, num=5)\n",
    "    \n",
    "    # Гиперпараметры для ARDRegression\n",
    "    alphas_lambdas = np.logspace(-7, -4, num=4)\n",
    "    \n",
    "    # Гиперпараметры для SGDRegressor\n",
    "    losses = ['squared_error', 'huber', \n",
    "              'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "    sgd_alphas = np.logspace(-4, 1, num=100)\n",
    "   \n",
    "    # Гиперпараметры для PassiveAggressiveRegressor\n",
    "    cc = np.linspace(0.1, 1.5, 50)\n",
    "    \n",
    "    # Гиперпараметры для HuberRegressor\n",
    "    epsilons = np.append(np.linspace(1.1, 2.0, 10), [1.35])\n",
    "    \n",
    "    # Гиперпараметры для TheilSenRegressor\n",
    "    # n_subsamples = np.arange(15, 24)\n",
    "    n_subsamples = (16, 24, 32)\n",
    "    \n",
    "    # Гиперпараметры для QuantileRegressor\n",
    "    # q_alphas = np.linspace(0, 1, 5)\n",
    "    q_alphas = (0.1, 1, 2)    \n",
    "    \n",
    "    regressors = [\n",
    "        LinearRegression(),\n",
    "       \n",
    "        # Ridge(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=Ridge(random_state=0), \n",
    "        #     param_grid={\"alpha\": alphas}\n",
    "        # ),\n",
    "        RidgeCV(),\n",
    "\n",
    "        ElasticNetCV(random_state=0),\n",
    "        \n",
    "        LassoCV(max_iter=10000, n_alphas=300, random_state=0),  \n",
    "        \n",
    "        LarsCV(),\n",
    "        \n",
    "        Lars(n_nonzero_coefs=1, random_state=0),\n",
    "        Lars(n_nonzero_coefs=2, random_state=0),\n",
    "        Lars(n_nonzero_coefs=3, random_state=0),\n",
    "        Lars(n_nonzero_coefs=4, random_state=0),\n",
    "        Lars(n_nonzero_coefs=5, random_state=0),\n",
    "        Lars(n_nonzero_coefs=6, random_state=0),\n",
    "        Lars(n_nonzero_coefs=7, random_state=0),\n",
    "        Lars(n_nonzero_coefs=8, random_state=0),\n",
    "        Lars(n_nonzero_coefs=9, random_state=0),\n",
    "        Lars(n_nonzero_coefs=10, random_state=0),\n",
    "        Lars(n_nonzero_coefs=11, random_state=0),\n",
    "        Lars(n_nonzero_coefs=12, random_state=0),\n",
    "        Lars(n_nonzero_coefs=13, random_state=0),\n",
    "        Lars(n_nonzero_coefs=14, random_state=0),\n",
    "\n",
    "        LassoLarsCV(max_iter=500, max_n_alphas=1000),\n",
    "\n",
    "        OrthogonalMatchingPursuitCV(n_jobs=-1),\n",
    "        \n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=1),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=2),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=3),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=4),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=5),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=6),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=7),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=8),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=9),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=10),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=11),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=12),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=13),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=14),\n",
    "        \n",
    "        BayesianRidge(),\n",
    "        # BayesianRidge() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=BayesianRidge(),\n",
    "        #     param_grid={\"alpha_init\": alphas_init, \"lambda_init\": lambdas_init}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # ARDRegression(),\n",
    "        # ARDRegression() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=ARDRegression(), \n",
    "        #     param_grid={\"alpha_1\": alphas_lambdas, \"alpha_2\": alphas_lambdas,\n",
    "        #                 \"lambda_1\": alphas_lambdas,\"lambda_2\": alphas_lambdas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # SGDRegressor(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=SGDRegressor(random_state=0), \n",
    "        #     param_grid={\"loss\": losses, \"alpha\": sgd_alphas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # PassiveAggressiveRegressor(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=PassiveAggressiveRegressor(random_state=0), \n",
    "        #     param_grid={\"C\": cc}, \n",
    "        #     n_jobs=-1, \n",
    "        #     cv=3\n",
    "        # ),\n",
    "\n",
    "        # # HuberRegressor(max_iter=1000),\n",
    "        # HuberRegressor(max_iter=1000) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=HuberRegressor(), \n",
    "        #     param_grid={\"epsilon\": epsilons, \"alpha\": alphas}, \n",
    "        #     n_jobs=-1 \n",
    "        # ),\n",
    "\n",
    "        ## TheilSenRegressor(random_state=0, n_jobs=-1),\n",
    "        # TheilSenRegressor(random_state=0, n_jobs=-1) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=TheilSenRegressor(random_state=0, n_jobs=-1), \n",
    "        #     param_grid={\"n_subsamples\": n_subsamples}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        QuantileRegressor(),\n",
    "        # QuantileRegressor() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=QuantileRegressor(), \n",
    "        #     param_grid={\"alpha\": q_alphas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "        \n",
    "        \n",
    "        \n",
    "        KNeighborsRegressor(n_neighbors=10, metric='euclidean'),\n",
    "        # NuSVR(C=5.0, nu=0.9, kernel='poly', degree=3),\n",
    "        # SVR(C=5.0, epsilon=0.2, kernel='poly', degree=3),\n",
    "        \n",
    "        \n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=(3, ), \n",
    "            activation='identity', \n",
    "            max_iter=100000, \n",
    "            early_stopping=True, \n",
    "            learning_rate='constant',\n",
    "            learning_rate_init=0.00025,\n",
    "            batch_size=75,\n",
    "            solver='adam',\n",
    "            random_state=0\n",
    "        ),\n",
    " \n",
    "        RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0),\n",
    "       \n",
    "        ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0),\n",
    "        \n",
    "        HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0),\n",
    "        \n",
    "        BaggingRegressor(\n",
    "            #KNeighborsRegressor(n_neighbors=20, metric='euclidean'),\n",
    "            estimator=ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0), \n",
    "            max_samples=0.75, max_features=0.75, n_estimators=10, random_state=0\n",
    "        ),\n",
    "\n",
    "        VotingRegressor(\n",
    "            estimators=[\n",
    "                ('hgbr', HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0)), \n",
    "                ('omp', ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0)), \n",
    "                ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean')),\n",
    "                ('rfr', RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)),\n",
    "            ]\n",
    "        ),\n",
    "\n",
    "\n",
    "        StackingRegressor( # RidgeCV - final estimator\n",
    "            estimators=[\n",
    "                ('knr', KNeighborsRegressor(n_neighbors=10, metric='euclidean')),\n",
    "                ('rfr', RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)),\n",
    "                ('hgbr', HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0)), \n",
    "                ('etr', ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0)),\n",
    "                ('omp', OrthogonalMatchingPursuit(n_nonzero_coefs=5)),\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "        AdaBoostRegressor(estimator=KNeighborsRegressor(n_neighbors=5, metric='euclidean'), n_estimators=100, loss='linear', random_state=0),\n",
    "        \n",
    "    ]\n",
    "    return regressors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08312297-9ca3-4e7d-be60-26969a4611e7",
   "metadata": {},
   "source": [
    "#### Функция оценки и выбора моделей регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dff9414-8a26-4393-b480-446307aafc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(year, pr_group, n_test=None, norms=True, aug_n=0, aug_mpl=40, aug_pow=2, aug_mirror=False, grid_search=False, scaler_x=None, scaler_y=None, shuffle=True, verify=True, serial=True, top_best=None):\n",
    "    \n",
    "    ds_dir = f'data/{year}/Train'\n",
    "    \n",
    "    \n",
    "   \n",
    "    datasets = get_datasets()\n",
    "\n",
    "    fieldnames = [\n",
    "        'Predictors', \n",
    "        'Equations', \n",
    "        'Method', \n",
    "        'Criterion', \n",
    "        'Correlation', \n",
    "        'Pm',\n",
    "        # 'R2',\n",
    "\n",
    "        # 'Criterion_t', \n",
    "        # 'Correlation_t', \n",
    "        # 'Pm_t',\n",
    "        # 'R2_t',\n",
    "\n",
    "        # 'Group',\n",
    "        # 'Augmentation',\n",
    "        # 'Data size',\n",
    "        # 'Normalization',\n",
    "        # 'Equations',\n",
    "    ]\n",
    "\n",
    "    # Описание структуры данных переменной datasets_result\n",
    "    # datasets_result = {\n",
    "    #     \"hydropost_0\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    #     ...,\n",
    "    #     \"hydropost_n\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    # Итерация по датасетам\n",
    "    datasets_result = dict()\n",
    "    for ds in datasets:\n",
    "        result_list = []\n",
    "        \n",
    "        pr_list = get_predictors(ds, group=pr_group)\n",
    "        \n",
    "        X, y = get_river_dataset(f'{ds_dir}/{ds}.csv', pr_list=pr_list)\n",
    "\n",
    "        # Проверочный набор данных (исходный)\n",
    "        X_prior = X.copy()\n",
    "        y_prior = y.copy()\n",
    "\n",
    "        # Полный набор данных\n",
    "        X_full = X.copy()\n",
    "        y_full = y.copy()\n",
    "\n",
    "        if aug_mpl > 1:\n",
    "            # X_full, y_full = augment_data(X_full, y_full, aug_n, aug_pow=aug_pow, mirror=aug_mirror)\n",
    "            X_full, y_full = augment_data_2(X_full, y_full, aug_mpl, aug_pow=aug_pow, mirror=aug_mirror)\n",
    "        \n",
    "        if shuffle:\n",
    "            X_full, y_full = shuffle_xy(X_full, y_full, shuffle=True)\n",
    "        \n",
    "        if n_test:\n",
    "            X_train, y_train, X_test, y_test = train_test_split(X_full, y_full, n_test, split=True)\n",
    "\n",
    "        print(\"SHAPES:\")\n",
    "        print(\"X_train.shape, y_train.shape\", X_train.shape, y_train.shape)\n",
    "        print(\"X_test.shape, y_test.shape\", X_test.shape, y_test.shape)\n",
    "        \n",
    "        if norms:\n",
    "            norms_data = get_norms(ds)\n",
    "            # Подстановка норм в тестовый набор признаков\n",
    "            X_test = test_norm(X_test, pr_list, norms_data)\n",
    "            # Подстановка норм в исходный набор признаков\n",
    "            X_prior = test_norm(X_prior, pr_list, norms_data)\n",
    "            \n",
    "        # print(\"X_test:\")\n",
    "        # print(X_test)\n",
    "        # print(\"X_train:\")\n",
    "        # print(X_train)\n",
    "\n",
    "        transformer_y = get_transformer(scaler_y, n_samples=y_train.shape[0]) # !!!\n",
    "        transformer_x = get_transformer(scaler_x, n_samples=y_train.shape[0]) # !!!\n",
    "        print(transformer_y)\n",
    "        print(transformer_x)    \n",
    "            \n",
    "        # Список оцениваемых ререссионных моделей !!!!!\n",
    "        names = get_regressors_list()\n",
    "        regressors = get_regressors_objects()\n",
    "\n",
    "        # Итерация по моделям регрессии\n",
    "        for name, model in zip(names, regressors):\n",
    "\n",
    "            # Препроцессинг - трансформация целевых значений y\n",
    "            if scaler_y: \n",
    "                regressor = TransformedTargetRegressor(regressor=model, transformer=transformer_y)\n",
    "            else:\n",
    "                regressor = model\n",
    "            \n",
    "            \n",
    "            one_model_row = dict()\n",
    "\n",
    "            print('X_train.shape', X_train.shape)\n",
    "            print('y_train.shape', y_train.shape)\n",
    "            print('X_test.shape', X_test.shape)\n",
    "            print('y_test.shape', y_test.shape)\n",
    "            # n_samples = min(10000, y_train.shape[0]) ## Не нужно???\n",
    "\n",
    "            # Препроцессинг - трансформация признаков X\n",
    "            regr = make_pipeline(transformer_x, regressor) if transformer_x else regressor\n",
    "\n",
    "            # Обучение моделей на тренировочном наборе данных\n",
    "            # regr.fit(X_train, y_train)\n",
    "            try:\n",
    "                regr.fit(X_train, y_train)\n",
    "            except ValueError as error:\n",
    "                print(error)\n",
    "                continue\n",
    "            if serial:\n",
    "                serial_model = pickle.dumps(regr)\n",
    "                regr = pickle.loads(serial_model)\n",
    "\n",
    "            \n",
    "            # Прогноз по тестовому набору \n",
    "            y_predicted_test = np.ravel(regr.predict(X_test))\n",
    "            # Прогноз по исходному набору \n",
    "            y_predicted_prior = np.ravel(regr.predict(X_prior))\n",
    "\n",
    "            print('Estimation')\n",
    "            print(y_predicted_prior)\n",
    "            \n",
    "            # Очистка значений строк предикторов и уравнений перед переходом к следующей модели\n",
    "            coef = None\n",
    "            intercept = None\n",
    "            \n",
    "            try:\n",
    "                coef = regr.best_estimator_.coef_\n",
    "                intercept = regr.best_estimator_.intercept_\n",
    "                \n",
    "                if isinstance(intercept, np.ndarray):\n",
    "                    intercept = intercept[0]\n",
    "            except Exception as error:\n",
    "                                \n",
    "                try:\n",
    "                    coef = regr.coef_\n",
    "                    intercept = regr.intercept_\n",
    "                \n",
    "                    if isinstance(intercept, np.ndarray):\n",
    "                        intercept = intercept[0]\n",
    "                    print(\"ERROR1 START\")\n",
    "                    print(error)\n",
    "                    print(\"ERROR1 FINISH\")\n",
    "                except Exception as error:\n",
    "                    print(\"ERROR2 START\")\n",
    "                    print(error)\n",
    "                    print(\"ERROR2 FINISH\")\n",
    "                \n",
    "            \n",
    "            try:\n",
    "                # Коэффициенты уравнения (если есть)\n",
    "                coef = np.around(np.ravel(coef), 3)\n",
    "                intercept = round(intercept, 3)\n",
    "                \n",
    "                predictors_coef = {f: c for f, c \n",
    "                                   in zip(pr_list, coef) if c != 0.0}\n",
    "                \n",
    "                predictors = \", \".join(predictors_coef.keys())\n",
    "                \n",
    "                equation = (\n",
    "                    str(intercept) \n",
    "                    + ' ' \n",
    "                    + ' '.join(str(c) + '*' \n",
    "                               + f for f, c in predictors_coef.items())\n",
    "                )\n",
    "                \n",
    "                equation = equation.replace(\" -\", \"-\")\n",
    "                equation = equation.replace(\" \", \" + \")\n",
    "                equation = equation.replace(\"-\", \" - \")\n",
    "    \n",
    "                one_model_row['Predictors'] = predictors\n",
    "                one_model_row['Equations'] = equation\n",
    "            except Exception as error:\n",
    "                print(\"ERROR3 START\")\n",
    "                print(error)\n",
    "                print(\"ERROR3 FINISH\")\n",
    "                one_model_row['Predictors'] = \"\"\n",
    "                one_model_row['Equations'] = \"\"\n",
    "\n",
    "            # Название датасета\n",
    "            one_model_row['Dataset_name'] = ds\n",
    "\n",
    "            # Группа предикторов\n",
    "            one_model_row['Group'] = pr_group\n",
    "                \n",
    "            # Название метода\n",
    "            one_model_row['Method'] = name\n",
    "\n",
    "            # Расчет показателей качества по методике\n",
    "\n",
    "            # Сумма, максимум, минимум, среднее максимальных уровней\n",
    "            # по исходному набору:\n",
    "            one_model_row['H_sum'] = get_sum(y_prior)\n",
    "            one_model_row['H_max'] = get_max(y_prior)\n",
    "            one_model_row['H_min'] = get_min(y_prior)\n",
    "            h_max_avg = get_hmax_avg(y_prior)\n",
    "            one_model_row['H_avg'] = h_max_avg\n",
    "            # по тестовому набору:\n",
    "            one_model_row['H_sum_t'] = get_sum(y_test)\n",
    "            one_model_row['H_max_t'] = get_max(y_test)\n",
    "            one_model_row['H_min_t'] = get_min(y_test)\n",
    "            h_max_avg_t = get_hmax_avg(y_test)\n",
    "            one_model_row['H_avg_t'] = h_max_avg_t\n",
    "            \n",
    "            # # Среднее значение максимального уровня по всей выборке\n",
    "            # # по исходному набору:\n",
    "            # h_max_avg = get_hmax_avg(y_prior)\n",
    "            # one_model_row['H_avg'] = h_max_avg\n",
    "            # # по тестовому набору:\n",
    "            # h_max_avg_t = get_hmax_avg(y_test)\n",
    "            # one_model_row['H_avg_t'] = h_max_avg_t\n",
    "            \n",
    "            # Среднеквадратическое отклонение\n",
    "            # по исходному набору:\n",
    "            sigma = get_sigma(y_prior)\n",
    "            one_model_row['Sigma'] = sigma\n",
    "            # по тестовому набору:\n",
    "            sigma_t = get_sigma(y_test)\n",
    "            one_model_row['Sigma_t'] = sigma_t\n",
    "            \n",
    "            # Допустимая погрешность прогноза\n",
    "            # по исходному набору:\n",
    "            delta_dop = get_delta_dop(sigma)\n",
    "            one_model_row['Delta_dop'] = delta_dop\n",
    "            # по тестовому набору:\n",
    "            delta_dop_t = get_delta_dop(sigma_t)\n",
    "            one_model_row['Delta_dop_t'] = delta_dop_t\n",
    "            \n",
    "            # Обеспеченность климатическая Pk \n",
    "            # по исходному набору:\n",
    "            pk = get_pk(y_prior, h_max_avg, delta_dop)\n",
    "            one_model_row['Pk'] = pk\n",
    "            # по тестовому набору:\n",
    "            pk_t = get_pk(y_test, h_max_avg_t, delta_dop_t)\n",
    "            one_model_row['Pk_t'] = pk_t\n",
    "\n",
    "            # Обеспеченность метода (оправдываемость) Pm\n",
    "            # по исходному набору:\n",
    "            pm = get_pm(y_prior, y_predicted_prior, delta_dop)\n",
    "            one_model_row['Pm'] = pm\n",
    "            # по тестовому набору:\n",
    "            pm_t = get_pm(y_test, y_predicted_test, delta_dop_t)\n",
    "            one_model_row['Pm_t'] = pm_t\n",
    "\n",
    "            # Среднеквадратическая погрешность прогноза\n",
    "            # по исходному набору:\n",
    "            s_forecast = get_s(y_prior, y_predicted_prior)\n",
    "            one_model_row['S'] = s_forecast\n",
    "            # по тестовому набору:\n",
    "            s_forecast_t = get_s(y_test, y_predicted_test)\n",
    "            one_model_row['S_t'] = s_forecast_t\n",
    "            \n",
    "            # Критерий эффективности метода прогнозирования \n",
    "            # климатический S/sigma\n",
    "            # по исходному набору:\n",
    "            criterion_forecast = get_criterion(s_forecast, sigma)\n",
    "            one_model_row['Criterion'] = criterion_forecast\n",
    "            # по тестовому набору:\n",
    "            criterion_forecast_t = get_criterion(s_forecast_t, sigma_t)\n",
    "            one_model_row['Criterion_t'] = criterion_forecast_t\n",
    "            \n",
    "            # Критерий эффективности метода прогнозирования \n",
    "            # климатический S/sigma в квадрате\n",
    "            # по исходному набору:\n",
    "            criterion_sqr = get_criterion(s_forecast, sigma) ** 2.0\n",
    "            one_model_row['Criterion_sqr'] = criterion_sqr\n",
    "            # по тестовому набору:\n",
    "            criterion_sqr_t = get_criterion(s_forecast_t, sigma_t) ** 2.0\n",
    "            one_model_row['Criterion_sqr_t'] = criterion_sqr_t\n",
    "\n",
    "            \n",
    "            # Корреляционное отношение ro\n",
    "            # по исходному набору:\n",
    "            correlation_forecast = get_correlation_ratio(criterion_forecast)\n",
    "            one_model_row['Correlation'] = correlation_forecast\n",
    "            # по тестовому набору:\n",
    "            correlation_forecast_t = get_correlation_ratio(criterion_forecast_t)\n",
    "            one_model_row['Correlation_t'] = correlation_forecast_t\n",
    "            \n",
    "            # Коэффициент детерминации R2\n",
    "            # по исходному набору:\n",
    "            one_model_row['R2'] = regr.score(X_prior, y_prior)\n",
    "            # по тестовому набору:\n",
    "            one_model_row['R2_t'] = regr.score(X_test, y_test)\n",
    "            \n",
    "            # Обученная модель\n",
    "            one_model_row['Model'] = regr\n",
    "\n",
    "            # Дополнительные поля для сравнения результатов экспериментов\n",
    "            one_model_row['Group'] = pr_group\n",
    "            one_model_row['Augmentation'] = f'Spline {aug_pow}' if aug_mpl > 0 else ''\n",
    "            one_model_row['Aug-Mirror'] = f'Mirror' if aug_mirror > 0 else ''\n",
    "            one_model_row['Data size'] = X_train.shape[0]\n",
    "            one_model_row['X_scaling'] = scaler_x if scaler_x else ''\n",
    "            one_model_row['Y_scaling'] = scaler_y if scaler_y else ''\n",
    "            one_model_row['Equation'] = 'Yes' if one_model_row['Equations'] else ''\n",
    "            one_model_row['Shuffle'] = 'Yes' if shuffle else ''\n",
    "            \n",
    "            # Добавление результатов модели в результирующий список по датасету\n",
    "            result_list.append(one_model_row)\n",
    "            \n",
    "\n",
    "        # Сортировка результатов по каждому датасету\n",
    "        result_list.sort(\n",
    "            key=lambda row: (row['Criterion'], \n",
    "                             -row['Correlation'], \n",
    "                             -row['Pm'])\n",
    "        )\n",
    "\n",
    "        datasets_result[ds] = result_list\n",
    "\n",
    "        # Запись в .csv файл\n",
    "        write_dataset_csv(year, result_list, ds, fieldnames, pr_group=pr_group, mode='estimation')\n",
    "\n",
    "        # Формирование проверочных прогнозов по исходным данным\n",
    "        if verify:\n",
    "            for i, rl in enumerate(result_list):\n",
    "                if top_best is not None:\n",
    "                    if i >= top_best:\n",
    "                        break\n",
    "                verify_forecast(year, ds, rl, i, pr_group=pr_group, n_test=n_test, norms=norms)\n",
    "\n",
    "    return datasets_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d848a-fd25-4827-989d-013f2e6d1fbd",
   "metadata": {},
   "source": [
    "#### Функция формирования проверочных прогнозов и записи их в csv-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d9ced3-0d12-460b-8cd3-6b030f713b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_forecast(year, dataset_name, rl, num, pr_group, n_test=None, norms=True):\n",
    "\n",
    "    ds_dir = f'data/{year}/Train'\n",
    "\n",
    "    pr_list = get_predictors(dataset_name, group=pr_group)\n",
    "    pr_list = ['year'] + pr_list\n",
    "    \n",
    "    fieldnames = [\n",
    "        '№', \n",
    "        'Год',\n",
    "        'Hmax фактический', \n",
    "        'Hф-Hср', \n",
    "        '(Hф-Hср)^2', \n",
    "        \n",
    "        'δ50% Погрешность климатических прогнозов '\n",
    "        'в долях от допустимой погрешности',\n",
    "        \n",
    "        'Hmax прогнозный', \n",
    "        'Hф-Hп', \n",
    "        '(Hф-Hп)^2', \n",
    "        \n",
    "        'δ50% Погрешность проверочных прогнозов '\n",
    "        'в долях от допустимой погрешности',\n",
    "    ]\n",
    "\n",
    "    X, y = get_river_dataset(\n",
    "        f'{ds_dir}/{dataset_name}.csv', pr_list=pr_list, y_name='H_max'\n",
    "    )\n",
    "\n",
    "    X_test = X.copy()\n",
    "    y_test = y.copy()\n",
    "    \n",
    "    if norms:\n",
    "        norms = get_norms(dataset_name)\n",
    "        X_test = test_norm(X_test, pr_list, norms)\n",
    "\n",
    "    # Выделение первой колонки (года) из набора предикторов\n",
    "    years = X_test[:, 0].copy()\n",
    "    X_test = X_test[:, 1:].copy()\n",
    "    \n",
    "    # Forecast\n",
    "    h_max_forecast = np.ravel(rl['Model'].predict(X_test))\n",
    "\n",
    "    print('verify_forecast')\n",
    "    print(h_max_forecast)\n",
    "    \n",
    "    # Hсредний\n",
    "    h_max_avg = np.mean(y)\n",
    "\n",
    "    # H - Hсредний\n",
    "    diff_fact = y_test - h_max_avg\n",
    "\n",
    "    # (H - Hсредний) в квадрате\n",
    "    diff_fact_sqr = diff_fact ** 2\n",
    "\n",
    "    # Погрешность климатических прогнозов в долях от допустимой погрешности\n",
    "    delta_dop = get_delta_dop(get_sigma(y))\n",
    "    error_climate = get_delta50(y_test, delta_dop, h_max_avg=h_max_avg)\n",
    "\n",
    "    # H - Hпрогнозный\n",
    "    diff_forecast = y_test - h_max_forecast\n",
    "\n",
    "    # (H - Hпрогнозный) в квадрате\n",
    "    diff_forecast_sqr = diff_forecast ** 2       \n",
    "\n",
    "    # Погрешность проверочных прогнозов в долях от допустимой погрешности\n",
    "    error_forecast = get_delta50(\n",
    "        y_test, delta_dop, h_max_forecast=h_max_forecast\n",
    "    )\n",
    "\n",
    "    # Номер по порядку\n",
    "    rows_num = y_test.shape[0]\n",
    "    npp = np.arange(1, rows_num + 1, 1)\n",
    "\n",
    "    # Конкатенация массивов\n",
    "    att_tuple = (\n",
    "        npp, \n",
    "        years, \n",
    "        y_test, \n",
    "        diff_fact, \n",
    "        diff_fact_sqr, \n",
    "        error_climate, \n",
    "        h_max_forecast, \n",
    "        diff_forecast, \n",
    "        diff_forecast_sqr, \n",
    "        error_forecast\n",
    "    )\n",
    "    \n",
    "    arr = np.column_stack(att_tuple)\n",
    "    arr = arr.tolist()\n",
    "\n",
    "    # Обеспеченность метода (оправдываемость) Pm\n",
    "    pm = get_pm(y_test, h_max_forecast, delta_dop)\n",
    "    \n",
    "    # Запись проверочного прогноза в csv файл\n",
    "    with open(\n",
    "        f'results/Estimation/{year}/{dataset_name}/group-{pr_group}/{dataset_name}'\n",
    "        f'-проверочный-гр{pr_group}-{num:0>2}.csv', \n",
    "        'w', \n",
    "        newline='', \n",
    "        encoding='utf-8'\n",
    "    ) as csvfile:\n",
    "        \n",
    "        stat_header = (\n",
    "            f\"Таблица  - \"\n",
    "            f\"Проверочные прогнозы максимумов весеннего половодья\\n\"\n",
    "            f\"р.{rl['Dataset_name']}\\n\"\n",
    "            f\"Предикторы:;; {rl['Predictors']}\\n\"\n",
    "            f\"Уравнение:;; {rl['Equations']}\\n\"\n",
    "            f\"Модель:;; {rl['Method']}\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        csvfile.write(stat_header)\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(arr)\n",
    "        \n",
    "        stat_footer = (\n",
    "            f\"Сумма;;{rl['H_sum']}\\n\"  \n",
    "            f\"Средний;;{rl['H_avg']}\\n\" \n",
    "            f\"Высший;;{rl['H_max']}\\n\"\n",
    "            f\"Низший;;{rl['H_min']}\\n\\n\"\n",
    "            \n",
    "            f\"σ = ;;{rl['Sigma']};;σ -;\"\n",
    "            f\"среднеквадратическое отклонение (см)\\n\" \n",
    "            \n",
    "            f\"δдоп =;;{rl['Delta_dop']};;δдоп -;\"\n",
    "            f\"допустимая погрешность прогноза (см)\\n\" \n",
    "            \n",
    "            f\"Pк =;;{rl['Pk']};;Pк -;\"\n",
    "            f\"климатическая обеспеченность в %\\n\"\n",
    "            \n",
    "            f\"Pм =;;{rl['Pm']};;Pм -;\"\n",
    "            f\"обеспеченность метода в %\\n\"\n",
    "            \n",
    "            f\"S =;;{rl['S']};;;\"\n",
    "            f\"(допустимой погрешности проверочных прогнозов)\\n\"\n",
    "            \n",
    "            f\"S/σ =;;{rl['Criterion']};;S -;\"\n",
    "            f\"среднеквадратическая погрешность (см)\\n\" \n",
    "            \n",
    "            f\"(S/σ)^2 =;;{rl['Criterion_sqr']};;S/σ -;\"\n",
    "            f\"критерий эффективности метода прогнозирования\\n\"\n",
    "            \n",
    "            f\"ρ =;;{rl['Correlation']};;ρ -;\"\n",
    "            f\"корреляционное отношение\\n\"\n",
    "            \n",
    "            f\";;;;;(оценка эффективности метода прогнозирования)\\n\"\n",
    "            f\";;;;δ50% -;погрешность (ошибка) прогнозов (см)\\n\"\n",
    "        )\n",
    "        \n",
    "        csvfile.write(stat_footer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1899385e-1136-4e1c-b3f0-02785c3a04c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "exp_fieldnames = [\n",
    "        'Dataset_name',\n",
    "        'Group',\n",
    "        'Method',\n",
    "        'Criterion',\n",
    "        'Correlation',\n",
    "        'Pm',\n",
    "        'R2',\n",
    "        'Criterion_t',\n",
    "        'Correlation_t',\n",
    "        'Pm_t',\n",
    "        'R2_t',\n",
    "    \n",
    "        \n",
    "        'Augmentation',\n",
    "        'Aug-Mirror',\n",
    "        'Data size',\n",
    "        'X_scaling',\n",
    "        'Y_scaling',\n",
    "        'Shuffle',\n",
    "        'Equation',\n",
    "        'Predictors',\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae6e8e-07c7-4de1-b8cc-0ae599007b06",
   "metadata": {},
   "source": [
    "#### Запуск модуля оценки и выбора моделей множественной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c19167-8fd4-4c5c-a519-3833f79076db",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    compare_models(2024, pr_group=0, n_test=100, norms=True, aug_n=1000, aug_mpl=40, aug_pow=2, aug_mirror=False, grid_search=False, scaler_x=None, scaler_y=None, shuffle=True, verify=True),\n",
    "    compare_models(2024, pr_group=1, n_test=100, norms=True, aug_n=1000, aug_mpl=40, aug_pow=2, aug_mirror=False, grid_search=True, scaler_x=None, scaler_y=None, shuffle=True, verify=True),\n",
    "    compare_models(2024, pr_group=2, n_test=100, norms=True, aug_n=1000, aug_mpl=40, aug_pow=2, aug_mirror=False, grid_search=True, scaler_x=None, scaler_y=None, shuffle=True, verify=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cff84-72b9-45d5-a02f-839d7814065a",
   "metadata": {},
   "source": [
    "#### Код записи результатов экспериментов в файл experiment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6785b086-8307-4300-8021-c39efd6eef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "for ds in experiments:\n",
    "    #print(ds)\n",
    "    for k, v in ds.items():\n",
    "        exp_results.extend(v)\n",
    "        \n",
    "exp_results.sort(\n",
    "    key=lambda row: (row['Dataset_name'], -row['R2'])\n",
    ")\n",
    "write_experiment_csv(2024, exp_results, exp_fieldnames, 'experiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
