{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e64d76-c788-4b3d-9eab-a19bc693668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a36d14-2f54-4edd-bd04-3ecba00281fe",
   "metadata": {},
   "source": [
    "#### Функция чтения набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dff2f17e-4fab-4bbe-9c1f-465b9638cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_river_dataset(fname, pr_list=None, y_name='H_max'):\n",
    "    pr_arr = []\n",
    "    y_arr = []\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            pr_arr_row = []\n",
    "            for pr in pr_list:\n",
    "                # Если пустая строка, присвоить None (== np.nan после преобразования во float в np-массиве)\n",
    "                row_pr = None if len(row[pr]) == 0 else row[pr]\n",
    "                pr_arr_row.append(row_pr)\n",
    "            pr_arr.append(pr_arr_row)\n",
    "            y_arr.append(row[y_name] if row[y_name] else 0)\n",
    "    X = np.asarray(pr_arr, dtype=np.float64)\n",
    "    y = np.asarray(y_arr, dtype=np.float64)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f85915-970a-4a02-b6e5-85aa86fa8e1d",
   "metadata": {},
   "source": [
    "#### Функция формирования тестового набора данных с подстановкой нормированных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260024c9-0662-4996-8761-ea9bfe657cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_norm(x, pr_list, norms):\n",
    "    x_norm = np.copy(x)\n",
    "    # print('norms')\n",
    "    # print(norms)\n",
    "    for col, pr in enumerate(pr_list):\n",
    "        # print(f'Predictor: {pr}')\n",
    "        if pr in norms:\n",
    "            # print(f'x_norm[:, col]: {pr} norm: {norms[pr]}')\n",
    "            # print(x_norm[:, col])\n",
    "            ix = (np.isnan(x_norm[:, col]))\n",
    "            # print(ix)\n",
    "            x_norm[ix, col] = norms[pr] \n",
    "            # print('CORRECTED', x_norm[:, col])\n",
    "            # x_norm[:, col:col+1] = norms[pr]\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4310b5c9-b79e-427d-b7b4-68e70d34794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset_csv(year, dataset, dataset_name, fieldnames, pr_group=None, mode='training'):\n",
    "    if mode == 'estimation':\n",
    "        dir_path = f'results/Estimation/{year}/{dataset_name}/group-{pr_group}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Оценка.csv'\n",
    "    elif mode == 'training':\n",
    "        dir_path = f'results/Models/{year}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Обучение.csv'\n",
    "    elif mode == 'forecast':\n",
    "        dir_path = f'results/Forecast/{year}/'\n",
    "        if pr_group is None:\n",
    "            file_name = f'{dataset_name}-Прогноз.csv'\n",
    "        else:\n",
    "            file_name = f'{dataset_name}-гр{pr_group}-Прогноз.csv'\n",
    "    else:\n",
    "        ...\n",
    "    \n",
    "    with open(\n",
    "        f'{dir_path}'\n",
    "        f'{file_name}', \n",
    "        'w', newline='', encoding='utf-8'\n",
    "    ) as csvfile:\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, \n",
    "                                delimiter=';', extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb25dc-b5b4-4ca1-a00c-1bf000b6e5bc",
   "metadata": {},
   "source": [
    "### Функция прогнозирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d417d6-3c45-4804-b8b6-3ce9f89a3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(year, norms=False, groups_files=False):\n",
    "\n",
    "    fieldnames = [\n",
    "        'Group',\n",
    "        'Prediction',\n",
    "        'Criterion', \n",
    "        'Correlation', \n",
    "        'Pm',\n",
    "        'R2',        \n",
    "        'R2_t',\n",
    "        'Method', \n",
    "        'Predictors', \n",
    "        'Equations',        \n",
    "    ]\n",
    "    \n",
    "    predict_data_dir = f'data/{year}/Predict'\n",
    "    \n",
    "    models_dir = f'results/Models/{year}'\n",
    "\n",
    "    # Получить список файлов .ipnb из results\\Models\\<year>\n",
    "    file_list = tuple(filter(lambda fn: '.pickle' in fn, os.listdir(models_dir)))\n",
    "    # print(file_list)\n",
    "\n",
    "    # Сделать множество из кортежей (Название-датасета, группа), преобразовать в список, отсортировать\n",
    "    dataset_group = set()\n",
    "    for fn in file_list:\n",
    "        dataset, year, group, model = fn.split('_')\n",
    "        dataset_group |= {(dataset, group.split('гр')[1])}\n",
    "    ds_list = sorted(dataset_group)\n",
    "    # print(ds_list)\n",
    "\n",
    "    # Для каждого элемента списка создать result_list[one_model_raw: dict]\n",
    "    \n",
    "    for ds, group in ds_list:\n",
    "        # print(f'ds: {ds}, group: {group}')\n",
    "        result_list = []\n",
    "        # ds_models = filter(lambda fn: ds in fn and f'гр{group}' in fn, file_list)\n",
    "        ds_models = filter(lambda fn: ds in fn, file_list)\n",
    "        # print(list(ds_models))\n",
    "        # print('------------------------------------------------------------------------')\n",
    "        # print()\n",
    "        for file_name in ds_models:\n",
    "            with open(f'results/Models/{year}/{file_name}', 'rb') as f:\n",
    "                model_info = pickle.load(f)\n",
    "                model = model_info['Model_full']\n",
    "                #model = model_info['Model_train']\n",
    "\n",
    "                # print(model_info['Group'])\n",
    "                \n",
    "                # Прочитать новые признаки (предикторы) для прогноза\n",
    "                X_new, y = get_river_dataset(f'{predict_data_dir}/{ds}.csv', pr_list=model_info['Predictors_list'])\n",
    "\n",
    "                # Подстановка нормированных значений для целей тестирования\n",
    "                if norms:\n",
    "                    # Подстановка норм в исходный набор признаков\n",
    "                    X_new = test_norm(X_new, model_info['Predictors_list'], model_info['Norms_data'])\n",
    "\n",
    "                # print(model_info['Dataset_name'])\n",
    "                # print(model_info['Predictors_list'])\n",
    "                # print(model_info['Norms_data'])\n",
    "                # print(model_info['Method'])\n",
    "                # print(\"y\")\n",
    "                # print(y)\n",
    "                # print(f'R2={model_info[\"R2\"]}, R2_t={model_info[\"R2_t\"]}')\n",
    "                \n",
    "                y_predicted = np.ravel(model.predict(X_new))\n",
    "                # print(\"y_predicted\")\n",
    "                # print(y_predicted)\n",
    "                # print('X_new')\n",
    "                # print(X_new)\n",
    "                # print(model)\n",
    "                model_info['Prediction'] = round(y_predicted[-1])\n",
    "                result_list.append(model_info)\n",
    "                # Сортировка результатов по каждому датасету\n",
    "                result_list.sort(key=lambda row: (row['Criterion'], -row['Correlation'], -row['Pm']))\n",
    "                \n",
    "        # Запись в .csv файл\n",
    "        if groups_files:\n",
    "            write_dataset_csv(year, result_list, ds, fieldnames, pr_group=group, mode='forecast')\n",
    "        else:\n",
    "            write_dataset_csv(year, result_list, ds, fieldnames, pr_group=None, mode='forecast')\n",
    "        print(model_info['Dataset_name'])\n",
    "        # print('------------------------------------------------------------------------------')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96182e4d-99d5-43cd-8574-4cc27106bf9d",
   "metadata": {},
   "source": [
    "#### Запуск процесса прогнозирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d381c631-abb7-45dc-94cf-599ce7731cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЗападнаяДвина-Верхнедвинск\n",
      "ЗападнаяДвина-Верхнедвинск\n",
      "ЗападнаяДвина-Верхнедвинск\n",
      "ЗападнаяДвина-Витебск\n",
      "ЗападнаяДвина-Витебск\n",
      "ЗападнаяДвина-Витебск\n",
      "ЗападнаяДвина-Полоцк\n",
      "ЗападнаяДвина-Полоцк\n",
      "ЗападнаяДвина-Полоцк\n",
      "ЗападнаяДвина-Сураж\n",
      "ЗападнаяДвина-Сураж\n",
      "ЗападнаяДвина-Сураж\n"
     ]
    }
   ],
   "source": [
    "forecast(2024, norms=True, groups_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b496c57-fc4d-46d9-8f84-379b43ddb0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False]\n",
      "[False False False  True]\n",
      "[ True False False False]\n"
     ]
    }
   ],
   "source": [
    "ar = np.array(\n",
    "    [\n",
    "        [1, 2, 0],\n",
    "        [3, 4, 6],\n",
    "        [0, 5, 9],\n",
    "        [7, 0, 8]\n",
    "    ]\n",
    "    \n",
    ")\n",
    "v = [-1, -2, -3]\n",
    "for i, c in enumerate(range(ar.shape[1])):\n",
    "    ix = (ar[:, c]==0)\n",
    "    print(ix)\n",
    "    ar[ix, c] = v[i]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c5c723-6367-475d-9573-4447beeba584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2, -3],\n",
       "       [ 3,  4,  6],\n",
       "       [-1,  5,  9],\n",
       "       [ 7, -2,  8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f5224-c035-4387-bcce-e3ca069089bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
