{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c8031c-3e29-4149-aaa3-ef3f3f0d444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cff566a-8886-47b1-a651-f3df2c864e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_river_dataset(fname, pr_list=None, y_name='H_max'):\n",
    "    pr_arr = []\n",
    "    y_arr = []\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            pr_arr_row = []\n",
    "            for pr in pr_list:\n",
    "                pr_arr_row.append(row[pr])\n",
    "\n",
    "            pr_arr.append(pr_arr_row)\n",
    "            y_arr.append(row[y_name])\n",
    "    X = np.asarray(pr_arr, dtype=np.float64)\n",
    "    y = np.asarray(y_arr, dtype=np.float64)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a96a1-c37d-47ba-b369-2a015ec5706f",
   "metadata": {},
   "source": [
    "#### Сумма, средний, высший, низший уровни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa78832-4724-42e1-8e4f-8513406d88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(h_max):\n",
    "    return np.sum(h_max)\n",
    "    \n",
    "def get_avg(h_max):\n",
    "    return np.mean(h_max)\n",
    "    \n",
    "def get_max(h_max):\n",
    "    return np.amax(h_max)\n",
    "    \n",
    "def get_min(h_max):\n",
    "    return np.amin(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dafa4-e1f7-4c26-b28d-3dfd6a2b1b14",
   "metadata": {},
   "source": [
    "#### Среднеквадратическая погрешность прогноза S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad85fc4-c2cd-4a9f-8a0a-bb4fe4cdfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s(h_max, h_forecast=None):\n",
    "    # Среднеквадратическая погрешность прогноза\n",
    "    n = h_max.shape[0]\n",
    "    sqr_diff = np.sum((h_max - h_forecast) ** 2) / (n - 1)\n",
    "    std = sqr_diff ** 0.5\n",
    "    return std    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e98c4-4841-4b3f-b61b-284bc99af53d",
   "metadata": {},
   "source": [
    "#### Среднеквадратическое отклонение sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5079b8f9-3ae5-47a1-a6f3-214bdf69ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma(h_max):\n",
    "    # Среднеквадратическая погрешность климатическая.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.std(h_max, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fe5ef3-810d-40e8-87f0-9432f139319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hmax_avg(h_max):\n",
    "    # Среднее значение h_max.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.mean(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10fa8d-d7cd-44f4-b4f2-4f652409383f",
   "metadata": {},
   "source": [
    "#### Допустимая погрешность прогноза delta_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d4648d-8e14-4c63-bcf8-6a3c2cb7a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_dop(sigma):\n",
    "    return 0.674 * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1c7ff-c4e1-4639-a877-d3b78d944dea",
   "metadata": {},
   "source": [
    "#### Критерий эффективности метода прогнозирования климатический S/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92739a9-3b12-44bd-b51e-4ffd73e828a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(s, sigma):\n",
    "    return s / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec5970-ba12-4121-8403-cd13b812b2de",
   "metadata": {},
   "source": [
    "#### Климатическая обеспеченность Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e1b890-8fb7-4fdd-8196-610544acee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pk(h_max, h_max_avg, delta_dop):\n",
    "    #avg_level = np.mean(h_max)\n",
    "    diff = np.abs(h_max - h_max_avg) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4c686-2946-4ea0-980a-172b2ff4dd64",
   "metadata": {},
   "source": [
    "#### Обеспеченность метода (оправдываемость) Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ca5adf-fbf3-49a4-8458-06d103bfacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pm(h_max, h_forecast, delta_dop):\n",
    "    diff = np.abs(h_max - h_forecast) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d93f0-e243-4107-b007-cb24efbdc6f6",
   "metadata": {},
   "source": [
    "#### Корреляционное отношение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a501afb2-3736-4023-a488-a19747e54910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_ratio(criterion):\n",
    "    c_1 = (1 - criterion ** 2)\n",
    "    ro = c_1 ** 0.5 if c_1 > 0 else 0\n",
    "    return ro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059ec96-bf22-4b2b-83f9-dc2112e0c942",
   "metadata": {},
   "source": [
    "#### Вероятная ошибка прогноза S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e88b65-02b2-4bee-9262-b84a7a1ea70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_error(s):\n",
    "    return 0.674 * s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0abe2-cca4-4ebc-b6bf-f27c01c6b1bd",
   "metadata": {},
   "source": [
    "#### Ошибки климатического/природного прогноза для каждого года delta50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b00b24-686d-449b-9920-aa4733a0bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta50(h_max, delta_dop, h_max_avg=None, h_max_forecast=None):\n",
    "    if h_max_forecast is None:\n",
    "        # delta50 климатическая\n",
    "        return (h_max - h_max_avg) / delta_dop\n",
    "    else:\n",
    "        # delta50 прогноза\n",
    "        return (h_max - h_max_forecast) / delta_dop\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0cc72-ec19-437b-b6c5-8010c2e3a5bf",
   "metadata": {},
   "source": [
    "#### Функция записи в csv файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c76ad0-d75c-4746-90dc-0d8465218463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_dataset_csv(dataset, dataset_name, fieldnames, pr_group):\n",
    "    with open(f'results/{dataset_name}/group-{pr_group}/{dataset_name}-гр{pr_group}.csv', 'w', newline='', encoding='utf-8') as csvfile:# , encoding='utf-8'\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=';', extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2d2a0-f9da-46e5-a0ae-45e8fe0c2482",
   "metadata": {},
   "source": [
    "#### Функция разделения набора данных на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82db9e50-fadc-4c7f-9151-58c2b74df8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, n_test):\n",
    "    X_train = X[:-n_test]\n",
    "    y_train = y[:-n_test]\n",
    "    X_test = X[-n_test:]\n",
    "    y_test = y[-n_test:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719a5f9-176e-473d-8ce7-b7744d9c6e19",
   "metadata": {},
   "source": [
    "#### Функция формирования тестового набора данных с подстановкой нормированных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b464d45-843c-4792-ab02-2819edddee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_norm(x, pr_list, norms):\n",
    "    x_norm = np.copy(x)\n",
    "    for col, pr in enumerate(pr_list):\n",
    "        if pr in norms:\n",
    "            x_norm[:, col:col+1] = norms[pr]\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3a942-95d0-4f7f-aa39-82f61624b070",
   "metadata": {},
   "source": [
    "#### Функция получения датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7628a977-9d54-4129-928c-625b4e80d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    datasets = {\n",
    "        # 'Неман-Белица': 'Неман',\n",
    "        # 'Неман-Гродно': 'Неман',\n",
    "        #'Неман-Мосты': 'Неман',\n",
    "        'Неман-Столбцы': 'Неман',\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83168b0-64e2-4e58-a47b-f49f677a9eb4",
   "metadata": {},
   "source": [
    "#### Функция получения списка предикторов по названию датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca1eea5-a699-4b0e-91fa-9fe6f4451ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictors(dataset_name, pr_group):\n",
    "\n",
    "    datasets = get_datasets()   \n",
    "    \n",
    "    # predictors_lists = {\n",
    "    #     'Неман': ['s_2802', 's_max', 'h', 'x', 'x1', 'x2', 'x3', 'x4', 'xs'],\n",
    "    # }\n",
    "\n",
    "    predictors_lists = {\n",
    "        'Неман': (\n",
    "            ['S_2802', 'Smax', 'H', 'X', 'X1', 'X2', 'X3', 'X4', 'Xs'],\n",
    "            ['Smax', 'H', 'X', 'X1', 'X3'],\n",
    "            ['S_2802', 'H', 'X2', 'X3', 'Xs'],\n",
    "        )\n",
    "    }\n",
    "    return predictors_lists[datasets[dataset_name]][pr_group]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a741f9dd-31c9-4ea6-a1a8-b2b684b05013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norms(dataset_name):\n",
    "    norms_list = {\n",
    "        # 'Неман-Белица': {'X1': 46.0, 'X2':35.0},\n",
    "        # 'Неман-Гродно': {'X1': 36.0, 'X2':26.0},\n",
    "        'Неман-Мосты': {'x1': 40.0, 'x2':31.0},\n",
    "        'Неман-Столбцы': {'x1': 43.0, 'x2':34.0},\n",
    "    }\n",
    "    return norms_list[dataset_name]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecbd695a-3c19-44bf-ab0f-49cccc3686dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-02, 1.12332403e-02, 1.26185688e-02, 1.41747416e-02,\n",
       "       1.59228279e-02, 1.78864953e-02, 2.00923300e-02, 2.25701972e-02,\n",
       "       2.53536449e-02, 2.84803587e-02, 3.19926714e-02, 3.59381366e-02,\n",
       "       4.03701726e-02, 4.53487851e-02, 5.09413801e-02, 5.72236766e-02,\n",
       "       6.42807312e-02, 7.22080902e-02, 8.11130831e-02, 9.11162756e-02,\n",
       "       1.02353102e-01, 1.14975700e-01, 1.29154967e-01, 1.45082878e-01,\n",
       "       1.62975083e-01, 1.83073828e-01, 2.05651231e-01, 2.31012970e-01,\n",
       "       2.59502421e-01, 2.91505306e-01, 3.27454916e-01, 3.67837977e-01,\n",
       "       4.13201240e-01, 4.64158883e-01, 5.21400829e-01, 5.85702082e-01,\n",
       "       6.57933225e-01, 7.39072203e-01, 8.30217568e-01, 9.32603347e-01,\n",
       "       1.04761575e+00, 1.17681195e+00, 1.32194115e+00, 1.48496826e+00,\n",
       "       1.66810054e+00, 1.87381742e+00, 2.10490414e+00, 2.36448941e+00,\n",
       "       2.65608778e+00, 2.98364724e+00, 3.35160265e+00, 3.76493581e+00,\n",
       "       4.22924287e+00, 4.75081016e+00, 5.33669923e+00, 5.99484250e+00,\n",
       "       6.73415066e+00, 7.56463328e+00, 8.49753436e+00, 9.54548457e+00,\n",
       "       1.07226722e+01, 1.20450354e+01, 1.35304777e+01, 1.51991108e+01,\n",
       "       1.70735265e+01, 1.91791026e+01, 2.15443469e+01, 2.42012826e+01,\n",
       "       2.71858824e+01, 3.05385551e+01, 3.43046929e+01, 3.85352859e+01,\n",
       "       4.32876128e+01, 4.86260158e+01, 5.46227722e+01, 6.13590727e+01,\n",
       "       6.89261210e+01, 7.74263683e+01, 8.69749003e+01, 9.77009957e+01,\n",
       "       1.09749877e+02, 1.23284674e+02, 1.38488637e+02, 1.55567614e+02,\n",
       "       1.74752840e+02, 1.96304065e+02, 2.20513074e+02, 2.47707636e+02,\n",
       "       2.78255940e+02, 3.12571585e+02, 3.51119173e+02, 3.94420606e+02,\n",
       "       4.43062146e+02, 4.97702356e+02, 5.59081018e+02, 6.28029144e+02,\n",
       "       7.05480231e+02, 7.92482898e+02, 8.90215085e+02, 1.00000000e+03])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-2, 3, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15fc6558-b60a-4b4a-9452-bdec88bac946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.utils.fixes import loguniform\n",
    "# loguniform(1e-2, 1e0)\n",
    "\n",
    "# import scipy.stats as stats\n",
    "# stats.uniform(0, 1)\n",
    "\n",
    "# np.logspace(-7, -5, num=10)\n",
    "\n",
    "#np.linspace(0.01, 1.0, num=50)\n",
    "\n",
    "#np.arange(0.1, 1.5, 50)\n",
    "np.arange(1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08312297-9ca3-4e7d-be60-26969a4611e7",
   "metadata": {},
   "source": [
    "#### Функция обучения и оценки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dff9414-8a26-4393-b480-446307aafc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(pr_group, n_test=None, norms=True, top_best=None):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Ridge, RidgeCV\n",
    "    from sklearn.linear_model import Lasso, LassoCV\n",
    "    from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "    from sklearn.linear_model import Lars, LarsCV\n",
    "    from sklearn.linear_model import LassoLars\n",
    "    from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    from sklearn.linear_model import ARDRegression\n",
    "    from sklearn.linear_model import TweedieRegressor\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "    from sklearn.linear_model import HuberRegressor\n",
    "    from sklearn.linear_model import TheilSenRegressor\n",
    "    from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF\n",
    "\n",
    "    import scipy.stats as stats\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "    from sklearn.feature_selection import r_regression\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ds_dir = 'data' # В константы\n",
    "    \n",
    "    names = [\n",
    "        # 'LinearRegression',\n",
    "        # 'Ridge_GridSearchCV',\n",
    "        # 'Lasso_GridSearchCV',\n",
    "        # 'ElasticNet_GridSearchCV',\n",
    "        # 'Lars1',\n",
    "        # 'Lars2',\n",
    "        # 'Lars3',\n",
    "        # 'Lars4',\n",
    "        'Lars5',\n",
    "        # 'Lars6',\n",
    "        # 'Lars7',\n",
    "        # 'Lars8',\n",
    "        # 'LassoLars_GridSearchCV',\n",
    "        # 'OMP1',\n",
    "        # 'OMP2',\n",
    "        # 'OMP3',\n",
    "        # 'OMP4',\n",
    "        # 'OMP5',\n",
    "        # 'BayesianRidge',\n",
    "        # 'ARDRegression',\n",
    "        # 'TweedieRegressor_GridSearchCV',\n",
    "        # 'SGDRegressor', \n",
    "        # 'PassiveAggressiveRegressor',\n",
    "        # 'HuberRegressor',\n",
    "        # 'TheilSenRegressor',\n",
    "        # 'QuantileRegressor',\n",
    "        \n",
    "        \n",
    "        \n",
    "        #'Perceptron'\n",
    "        #'GaussianProcessRegressor',\n",
    "        \n",
    "    ]\n",
    "    rng = np.random.RandomState(0)\n",
    "    alphas = np.logspace(-2, 3, num=100)\n",
    "    sgd_alphas = np.logspace(-4, 1, num=100)\n",
    "    l1_ratio = np.linspace(0.01, 1.0, num=50)\n",
    "\n",
    "    alphas_lambdas = np.logspace(-7, -5, num=16)\n",
    "    threshold_lambdas = np.arange(5000, 15000, step=10)\n",
    "    powers = np.arange(0, 4)\n",
    "    losses = ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "    cc = np.linspace(0.1, 1.5, 50)\n",
    "    epsilons = np.linspace(1, 2, 10)\n",
    "    huber_alphas = np.logspace(-4, -1, num=20)\n",
    "    quantiles = [0.25, 0.5, 0.75]\n",
    "    q_alphas = np.linspace(0.25, 2, 20)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # n_nonzero_coefs = np.arange(1, len(get_predictors(ds, pr_group)))\n",
    "    \n",
    "    regressors = [\n",
    "        # LinearRegression(),\n",
    "        # GridSearchCV(estimator=Ridge(random_state=rng), param_grid={\"alpha\": alphas}),\n",
    "        # GridSearchCV(estimator=Lasso(random_state=rng), param_grid={\"alpha\": alphas}),\n",
    "        # GridSearchCV(estimator=ElasticNet(random_state=rng), param_grid={\"alpha\": alphas, \"l1_ratio\": l1_ratio}, n_jobs=-1),\n",
    "        # Lars(n_nonzero_coefs=1),\n",
    "        # Lars(n_nonzero_coefs=2),\n",
    "        # Lars(n_nonzero_coefs=3),\n",
    "        # Lars(n_nonzero_coefs=4),\n",
    "        Lars(n_nonzero_coefs=5),\n",
    "        # Lars(n_nonzero_coefs=6),\n",
    "        # Lars(n_nonzero_coefs=7),\n",
    "        # Lars(n_nonzero_coefs=8),\n",
    "        # GridSearchCV(estimator=LassoLars(random_state=rng), param_grid={\"alpha\": alphas}),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=1),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=2),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=3),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=4),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=5),\n",
    "        # BayesianRidge(tol=1e-6),\n",
    "        # ARDRegression(),\n",
    "        # GridSearchCV(estimator=TweedieRegressor(), param_grid={\"power\": powers, \"alpha\": alphas}, n_jobs=-1),\n",
    "        # GridSearchCV(estimator=SGDRegressor(random_state=rng), param_grid={\"loss\": losses, \"alpha\": sgd_alphas}, n_jobs=-1),\n",
    "        # GridSearchCV(estimator=PassiveAggressiveRegressor(random_state=rng), param_grid={\"C\": cc}, n_jobs=-1, cv=3),\n",
    "        # HuberRegressor(),\n",
    "        # TheilSenRegressor(),\n",
    "        # GridSearchCV(estimator=QuantileRegressor(), param_grid={\"alpha\": q_alphas, \"quantile\": quantiles}, n_jobs=-1),\n",
    "        \n",
    "        #\"quantile\": quantiles,\n",
    "\n",
    "        ##PassiveAggressiveRegressor(random_state=rng),\n",
    "        ##GridSearchCV(estimator=HuberRegressor(), param_grid={\"epsilon\": epsilons, \"alpha\": huber_alphas}, n_jobs=-1),\n",
    "        ##GridSearchCV(estimator=BayesianRidge(tol=1e-6), param_grid={\"alpha_1\": alphas_lambdas, \"alpha_2\": alphas_lambdas, \"lambda_1\": alphas_lambdas, \"lambda_2\": alphas_lambdas}, n_jobs=-1),       \n",
    "        #GaussianProcessRegressor(kernel=RBF(length_scale=1.1) + WhiteKernel() + DotProduct(), random_state=0)\n",
    "        #GridSearchCV(Pipeline(steps=[('feature_selection', SelectKBest(r_regression, k=4)), ('regression', Lasso(random_state=rng))]), param_grid={\"regression__alpha\": alphas}),\n",
    "        #GridSearchCV(Pipeline(steps=[('feature_selection', SelectFromModel(Lars(n_nonzero_coefs=4))), ('regression', Lasso(random_state=rng))]), param_grid={\"regression__alpha\": alphas}),\n",
    "    ]\n",
    "\n",
    "    datasets = get_datasets()\n",
    "\n",
    "    fieldnames = ['Predictors', 'Equations', 'Method', 'Criterion', 'Correlation', 'Pm']\n",
    "\n",
    "    # datasets_result = {\n",
    "    #     \"hydropost_0\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    #     ...,\n",
    "    #     \"hydropost_n\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    # Итерация по датасетам\n",
    "    datasets_result = dict()\n",
    "    for ds in datasets:\n",
    "        result_list = []\n",
    "        \n",
    "        pr_list = get_predictors(ds, pr_group)\n",
    "        \n",
    "        X, y = get_river_dataset(f'{ds_dir}/{ds}.csv', pr_list=pr_list)\n",
    "\n",
    "        if n_test is not None and n_test != 0:\n",
    "            X_train, y_train, X_test, y_test = train_test_split(X, y, n_test)\n",
    "        else:\n",
    "            X_train = X[:]\n",
    "            y_train = y[:]\n",
    "            X_test = X_train\n",
    "            y_test = y_train\n",
    "\n",
    "        if norms:\n",
    "            norms = get_norms(ds)\n",
    "            X_test = test_norm(X_test, pr_list, norms)\n",
    "            \n",
    "        # Итерация по моделям регрессии\n",
    "        for name, regr in zip(names, regressors):\n",
    "            one_model_row = dict()\n",
    "\n",
    "            # try:\n",
    "            regr.fit(X_train, y_train)\n",
    "            y_predicted = regr.predict(X_test)\n",
    "            # except Exception:\n",
    "                # continue\n",
    "            \n",
    "            # pipe_params = regr.get_params()\n",
    "            # print(pipe_params)\n",
    "\n",
    "            try:\n",
    "                coef = regr.best_estimator_.coef_\n",
    "                intercept = regr.best_estimator_.intercept_\n",
    "                \n",
    "                if isinstance(intercept, np.ndarray):\n",
    "                    intercept = intercept[0]\n",
    "                \n",
    "                # coef = regr.best_estimator_.named_steps['regression'].coef_\n",
    "                # intercept = regr.best_estimator_.named_steps['regression'].intercept_\n",
    "                # print('cv_coef', coef)\n",
    "                print('cv_intercept', intercept, type(intercept))\n",
    "                # print(ds, regr.best_estimator_.alpha_)\n",
    "            except Exception as error:\n",
    "                coef = regr.coef_\n",
    "                intercept = regr.intercept_\n",
    "                \n",
    "                if isinstance(intercept, np.ndarray):\n",
    "                    intercept = intercept[0]\n",
    "                \n",
    "                # print('rg_coef', coef)\n",
    "                print('rg_intercept', intercept, type(intercept))\n",
    "                # print(ds, regr.alpha_)\n",
    "                print(error)\n",
    "                \n",
    "            # Коэффициенты уравнения (если есть)\n",
    "            print('COEF', coef)\n",
    "            try:\n",
    "                predictors_coef = {f: c for f, c in zip(pr_list, coef) if c != 0.0}\n",
    "                predictors = \", \".join(predictors_coef.keys())\n",
    "                #predictors = predictors.upper()\n",
    "                print(intercept, predictors_coef.items())\n",
    "                equation = str(round(intercept, 2)) + ' ' + ' '.join(str(round(c, 2))+'*'+f for f, c in predictors_coef.items())\n",
    "                equation = equation.replace(\" -\", \"-\")\n",
    "                equation = equation.replace(\" \", \" + \")\n",
    "                equation = equation.replace(\"-\", \" - \")\n",
    "                #equation = equation.upper()\n",
    "    \n",
    "                one_model_row['Predictors'] = predictors\n",
    "                one_model_row['Equations'] = equation\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                one_model_row['Predictors'] = \"\"\n",
    "                one_model_row['Equations'] = \"\"\n",
    "\n",
    "            # Название датасета\n",
    "            one_model_row['Dataset_name'] = ds\n",
    "\n",
    "            # Группа предикторов\n",
    "            one_model_row['Group'] = pr_group\n",
    "                \n",
    "            # Название метода\n",
    "            one_model_row['Method'] = name\n",
    "\n",
    "            # Расчет показателей качества по методике\n",
    "            \n",
    "            one_model_row['H_sum'] = get_sum(y)\n",
    "            one_model_row['H_max'] = get_max(y)\n",
    "            one_model_row['H_min'] = get_min(y)\n",
    "            \n",
    "            # Среднее значение максимального уровня по всей выборке\n",
    "            h_max_avg = get_hmax_avg(y)\n",
    "            one_model_row['H_avg'] = h_max_avg\n",
    "            \n",
    "            # Среднеквадратическое отклонение\n",
    "            sigma = get_sigma(y)\n",
    "            one_model_row['Sigma'] = sigma\n",
    "\n",
    "            # Допустимая погрешность прогноза\n",
    "            delta_dop = get_delta_dop(sigma)\n",
    "            one_model_row['Delta_dop'] = delta_dop\n",
    "\n",
    "            # Обеспеченность климатическая Pk\n",
    "            pk = get_pk(y_test, h_max_avg, delta_dop)\n",
    "            one_model_row['Pk'] = round(pk, 8)\n",
    "\n",
    "            # Обеспеченность метода (оправдываемость) Pm\n",
    "            pm = get_pm(y_test, y_predicted, delta_dop)\n",
    "            one_model_row['Pm'] = round(pm, 8)\n",
    "\n",
    "            # Среднеквадратическая погрешность прогноза\n",
    "            s_forecast = get_s(y_test, y_predicted)\n",
    "            one_model_row['S'] = s_forecast\n",
    "            \n",
    "            # Критерий эффективности метода прогнозирования климатический S/sigma\n",
    "            criterion_forecast = get_criterion(s_forecast, sigma)\n",
    "            one_model_row['Criterion'] = criterion_forecast\n",
    "\n",
    "            # Критерий эффективности метода прогнозирования климатический S/sigma в квадрате\n",
    "            criterion_sqr = get_criterion(s_forecast, sigma) ** 2.0\n",
    "            one_model_row['Criterion_sqr'] = criterion_sqr\n",
    "            \n",
    "            # Корреляционное отношение ro\n",
    "            correlation_forecast = get_correlation_ratio(criterion_forecast)\n",
    "            one_model_row['Correlation'] = correlation_forecast\n",
    "                        \n",
    "            # Model\n",
    "            one_model_row['Model'] = regr\n",
    "\n",
    "            # models_list.append(one_model_row)\n",
    "            result_list.append(one_model_row)\n",
    "\n",
    "        # Сортировка результатов по каждому датасету\n",
    "        result_list.sort(key=lambda row: (row['Criterion'], -row['Correlation'], -row['Pm']))\n",
    "\n",
    "        datasets_result[ds] = result_list\n",
    "\n",
    "        # Запись в .csv файл\n",
    "        write_dataset_csv(result_list, ds, fieldnames, pr_group=pr_group)\n",
    "\n",
    "        for i, rl in enumerate(result_list):\n",
    "            if top_best is not None:\n",
    "                if i >= top_best:\n",
    "                    break\n",
    "            verify_forecast(ds, rl, i, pr_group=pr_group, n_test=n_test)\n",
    "\n",
    "    return datasets_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d848a-fd25-4827-989d-013f2e6d1fbd",
   "metadata": {},
   "source": [
    "#### Функция формирования проверочных прогнозов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7d9ced3-0d12-460b-8cd3-6b030f713b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_forecast(dataset_name, rl, num, pr_group, n_test=None, norms=True):\n",
    "\n",
    "    ds_dir = 'data' # В константы\n",
    "\n",
    "    pr_list = get_predictors(dataset_name, pr_group)\n",
    "    pr_list = ['year'] + pr_list\n",
    "    \n",
    "    fieldnames = [\n",
    "        '№', 'Год',\n",
    "        'Hmax фактический', 'Hф-Hср', '(Hф-Hср)^2', 'Погрешность климатических прогнозов в долях от допустимой погрешности',\n",
    "        'Hmax прогнозный', 'Hф-Hп', '(Hф-Hп)^2', 'Погрешность проверочных прогнозов в долях от допустимой погрешности',\n",
    "    ]\n",
    "\n",
    "    # fieldnames = [\n",
    "    #     'Year',\n",
    "    #     'Hmax fact', 'Hf-Havg', '(Hf-Havg)^2', 'Error climate',\n",
    "    #     'Hmax forecast', 'Hf-Hfor', '(Hf-Hfor)^2', 'Error forecast',\n",
    "    # ]\n",
    "\n",
    "    X, y = get_river_dataset(f'{ds_dir}/{dataset_name}.csv', pr_list=pr_list, y_name='H_max')\n",
    "\n",
    "    if n_test is not None and n_test != 0:\n",
    "        _, _, X_test, y_test = train_test_split(X, y, n_test)\n",
    "    else:\n",
    "        X_test = X\n",
    "        y_test = y\n",
    "\n",
    "    if norms:\n",
    "        norms = get_norms(dataset_name)\n",
    "        X_test = test_norm(X_test, pr_list, norms)\n",
    "\n",
    "    # Выделение первой колонки (года) из набора предикторов\n",
    "    years = X_test[:, 0]\n",
    "    X_test = X_test[:, 1:]\n",
    "    \n",
    "    # Forecast\n",
    "    #h_max_forecast = np.around(model.predict(X_test))\n",
    "    h_max_forecast = rl['Model'].predict(X_test)\n",
    "    \n",
    "    # Hсредний\n",
    "    #h_max_avg = np.around(np.mean(y))\n",
    "    h_max_avg = np.mean(y)\n",
    "\n",
    "    # H - Hсредний\n",
    "    diff_fact = y_test - h_max_avg\n",
    "\n",
    "    # (H - Hсредний) в квадрате\n",
    "    diff_fact_sqr = diff_fact ** 2\n",
    "\n",
    "    # Погрешность климатических прогнозов в долях от допустимой погрешности\n",
    "    delta_dop = get_delta_dop(get_sigma(y))\n",
    "    error_climate = np.around(get_delta50(y_test, delta_dop, h_max_avg=h_max_avg), decimals=2)\n",
    "\n",
    "    # H - Hпрогнозный\n",
    "    diff_forecast = y_test - h_max_forecast\n",
    "\n",
    "    # (H - Hпрогнозный) в квадрате\n",
    "    diff_forecast_sqr = diff_forecast ** 2       \n",
    "\n",
    "    # Погрешность проверочных прогнозов в долях от допустимой погрешности\n",
    "    error_forecast = np.around(get_delta50(y_test, delta_dop, h_max_forecast=h_max_forecast), decimals=2)\n",
    "\n",
    "    # Номер по порядку\n",
    "    rows_num = y_test.shape[0]\n",
    "    npp = np.arange(1, rows_num + 1, 1)\n",
    "\n",
    "    # Конкатенация массивов\n",
    "    att_tuple = (npp, years, y_test, diff_fact, diff_fact_sqr, error_climate, h_max_forecast, diff_forecast, diff_forecast_sqr, error_forecast)\n",
    "    arr = np.column_stack(att_tuple)\n",
    "    arr = arr.tolist()\n",
    "\n",
    "    # Обеспеченность метода (оправдываемость) Pm\n",
    "    pm = get_pm(y_test, h_max_forecast, delta_dop)\n",
    "    \n",
    "    # Запись проверочного прогноза в csv файл\n",
    "    with open(f'results/{dataset_name}/group-{pr_group}/{dataset_name}-проверочный-гр{pr_group}-{num:0>2}.csv', 'w', newline='', encoding='utf-8') as csvfile: #, encoding='utf-8'\n",
    "        stat_str = (\n",
    "            f\"Таблица  - Проверочные прогнозы максимумов весеннего\\n\"\n",
    "            f\"половодья р.{rl['Dataset_name']} по данным, рассчитанным\\n\"\n",
    "            f\"на дату 28 февраля\\n\"\n",
    "            f\"Предикторы;; {rl['Predictors']}\\n\"\n",
    "            f\"Уравнение;; {rl['Equations']}\\n\\n\"\n",
    "        )\n",
    "        csvfile.write(stat_str)\n",
    "       \n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(arr)\n",
    "        \n",
    "      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48df2181-e85a-449e-bbd2-41e64908cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rg_intercept 103.11257392554633 <class 'numpy.float64'>\n",
      "'Lars' object has no attribute 'best_estimator_'\n",
      "COEF [ 0.48370686 -0.00447091  0.28625128  0.18275136  0.18883031]\n",
      "103.11257392554633 dict_items([('Smax', 0.4837068649353534), ('H', -0.0044709076676901945), ('X', 0.2862512760592987), ('X1', 0.18275136347701001), ('X3', 0.18883030687342467)])\n",
      "Неман-Столбцы\n"
     ]
    }
   ],
   "source": [
    "top_best = 3\n",
    "result = compare_models(pr_group=1, n_test=0, norms=True, top_best=top_best)\n",
    "for key in result:\n",
    "    print(key)\n",
    "    for num, r in enumerate(result[key]):\n",
    "        dataset_name = r['Dataset_name']\n",
    "        pr_group = r['Group']\n",
    "        stat_str = (\n",
    "            f\"Сумма;;{r['H_sum']}\\n\"  \n",
    "            f\"Средний;;{r['H_avg']}\\n\" \n",
    "            f\"Высший;;{r['H_max']}\\n\"\n",
    "            f\"Низший;;{r['H_min']}\\n\\n\"\n",
    "            \n",
    "            f\"σ = ;;{r['Sigma']};;σ -;среднеквадратическое отклонение (см)\\n\" \n",
    "            f\"δдоп =;;{r['Delta_dop']};;δдоп -;допустимая погрешность прогноза (см)\\n\" \n",
    "            f\"Pк =;;{r['Pk']};;Pк -;климатическая обеспеченность в %\\n\"\n",
    "            f\"Pм =;;{r['Pm']};;Pм -;обеспеченность метода (допустимой погрешности проверочных прогнозов) в %\\n\"\n",
    "            f\"S =;;{r['S']};;S -;среднеквадратическая погрешность (см)\\n\"\n",
    "            f\"S/σ =;;{r['Criterion']};;S/σ -;критерий эффективности метода прогнозирования\\n\" \n",
    "            f\"(S/σ)^2 =;;{r['Criterion_sqr']};;ρ -;корреляционное отношение\\n\"\n",
    "            f\"ρ =;;{r['Correlation']};;;(оценка эффективности метода прогнозирования)\\n\"\n",
    "            f\"Модель:;;{r['Method']};;δ50% -;погрешность (ошибка) прогнозов (см)\\n\"\n",
    "            \n",
    "            \n",
    "            # f\"Sigma;{r['Sigma']},\\n\" \n",
    "            # f\"Delta_dop;{r['Delta_dop']},\\n\" \n",
    "            # f\"Pk;{r['Pk']},\\n\"\n",
    "            # f\"Pm;{r['Pm']},\\n\"\n",
    "            # f\"S;{r['S']},\\n\"\n",
    "            # f\"Criterion;{r['Criterion']},\\n\" \n",
    "            # f\"Criterion^2;{r['Criterion_sqr']},\\n\"\n",
    "            # f\"Correlation;{r['Correlation']},\\n\"            \n",
    "            # f\"Модель:; {r['Method']}\\n\"\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        with open(f'results/{dataset_name}/group-{pr_group}/{dataset_name}-проверочный-гр{pr_group}-{num:0>2}.csv', 'a', newline='', encoding='utf-8') as csvfile: # , encoding='utf-8'\n",
    "            csvfile.write(stat_str)\n",
    "\n",
    "        \n",
    "        #print(stat_str)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d4e9e82-4391-4b5b-9a19-89386105288e",
   "metadata": {},
   "source": [
    "# Получить лучшую модель по датасету с индексом [0] из результатов функции обучения и оценки моделей\n",
    "dataset_name = 'Неман-Белица'\n",
    "ds_row = result[dataset_name][0]\n",
    "print(f\"{ds_row['Method']:>16}: Criterion={ds_row['Criterion']}, Correlation={ds_row['Correlation']}, Pm={ds_row['Pm']}, Model={ds_row['Model']}\")\n",
    "\n",
    "best_dataset_model = ds_row['Model']\n",
    "verify_forecast(dataset_name, best_dataset_model, n_test=None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42690a40-dbd6-43e0-833b-8cb84748377a",
   "metadata": {},
   "source": [
    "'LinearRegression' object has no attribute 'best_estimator_'\n",
    "COEF [1.19843309 0.51831696 0.57726038 0.48136935 0.23850776]\n",
    "14.460842336844678 dict_items([('Smax', 1.1984330909697833), ('H', 0.5183169584386271), ('X', 0.5772603790216918), ('X1', 0.4813693488543986), ('X3', 0.23850775841551075)])\n",
    "'LinearRegression' object has no attribute 'best_estimator_'\n",
    "COEF [2.09387104 0.39172162 0.05263355 0.55552737 0.37234925]\n",
    "49.63060861327068 dict_items([('Smax', 2.0938710396145406), ('H', 0.3917216174794405), ('X', 0.05263354695944203), ('X1', 0.5555273693710512), ('X3', 0.372349247641367)])\n",
    "Неман-Белица\n",
    "Неман-Гродно"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
