{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c8031c-3e29-4149-aaa3-ef3f3f0d444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cff566a-8886-47b1-a651-f3df2c864e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_river_dataset(fname, pr_list=None, y_name='H_max'):\n",
    "    pr_arr = []\n",
    "    y_arr = []\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            pr_arr_row = []\n",
    "            for pr in pr_list:\n",
    "                pr_arr_row.append(row[pr])\n",
    "\n",
    "            pr_arr.append(pr_arr_row)\n",
    "            y_arr.append(row[y_name])\n",
    "    X = np.asarray(pr_arr, dtype=np.float64)\n",
    "    y = np.asarray(y_arr, dtype=np.float64)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a96a1-c37d-47ba-b369-2a015ec5706f",
   "metadata": {},
   "source": [
    "#### Сумма, средний, высший, низший уровни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa78832-4724-42e1-8e4f-8513406d88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(h_max):\n",
    "    return np.sum(h_max)\n",
    "    \n",
    "def get_avg(h_max):\n",
    "    return np.mean(h_max)\n",
    "    \n",
    "def get_max(h_max):\n",
    "    return np.amax(h_max)\n",
    "    \n",
    "def get_min(h_max):\n",
    "    return np.amin(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dafa4-e1f7-4c26-b28d-3dfd6a2b1b14",
   "metadata": {},
   "source": [
    "#### Среднеквадратическая погрешность прогноза S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad85fc4-c2cd-4a9f-8a0a-bb4fe4cdfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s(h_max, h_forecast=None):\n",
    "    # Среднеквадратическая погрешность прогноза\n",
    "    n = h_max.shape[0]\n",
    "    sqr_diff = np.sum((h_max - h_forecast) ** 2) / (n - 1)\n",
    "    std = sqr_diff ** 0.5\n",
    "    return std    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e98c4-4841-4b3f-b61b-284bc99af53d",
   "metadata": {},
   "source": [
    "#### Среднеквадратическое отклонение sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5079b8f9-3ae5-47a1-a6f3-214bdf69ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma(h_max):\n",
    "    # Среднеквадратическая погрешность климатическая.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.std(h_max, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71fe5ef3-810d-40e8-87f0-9432f139319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hmax_avg(h_max):\n",
    "    # Среднее значение h_max.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.mean(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10fa8d-d7cd-44f4-b4f2-4f652409383f",
   "metadata": {},
   "source": [
    "#### Допустимая погрешность прогноза delta_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d4648d-8e14-4c63-bcf8-6a3c2cb7a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_dop(sigma):\n",
    "    return 0.674 * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1c7ff-c4e1-4639-a877-d3b78d944dea",
   "metadata": {},
   "source": [
    "#### Критерий эффективности метода прогнозирования климатический S/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92739a9-3b12-44bd-b51e-4ffd73e828a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(s, sigma):\n",
    "    return s / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec5970-ba12-4121-8403-cd13b812b2de",
   "metadata": {},
   "source": [
    "#### Климатическая обеспеченность Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e1b890-8fb7-4fdd-8196-610544acee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pk(h_max, h_max_avg, delta_dop):\n",
    "    diff = np.abs(h_max - h_max_avg) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4c686-2946-4ea0-980a-172b2ff4dd64",
   "metadata": {},
   "source": [
    "#### Обеспеченность метода (оправдываемость) Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68ca5adf-fbf3-49a4-8458-06d103bfacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pm(h_max, h_forecast, delta_dop):\n",
    "    diff = np.abs(h_max - h_forecast) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d93f0-e243-4107-b007-cb24efbdc6f6",
   "metadata": {},
   "source": [
    "#### Корреляционное отношение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a501afb2-3736-4023-a488-a19747e54910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_ratio(criterion):\n",
    "    c_1 = (1 - criterion ** 2)\n",
    "    ro = c_1 ** 0.5 if c_1 > 0 else 0\n",
    "    return ro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059ec96-bf22-4b2b-83f9-dc2112e0c942",
   "metadata": {},
   "source": [
    "#### Вероятная ошибка прогноза S'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e88b65-02b2-4bee-9262-b84a7a1ea70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forecast_error(s):\n",
    "    return 0.674 * s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0abe2-cca4-4ebc-b6bf-f27c01c6b1bd",
   "metadata": {},
   "source": [
    "#### Ошибки климатического/природного прогноза для каждого года delta50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b00b24-686d-449b-9920-aa4733a0bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta50(h_max, delta_dop, h_max_avg=None, h_max_forecast=None):\n",
    "    if h_max_forecast is None:\n",
    "        # delta50 климатическая\n",
    "        return (h_max - h_max_avg) / delta_dop\n",
    "    else:\n",
    "        # delta50 прогноза\n",
    "        return (h_max - h_max_forecast) / delta_dop\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0cc72-ec19-437b-b6c5-8010c2e3a5bf",
   "metadata": {},
   "source": [
    "#### Функция записи в csv файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c76ad0-d75c-4746-90dc-0d8465218463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_dataset_csv(dataset, dataset_name, fieldnames, pr_group):\n",
    "    with open(f'results/{dataset_name}/group-{pr_group}/{dataset_name}-гр{pr_group}.csv', 'w', newline='', encoding='utf-8') as csvfile:# , encoding='utf-8'\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=';', extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2d2a0-f9da-46e5-a0ae-45e8fe0c2482",
   "metadata": {},
   "source": [
    "#### Функция разделения набора данных на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82db9e50-fadc-4c7f-9151-58c2b74df8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, n_test):\n",
    "    X_train = X[:-n_test]\n",
    "    y_train = y[:-n_test]\n",
    "    X_test = X[-n_test:]\n",
    "    y_test = y[-n_test:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719a5f9-176e-473d-8ce7-b7744d9c6e19",
   "metadata": {},
   "source": [
    "#### Функция формирования тестового набора данных с подстановкой нормированных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b464d45-843c-4792-ab02-2819edddee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_norm(x, pr_list, norms):\n",
    "    x_norm = np.copy(x)\n",
    "    for col, pr in enumerate(pr_list):\n",
    "        if pr in norms:\n",
    "            x_norm[:, col:col+1] = norms[pr]\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3a942-95d0-4f7f-aa39-82f61624b070",
   "metadata": {},
   "source": [
    "#### Функция получения датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7628a977-9d54-4129-928c-625b4e80d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    datasets = {\n",
    "        'Неман-Белица': 'Неман',\n",
    "        # 'Неман-Гродно': 'Неман',\n",
    "        # 'Неман-Мосты': 'Неман',\n",
    "        # 'Неман-Столбцы': 'Неман',\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83168b0-64e2-4e58-a47b-f49f677a9eb4",
   "metadata": {},
   "source": [
    "#### Функция получения списка предикторов по названию датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca1eea5-a699-4b0e-91fa-9fe6f4451ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictors(dataset_name, pr_group):\n",
    "\n",
    "    datasets = get_datasets()   \n",
    "    predictors_lists = {\n",
    "        'Неман': (\n",
    "            ['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'X3', 'Xs'],\n",
    "            ['Smax', 'H_2802', 'X', 'X1', 'X3'],\n",
    "            ['S_2802', 'H_2802', 'X2', 'X3', 'Xs'],\n",
    "        )\n",
    "    }\n",
    "    return predictors_lists[datasets[dataset_name]][pr_group]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a741f9dd-31c9-4ea6-a1a8-b2b684b05013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norms(dataset_name):\n",
    "    norms_list = {\n",
    "        'Неман-Белица': {'X1': 46.0, 'X2':35.0},\n",
    "        'Неман-Гродно': {'X1': 36.0, 'X2':26.0},\n",
    "        'Неман-Мосты': {'x1': 40.0, 'x2':31.0},\n",
    "        'Неман-Столбцы': {'x1': 43.0, 'x2':34.0},\n",
    "    }\n",
    "    return norms_list[dataset_name]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecbd695a-3c19-44bf-ab0f-49cccc3686dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-02, 1.12332403e-02, 1.26185688e-02, 1.41747416e-02,\n",
       "       1.59228279e-02, 1.78864953e-02, 2.00923300e-02, 2.25701972e-02,\n",
       "       2.53536449e-02, 2.84803587e-02, 3.19926714e-02, 3.59381366e-02,\n",
       "       4.03701726e-02, 4.53487851e-02, 5.09413801e-02, 5.72236766e-02,\n",
       "       6.42807312e-02, 7.22080902e-02, 8.11130831e-02, 9.11162756e-02,\n",
       "       1.02353102e-01, 1.14975700e-01, 1.29154967e-01, 1.45082878e-01,\n",
       "       1.62975083e-01, 1.83073828e-01, 2.05651231e-01, 2.31012970e-01,\n",
       "       2.59502421e-01, 2.91505306e-01, 3.27454916e-01, 3.67837977e-01,\n",
       "       4.13201240e-01, 4.64158883e-01, 5.21400829e-01, 5.85702082e-01,\n",
       "       6.57933225e-01, 7.39072203e-01, 8.30217568e-01, 9.32603347e-01,\n",
       "       1.04761575e+00, 1.17681195e+00, 1.32194115e+00, 1.48496826e+00,\n",
       "       1.66810054e+00, 1.87381742e+00, 2.10490414e+00, 2.36448941e+00,\n",
       "       2.65608778e+00, 2.98364724e+00, 3.35160265e+00, 3.76493581e+00,\n",
       "       4.22924287e+00, 4.75081016e+00, 5.33669923e+00, 5.99484250e+00,\n",
       "       6.73415066e+00, 7.56463328e+00, 8.49753436e+00, 9.54548457e+00,\n",
       "       1.07226722e+01, 1.20450354e+01, 1.35304777e+01, 1.51991108e+01,\n",
       "       1.70735265e+01, 1.91791026e+01, 2.15443469e+01, 2.42012826e+01,\n",
       "       2.71858824e+01, 3.05385551e+01, 3.43046929e+01, 3.85352859e+01,\n",
       "       4.32876128e+01, 4.86260158e+01, 5.46227722e+01, 6.13590727e+01,\n",
       "       6.89261210e+01, 7.74263683e+01, 8.69749003e+01, 9.77009957e+01,\n",
       "       1.09749877e+02, 1.23284674e+02, 1.38488637e+02, 1.55567614e+02,\n",
       "       1.74752840e+02, 1.96304065e+02, 2.20513074e+02, 2.47707636e+02,\n",
       "       2.78255940e+02, 3.12571585e+02, 3.51119173e+02, 3.94420606e+02,\n",
       "       4.43062146e+02, 4.97702356e+02, 5.59081018e+02, 6.28029144e+02,\n",
       "       7.05480231e+02, 7.92482898e+02, 8.90215085e+02, 1.00000000e+03])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-2, 3, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08312297-9ca3-4e7d-be60-26969a4611e7",
   "metadata": {},
   "source": [
    "#### Функция обучения и оценки моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dff9414-8a26-4393-b480-446307aafc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(pr_group, n_test=None, norms=True, top_best=None):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.linear_model import Lars\n",
    "    from sklearn.linear_model import LassoLars\n",
    "    from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    from sklearn.linear_model import ARDRegression\n",
    "    from sklearn.linear_model import TweedieRegressor\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "    from sklearn.linear_model import HuberRegressor\n",
    "    from sklearn.linear_model import TheilSenRegressor\n",
    "    from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "    import scipy.stats as stats\n",
    "    from sklearn.pipeline import Pipeline, make_pipeline\n",
    "    from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "    from sklearn.feature_selection import r_regression  \n",
    "    \n",
    "    ds_dir = 'data'\n",
    "    \n",
    "    names = [\n",
    "        'LinearRegression',\n",
    "        'Ridge',\n",
    "        'Lasso',\n",
    "        'ElasticNet',\n",
    "        'Lars1',\n",
    "        'Lars2',\n",
    "        'Lars3',\n",
    "        'Lars4',\n",
    "        'Lars5',\n",
    "        'LassoLars',\n",
    "        'OMP1',\n",
    "        'OMP2',\n",
    "        'OMP3',\n",
    "        'OMP4',\n",
    "        'OMP5',\n",
    "        'BayesianRidge',\n",
    "        'ARDRegression',\n",
    "        'SGDRegressor', \n",
    "        'PassiveAggressiveRegressor',\n",
    "        'HuberRegressor',\n",
    "        'TheilSenRegressor',\n",
    "        'QuantileRegressor',               \n",
    "    ]\n",
    "\n",
    "    # Инициализация генератора случайных чисел для\n",
    "    # для обеспечения воспроизводимости результатов\n",
    "    rng = np.random.RandomState(0)\n",
    "\n",
    "    # Наборы гиперпараметров моделей для алгоритма кроссвалидации\n",
    "    # Гиперпараметры для Ridge, Lasso, ElasticNet, LassoLars, HuberRegressor\n",
    "    alphas = np.logspace(-4, 3, num=100)\n",
    "    # Гиперпараметры для ElasticNet\n",
    "    l1_ratio = np.linspace(0.01, 1.0, num=50)\n",
    "    # Гиперпараметры для BayesianRidge\n",
    "    alphas_init = np.linspace(0.5, 2, 5)\n",
    "    lambdas_init = np.logspace(-3, 1, num=5)\n",
    "    # Гиперпараметры для ARDRegression\n",
    "    alphas_lambdas = np.logspace(-7, -4, num=4)\n",
    "    # Гиперпараметры для SGDRegressor\n",
    "    losses = ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "    sgd_alphas = np.logspace(-4, 1, num=100)\n",
    "    # Гиперпараметры для PassiveAggressiveRegressor\n",
    "    cc = np.linspace(0.1, 1.5, 50)\n",
    "    # Гиперпараметры для HuberRegressor\n",
    "    epsilons = np.append(np.linspace(1.1, 2.0, 10), [1.35])\n",
    "    # Гиперпараметры для TheilSenRegressor\n",
    "    n_subsamples = np.arange(6, 24)\n",
    "    # Гиперпараметры для QuantileRegressor\n",
    "    q_alphas = np.linspace(0, 2, 20)    \n",
    "           \n",
    "    regressors = [\n",
    "        LinearRegression(),\n",
    "        GridSearchCV(estimator=Ridge(random_state=rng), param_grid={\"alpha\": alphas}),\n",
    "        GridSearchCV(estimator=Lasso(random_state=rng), param_grid={\"alpha\": alphas}),\n",
    "        GridSearchCV(estimator=ElasticNet(random_state=rng), param_grid={\"alpha\": alphas, \"l1_ratio\": l1_ratio}, n_jobs=-1),\n",
    "        Lars(n_nonzero_coefs=1),\n",
    "        Lars(n_nonzero_coefs=2),\n",
    "        Lars(n_nonzero_coefs=3),\n",
    "        Lars(n_nonzero_coefs=4),\n",
    "        Lars(n_nonzero_coefs=5),\n",
    "        GridSearchCV(estimator=LassoLars(random_state=rng), param_grid={\"alpha\": alphas}),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=1),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=2),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=3),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=4),\n",
    "        OrthogonalMatchingPursuit(n_nonzero_coefs=5),\n",
    "        GridSearchCV(estimator=BayesianRidge(), param_grid={\"alpha_init\": alphas_init, \"lambda_init\": lambdas_init}, n_jobs=-1),\n",
    "        GridSearchCV(estimator=ARDRegression(), param_grid={\"alpha_1\": alphas_lambdas, \"alpha_2\": alphas_lambdas,\"lambda_1\": alphas_lambdas,\"lambda_2\": alphas_lambdas}, n_jobs=-1),\n",
    "        GridSearchCV(estimator=SGDRegressor(random_state=rng), param_grid={\"loss\": losses, \"alpha\": sgd_alphas}, n_jobs=-1),\n",
    "        GridSearchCV(estimator=PassiveAggressiveRegressor(random_state=rng), param_grid={\"C\": cc}, n_jobs=-1, cv=3),\n",
    "        GridSearchCV(estimator=HuberRegressor(), param_grid={\"epsilon\": epsilons, \"alpha\": alphas}, n_jobs=-1, cv=5),\n",
    "        GridSearchCV(estimator=TheilSenRegressor(random_state=rng), param_grid={\"n_subsamples\": n_subsamples}, n_jobs=-1, cv=5),\n",
    "        GridSearchCV(estimator=QuantileRegressor(), param_grid={\"alpha\": q_alphas}, n_jobs=-1),\n",
    "    ]\n",
    "\n",
    "    datasets = get_datasets()\n",
    "\n",
    "    fieldnames = ['Predictors', 'Equations', 'Method', 'Criterion', 'Correlation', 'Pm']\n",
    "\n",
    "    # Описание структуры данных переменной datasets_result\n",
    "    # datasets_result = {\n",
    "    #     \"hydropost_0\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    #     ...,\n",
    "    #     \"hydropost_n\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    # Итерация по датасетам\n",
    "    datasets_result = dict()\n",
    "    for ds in datasets:\n",
    "        result_list = []\n",
    "        \n",
    "        pr_list = get_predictors(ds, pr_group)\n",
    "        \n",
    "        X, y = get_river_dataset(f'{ds_dir}/{ds}.csv', pr_list=pr_list)\n",
    "\n",
    "        if n_test is not None and n_test != 0:\n",
    "            X_train, y_train, X_test, y_test = train_test_split(X, y, n_test)\n",
    "        else:\n",
    "            X_train = X[:]\n",
    "            y_train = y[:]\n",
    "            X_test = X_train\n",
    "            y_test = y_train\n",
    "\n",
    "        if norms:\n",
    "            norms = get_norms(ds)\n",
    "            X_test = test_norm(X_test, pr_list, norms)\n",
    "            \n",
    "        # Итерация по моделям регрессии\n",
    "        for name, regr in zip(names, regressors):\n",
    "            one_model_row = dict()\n",
    "\n",
    "            regr.fit(X_train, y_train)\n",
    "            try:\n",
    "                regr.transorm(X_train, np.reshape(y_train, (y_train.shape[0], 1)))\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "            y_predicted = np.ravel(regr.predict(X_test))\n",
    "            print(y_predicted)            \n",
    "\n",
    "            try:\n",
    "                coef = regr.best_estimator_.coef_\n",
    "                intercept = regr.best_estimator_.intercept_\n",
    "                \n",
    "                if isinstance(intercept, np.ndarray):\n",
    "                    intercept = intercept[0]\n",
    "                print('cv_intercept', intercept, type(intercept))\n",
    "            except Exception as error:\n",
    "                \n",
    "                coef = regr.coef_\n",
    "                intercept = regr.intercept_\n",
    "                \n",
    "                if isinstance(intercept, np.ndarray):\n",
    "                    intercept = intercept[0]\n",
    "                print('rg_intercept', intercept, type(intercept))\n",
    "                print(error)\n",
    "                \n",
    "            # Коэффициенты уравнения (если есть)\n",
    "            print('COEF', coef)\n",
    "            coef = np.around(np.ravel(coef), 3)\n",
    "            intercept = round(intercept, 3)\n",
    "            try:\n",
    "                predictors_coef = {f: c for f, c in zip(pr_list, coef) if c != 0.0}\n",
    "                predictors = \", \".join(predictors_coef.keys())\n",
    "                print(intercept, predictors_coef.items())\n",
    "                equation = str(intercept) + ' ' + ' '.join(str(c) + '*' + f for f, c in predictors_coef.items())\n",
    "                equation = equation.replace(\" -\", \"-\")\n",
    "                equation = equation.replace(\" \", \" + \")\n",
    "                equation = equation.replace(\"-\", \" - \")\n",
    "    \n",
    "                one_model_row['Predictors'] = predictors\n",
    "                one_model_row['Equations'] = equation\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                one_model_row['Predictors'] = \"\"\n",
    "                one_model_row['Equations'] = \"\"\n",
    "\n",
    "            # Название датасета\n",
    "            one_model_row['Dataset_name'] = ds\n",
    "\n",
    "            # Группа предикторов\n",
    "            one_model_row['Group'] = pr_group\n",
    "                \n",
    "            # Название метода\n",
    "            one_model_row['Method'] = name\n",
    "\n",
    "            # Расчет показателей качества по методике\n",
    "\n",
    "            # Сумма, максимум, минимум максимальных уровней\n",
    "            one_model_row['H_sum'] = get_sum(y)\n",
    "            one_model_row['H_max'] = get_max(y)\n",
    "            one_model_row['H_min'] = get_min(y)\n",
    "            \n",
    "            # Среднее значение максимального уровня по всей выборке\n",
    "            h_max_avg = get_hmax_avg(y)\n",
    "            one_model_row['H_avg'] = h_max_avg\n",
    "            \n",
    "            # Среднеквадратическое отклонение\n",
    "            sigma = get_sigma(y)\n",
    "            one_model_row['Sigma'] = sigma\n",
    "\n",
    "            # Допустимая погрешность прогноза\n",
    "            delta_dop = get_delta_dop(sigma)\n",
    "            one_model_row['Delta_dop'] = delta_dop\n",
    "\n",
    "            # Обеспеченность климатическая Pk\n",
    "            pk = get_pk(y_test, h_max_avg, delta_dop)\n",
    "            one_model_row['Pk'] = pk\n",
    "\n",
    "            # Обеспеченность метода (оправдываемость) Pm\n",
    "            pm = get_pm(y_test, y_predicted, delta_dop)\n",
    "            one_model_row['Pm'] = pm\n",
    "\n",
    "            # Среднеквадратическая погрешность прогноза\n",
    "            s_forecast = get_s(y_test, y_predicted)\n",
    "            one_model_row['S'] = s_forecast\n",
    "            \n",
    "            # Критерий эффективности метода прогнозирования климатический S/sigma\n",
    "            criterion_forecast = get_criterion(s_forecast, sigma)\n",
    "            one_model_row['Criterion'] = criterion_forecast\n",
    "\n",
    "            # Критерий эффективности метода прогнозирования климатический S/sigma в квадрате\n",
    "            criterion_sqr = get_criterion(s_forecast, sigma) ** 2.0\n",
    "            one_model_row['Criterion_sqr'] = criterion_sqr\n",
    "            \n",
    "            # Корреляционное отношение ro\n",
    "            correlation_forecast = get_correlation_ratio(criterion_forecast)\n",
    "            one_model_row['Correlation'] = correlation_forecast\n",
    "                        \n",
    "            # Обученная модель\n",
    "            one_model_row['Model'] = regr\n",
    "\n",
    "            # Добавление результатов модели в результирующий список по датасету\n",
    "            result_list.append(one_model_row)\n",
    "\n",
    "        # Сортировка результатов по каждому датасету\n",
    "        result_list.sort(key=lambda row: (row['Criterion'], -row['Correlation'], -row['Pm']))\n",
    "\n",
    "        datasets_result[ds] = result_list\n",
    "\n",
    "        # Запись в .csv файл\n",
    "        write_dataset_csv(result_list, ds, fieldnames, pr_group=pr_group)\n",
    "\n",
    "        for i, rl in enumerate(result_list):\n",
    "            if top_best is not None:\n",
    "                if i >= top_best:\n",
    "                    break\n",
    "            verify_forecast(ds, rl, i, pr_group=pr_group, n_test=n_test)\n",
    "\n",
    "    return datasets_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d848a-fd25-4827-989d-013f2e6d1fbd",
   "metadata": {},
   "source": [
    "#### Функция формирования проверочных прогнозов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7d9ced3-0d12-460b-8cd3-6b030f713b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_forecast(dataset_name, rl, num, pr_group, n_test=None, norms=True):\n",
    "\n",
    "    ds_dir = 'data'\n",
    "\n",
    "    pr_list = get_predictors(dataset_name, pr_group)\n",
    "    pr_list = ['year'] + pr_list\n",
    "    \n",
    "    fieldnames = [\n",
    "        '№', 'Год',\n",
    "        'Hmax фактический', 'Hф-Hср', '(Hф-Hср)^2', 'δ50% Погрешность климатических прогнозов в долях от допустимой погрешности',\n",
    "        'Hmax прогнозный', 'Hф-Hп', '(Hф-Hп)^2', 'δ50% Погрешность проверочных прогнозов в долях от допустимой погрешности',\n",
    "    ]\n",
    "\n",
    "    X, y = get_river_dataset(f'{ds_dir}/{dataset_name}.csv', pr_list=pr_list, y_name='H_max')\n",
    "\n",
    "    if n_test is not None and n_test != 0:\n",
    "        _, _, X_test, y_test = train_test_split(X, y, n_test)\n",
    "    else:\n",
    "        X_test = X\n",
    "        y_test = y\n",
    "\n",
    "    if norms:\n",
    "        norms = get_norms(dataset_name)\n",
    "        X_test = test_norm(X_test, pr_list, norms)\n",
    "\n",
    "    # Выделение первой колонки (года) из набора предикторов\n",
    "    years = X_test[:, 0]\n",
    "    X_test = X_test[:, 1:]\n",
    "    \n",
    "    # Forecast\n",
    "    h_max_forecast = np.ravel(rl['Model'].predict(X_test))\n",
    "    \n",
    "    # Hсредний\n",
    "    h_max_avg = np.mean(y)\n",
    "\n",
    "    # H - Hсредний\n",
    "    diff_fact = y_test - h_max_avg\n",
    "\n",
    "    # (H - Hсредний) в квадрате\n",
    "    diff_fact_sqr = diff_fact ** 2\n",
    "\n",
    "    # Погрешность климатических прогнозов в долях от допустимой погрешности\n",
    "    delta_dop = get_delta_dop(get_sigma(y))\n",
    "    error_climate = get_delta50(y_test, delta_dop, h_max_avg=h_max_avg)\n",
    "\n",
    "    # H - Hпрогнозный\n",
    "    diff_forecast = y_test - h_max_forecast\n",
    "\n",
    "    # (H - Hпрогнозный) в квадрате\n",
    "    diff_forecast_sqr = diff_forecast ** 2       \n",
    "\n",
    "    # Погрешность проверочных прогнозов в долях от допустимой погрешности\n",
    "    error_forecast = get_delta50(y_test, delta_dop, h_max_forecast=h_max_forecast)\n",
    "\n",
    "    # Номер по порядку\n",
    "    rows_num = y_test.shape[0]\n",
    "    npp = np.arange(1, rows_num + 1, 1)\n",
    "\n",
    "    # Конкатенация массивов\n",
    "    att_tuple = (npp, years, y_test, diff_fact, diff_fact_sqr, error_climate, h_max_forecast, diff_forecast, diff_forecast_sqr, error_forecast)\n",
    "    arr = np.column_stack(att_tuple)\n",
    "    arr = arr.tolist()\n",
    "\n",
    "    # Обеспеченность метода (оправдываемость) Pm\n",
    "    pm = get_pm(y_test, h_max_forecast, delta_dop)\n",
    "    \n",
    "    # Запись проверочного прогноза в csv файл\n",
    "    with open(f'results/{dataset_name}/group-{pr_group}/{dataset_name}-проверочный-гр{pr_group}-{num:0>2}.csv', 'w', newline='', encoding='utf-8') as csvfile: #, encoding='utf-8'\n",
    "        stat_header = (\n",
    "            f\"Таблица  - Проверочные прогнозы максимумов весеннего половодья\\n\"\n",
    "            f\"р.{rl['Dataset_name']}\\n\"\n",
    "            f\"Предикторы:;; {rl['Predictors']}\\n\"\n",
    "            f\"Уравнение:;; {rl['Equations']}\\n\"\n",
    "            f\"Модель:;; {rl['Method']}\\n\\n\"\n",
    "        )\n",
    "        csvfile.write(stat_header)\n",
    "       \n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow(fieldnames)\n",
    "        writer.writerows(arr)\n",
    "        \n",
    "        stat_footer = (\n",
    "            f\"Сумма;;{rl['H_sum']}\\n\"  \n",
    "            f\"Средний;;{rl['H_avg']}\\n\" \n",
    "            f\"Высший;;{rl['H_max']}\\n\"\n",
    "            f\"Низший;;{rl['H_min']}\\n\\n\"\n",
    "            \n",
    "            f\"σ = ;;{rl['Sigma']};;σ -;среднеквадратическое отклонение (см)\\n\" \n",
    "            f\"δдоп =;;{rl['Delta_dop']};;δдоп -;допустимая погрешность прогноза (см)\\n\" \n",
    "            f\"Pк =;;{rl['Pk']};;Pк -;климатическая обеспеченность в %\\n\"\n",
    "            f\"Pм =;;{rl['Pm']};;Pм -;обеспеченность метода в %\\n\"\n",
    "            f\"S =;;{rl['S']};;;(допустимой погрешности проверочных прогнозов)\\n\"\n",
    "            f\"S/σ =;;{rl['Criterion']};;S -;среднеквадратическая погрешность (см)\\n\" \n",
    "            f\"(S/σ)^2 =;;{rl['Criterion_sqr']};;S/σ -;критерий эффективности метода прогнозирования\\n\"\n",
    "            f\"ρ =;;{rl['Correlation']};;ρ -;корреляционное отношение\\n\"\n",
    "            f\";;;;;(оценка эффективности метода прогнозирования)\\n\"\n",
    "            f\";;;;δ50% -;погрешность (ошибка) прогнозов (см)\\n\"\n",
    "        )\n",
    "        \n",
    "        csvfile.write(stat_footer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae6e8e-07c7-4de1-b8cc-0ae599007b06",
   "metadata": {},
   "source": [
    "#### Запуск процесса обучения моделей, формирования наборов уравнений множественной регрессии и проверочных прогнозов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48df2181-e85a-449e-bbd2-41e64908cb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230.57709273 336.29979341 404.41211451 329.62040277 291.18228692\n",
      " 347.49006106 201.35250779 268.33968844 316.67756636 368.80163277\n",
      " 285.19084736 276.52326282 258.41700919 273.68197261 209.91025204\n",
      " 264.42392224 344.73354168 197.79099127 394.00229566 330.70612064\n",
      " 206.90080302 286.5420064  295.87340215 266.0842359  320.68584359\n",
      " 289.70699205 247.24861466 211.27134961 354.25936278 268.63990223\n",
      " 242.72503073 424.79856223 191.8517008  270.988241   282.68342503\n",
      " 304.29501038 325.91236089]\n",
      "rg_intercept 67.03603750643762 <class 'numpy.float64'>\n",
      "'LinearRegression' object has no attribute 'best_estimator_'\n",
      "COEF [1.33679606 0.13208292 0.51642084 0.50910961 0.30562716]\n",
      "67.036 dict_items([('Smax', 1.337), ('H_2802', 0.132), ('X', 0.516), ('X1', 0.509), ('X3', 0.306)])\n",
      "[236.70096856 331.02509363 400.17344344 316.19063327 293.94889353\n",
      " 340.84591599 216.07418936 261.18054494 308.02540419 361.23305649\n",
      " 279.19465696 266.02698023 258.39258685 283.1912762  216.91952466\n",
      " 259.66955565 345.03112447 217.36869562 380.88474206 333.37680823\n",
      " 215.3916076  287.04537797 303.61684407 269.53370796 306.57094188\n",
      " 300.14428453 229.23569458 215.01558588 358.33900103 277.64347924\n",
      " 248.69157434 413.7931265  180.37963505 294.31383946 297.39346714\n",
      " 298.0248496  316.63633672]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[42.34957558]\n",
      " [ 9.62588805]\n",
      " [19.23467911]\n",
      " [ 6.64184563]\n",
      " [17.01393239]]\n",
      "289.595 dict_items([('Smax', 42.35), ('H_2802', 9.626), ('X', 19.235), ('X1', 6.642), ('X3', 17.014)])\n",
      "[234.2980575  338.01500643 406.47619502 331.55906546 287.31571761\n",
      " 347.88742948 203.64317225 268.49133277 317.03587192 368.79407084\n",
      " 287.56434341 272.44538346 258.44434537 273.33715331 210.36224532\n",
      " 261.25308542 339.83655047 198.70257305 394.06913922 325.86252715\n",
      " 208.78485687 284.46146027 297.79578047 267.39123185 319.80902344\n",
      " 292.65453352 242.34327062 212.22692132 354.29145887 271.5405764\n",
      " 246.37728126 423.06429711 191.9544425  272.13003639 281.99453346\n",
      " 302.14991607 325.84295839]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[45.36896519]\n",
      " [ 4.68861008]\n",
      " [21.93472689]\n",
      " [15.55080926]\n",
      " [12.91291186]]\n",
      "289.595 dict_items([('Smax', 45.369), ('H_2802', 4.689), ('X', 21.935), ('X1', 15.551), ('X3', 12.913)])\n",
      "[230.07696531 336.58057526 404.93783735 329.66946781 291.18491513\n",
      " 347.59809404 201.81649958 268.33008128 317.23503843 369.18720076\n",
      " 285.924297   275.77362212 258.83187032 273.7057111  209.40409553\n",
      " 264.82057384 342.82449316 198.41464677 394.30499347 330.76942148\n",
      " 206.27323462 287.36789949 295.85143508 266.78945609 320.2082669\n",
      " 289.46212232 247.06084755 212.27497974 354.28645043 267.95198437\n",
      " 242.86366104 424.06616616 191.28061011 271.32423688 282.65440259\n",
      " 304.12356948 325.33206144]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[44.26077054]\n",
      " [ 3.89535989]\n",
      " [23.51477335]\n",
      " [16.61406658]\n",
      " [14.35672745]]\n",
      "289.595 dict_items([('Smax', 44.261), ('H_2802', 3.895), ('X', 23.515), ('X1', 16.614), ('X3', 14.357)])\n",
      "[230.57709273 336.29979341 404.41211451 329.62040277 291.18228692\n",
      " 347.49006106 201.35250779 268.33968844 316.67756636 368.80163277\n",
      " 285.19084736 276.52326282 258.41700919 273.68197261 209.91025204\n",
      " 264.42392224 344.73354168 197.79099127 394.00229566 330.70612064\n",
      " 206.90080302 286.5420064  295.87340215 266.0842359  320.68584359\n",
      " 289.70699205 247.24861466 211.27134961 354.25936278 268.63990223\n",
      " 242.72503073 424.79856223 191.8517008  270.988241   282.68342503\n",
      " 304.29501038 325.91236089]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[43.99571611]\n",
      " [ 4.39983567]\n",
      " [24.10041183]\n",
      " [16.72883917]\n",
      " [14.05327144]]\n",
      "289.595 dict_items([('Smax', 43.996), ('H_2802', 4.4), ('X', 24.1), ('X1', 16.729), ('X3', 14.053)])\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n",
      "[240.50719851 355.95155668 421.50512734 293.38287229 287.4717346\n",
      " 361.17599686 232.22101666 281.56675445 309.03499136 387.06511521\n",
      " 311.16148681 252.14027218 236.14629057 269.74337782 225.45331842\n",
      " 266.9479733  333.54431313 248.50129441 397.43713718 333.41667995\n",
      " 234.9932134  283.24746195 334.09400537 294.5649535  297.36167075\n",
      " 325.75082608 205.19406329 244.00727658 336.75798259 272.55566214\n",
      " 246.78714039 363.23289688 198.06113431 244.9256801  275.2133597\n",
      " 239.27197193 257.34940579]\n",
      "rg_intercept 88.99987015107098 <class 'numpy.float64'>\n",
      "'LinearRegression' object has no attribute 'best_estimator_'\n",
      "COEF [1.44798087 0.20384832 0.95871042 0.23285844 0.30338826]\n",
      "89.0 dict_items([('S_2802', 1.448), ('H_2802', 0.204), ('X2', 0.959), ('X3', 0.233), ('Xs', 0.303)])\n",
      "[238.56013293 354.71173534 420.55487868 291.45204108 289.59433917\n",
      " 360.1644005  233.42975084 280.54022904 308.84265502 386.50313724\n",
      " 310.17155911 252.12516324 236.7680187  271.1465961  225.05713263\n",
      " 268.66301493 333.05200109 250.45347586 396.22273077 336.07931217\n",
      " 233.77479397 285.70625756 333.23028171 295.2128126  295.70308942\n",
      " 324.85540894 205.92475349 245.0553462  337.31853495 270.95548021\n",
      " 245.92446306 362.41203179 195.77378319 247.92506758 277.4365342\n",
      " 239.91734448 256.10714176]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[50.84135845]\n",
      " [ 6.45999117]\n",
      " [22.90869982]\n",
      " [12.0998698 ]\n",
      " [10.3578199 ]]\n",
      "289.595 dict_items([('S_2802', 50.841), ('H_2802', 6.46), ('X2', 22.909), ('X3', 12.1), ('Xs', 10.358)])\n",
      "[240.27258797 356.11366318 421.6126684  293.64862468 287.3201233\n",
      " 361.21153603 232.6224325  281.75571669 309.43764972 387.20389367\n",
      " 311.59967056 252.15366144 236.00523281 269.7170581  225.16608471\n",
      " 267.44867009 332.61339694 248.23888521 397.57316525 333.35405998\n",
      " 234.66844552 283.74220918 333.49638212 295.01519793 297.37471771\n",
      " 325.65251418 205.7787109  244.33438889 336.48384422 271.95454357\n",
      " 246.8444042  363.03196767 198.12155723 244.79180133 275.20971692\n",
      " 239.08335351 257.06692394]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[51.45760516]\n",
      " [ 6.505231  ]\n",
      " [22.39798128]\n",
      " [10.78308956]\n",
      " [10.42236605]]\n",
      "289.595 dict_items([('S_2802', 51.458), ('H_2802', 6.505), ('X2', 22.398), ('X3', 10.783), ('Xs', 10.422)])\n",
      "[240.47377037 355.96724158 421.54784001 293.36720967 287.48390098\n",
      " 361.18806233 232.24819113 281.55365658 309.04691746 387.09280578\n",
      " 311.16944999 252.1173794  236.11920019 269.76210749 225.42249376\n",
      " 266.96874564 333.52635417 248.5062141  397.45704951 333.45635807\n",
      " 234.96058362 283.28639264 334.08204825 294.59544026 297.33699884\n",
      " 325.77202223 205.17332202 244.00855214 336.77671364 272.52468567\n",
      " 246.77584284 363.24310796 197.99368675 244.96366716 275.25215742\n",
      " 239.24345689 257.308417  ]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[51.42510235]\n",
      " [ 6.78813744]\n",
      " [22.32982603]\n",
      " [10.72932077]\n",
      " [10.64025923]]\n",
      "289.595 dict_items([('S_2802', 51.425), ('H_2802', 6.788), ('X2', 22.33), ('X3', 10.729), ('Xs', 10.64)])\n",
      "[240.50719851 355.95155668 421.50512734 293.38287229 287.4717346\n",
      " 361.17599686 232.22101666 281.56675445 309.03499136 387.06511521\n",
      " 311.16148681 252.14027218 236.14629057 269.74337782 225.45331842\n",
      " 266.9479733  333.54431313 248.50129441 397.43713718 333.41667995\n",
      " 234.9932134  283.24746195 334.09400537 294.5649535  297.36167075\n",
      " 325.75082608 205.19406329 244.00727658 336.75798259 272.55566214\n",
      " 246.78714039 363.23289688 198.06113431 244.9256801  275.2133597\n",
      " 239.27197193 257.34940579]\n",
      "rg_intercept 289.5945945945946 <class 'numpy.float64'>\n",
      "'PLSRegression' object has no attribute 'best_estimator_'\n",
      "COEF [[51.41074608]\n",
      " [ 6.79042494]\n",
      " [22.36719512]\n",
      " [10.70723835]\n",
      " [10.65283573]]\n",
      "289.595 dict_items([('S_2802', 51.411), ('H_2802', 6.79), ('X2', 22.367), ('X3', 10.707), ('Xs', 10.653)])\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n",
      "h_max_forecast.shape (37,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n",
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n",
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n",
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n",
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n",
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n",
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n",
      "F:\\ruslan\\projects\\pkogo\\env_ogo\\lib\\site-packages\\sklearn\\cross_decomposition\\_pls.py:503: FutureWarning: The attribute `coef_` will be transposed in version 1.3 to be consistent with other linear models in scikit-learn. Currently, `coef_` has a shape of (n_features, n_targets) and in the future it will have a shape of (n_targets, n_features).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "top_best = None\n",
    "result1 = compare_models(pr_group=1, n_test=0, norms=True, top_best=top_best)          \n",
    "result2 = compare_models(pr_group=2, n_test=0, norms=True, top_best=top_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
