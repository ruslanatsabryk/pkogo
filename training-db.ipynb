{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75541273-d8a0-40fe-b676-e024f18f6b41",
   "metadata": {},
   "source": [
    "#### Импорт необходимых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d31a67-e1f4-4f73-b386-b54e9da7cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.interpolate import splrep, splev\n",
    "import pickle\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import Lars, LarsCV\n",
    "from sklearn.linear_model import LassoLars, LassoLarsCV\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, LassoLarsCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816429a8-0a9e-429c-a119-8f088e5505fc",
   "metadata": {},
   "source": [
    "#### Ошибки климатического/природного прогноза для каждого года delta50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a276ed0d-4966-452b-8049-0be188983ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta50(h_max, delta_dop, h_max_avg=None, h_max_forecast=None):\n",
    "    if h_max_forecast is None:\n",
    "        # delta50 климатическая\n",
    "        return (h_max - h_max_avg) / delta_dop\n",
    "    else:\n",
    "        # delta50 прогноза\n",
    "        return (h_max - h_max_forecast) / delta_dop\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f53f1-1316-4641-be21-2bbf50959fe8",
   "metadata": {},
   "source": [
    "### БД. Функция подключения к БД ПК ОГО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc0d6cfa-6c09-4984-89a5-eb8c06fd1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(dict_cursor=False):\n",
    "    db_params = {\n",
    "        \"host\": \"192.168.29.134\",\n",
    "        \"port\": \"5432\",\n",
    "        \"database\": \"pkogo\",\n",
    "        \"user\": \"pkogouser\",\n",
    "        \"password\": \"pkogouser\",\n",
    "    }\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        if dict_cursor:\n",
    "            cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "        else:\n",
    "            cursor = conn.cursor()\n",
    "        # print(cursor)\n",
    "        # print(\"DB connection works\")\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(f\"Error connecting to database: {error}\")\n",
    "\n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11988bc2-a8d0-41ab-bdae-529589985235",
   "metadata": {},
   "source": [
    "### БД. Функции закрытия подключения к БД ПК ОГО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22ea4d1-9995-4218-9465-477dcfabf853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_db(connection, cursor):\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace83765-8281-406c-95a6-da628f939250",
   "metadata": {},
   "source": [
    "#### БД. Функция чтения набора данных из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8612bdc-5859-497d-ad7a-ece6c12a20c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_river_dataset_db(station_id, pr_list=None, y_name='H_max'):\n",
    "    conn, cursor = connect_db()\n",
    "    \n",
    "    predictors = ', '.join(pr_list) + f', {y_name}' + ', obs_year'\n",
    "    sql_observations = f\"\"\"\n",
    "    SELECT {predictors} FROM maxlevel.observations\n",
    "    WHERE station_id = %s\n",
    "    ORDER BY obs_year\n",
    "    \"\"\"\n",
    "    station_arg = (station_id, )\n",
    "    cursor.execute(sql_observations, station_arg)\n",
    "    observations = cursor.fetchall()\n",
    "    \n",
    "    close_db(conn, cursor)\n",
    "        \n",
    "    X_y_years = np.asarray(observations, dtype=np.float64)\n",
    "    X = np.asarray(X_y_years[:, :-2], dtype=np.float64)\n",
    "    y = np.asarray(X_y_years[:, -2], dtype=np.float64)\n",
    "    obs_years = np.asarray(X_y_years[:, -1], dtype=np.float64)\n",
    "    return X, y, obs_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973df912-ed2f-4633-9afd-8343f2c8bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_river_dataset_db(74021, pr_list=['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'X3', 'Xs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c3d04-9081-4aa4-8bd4-85901cdc21b1",
   "metadata": {},
   "source": [
    "#### Сумма, средний, высший, низший уровни"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d899bbc9-cfa6-4808-855e-5bc2ffe32753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(h_max):\n",
    "    return np.sum(h_max)\n",
    "    \n",
    "def get_avg(h_max):\n",
    "    return np.mean(h_max)\n",
    "    \n",
    "def get_max(h_max):\n",
    "    return np.amax(h_max)\n",
    "    \n",
    "def get_min(h_max):\n",
    "    return np.amin(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d4851-a18c-4a9e-bdc7-ba8a79e0e17c",
   "metadata": {},
   "source": [
    "#### Среднее значение максимальных уровней воды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0519a77b-80db-4d94-ab39-1c60a7b7d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hmax_avg(h_max):\n",
    "    # Среднее значение h_max.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.mean(h_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dafa4-e1f7-4c26-b28d-3dfd6a2b1b14",
   "metadata": {},
   "source": [
    "#### Среднеквадратическая погрешность прогноза S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad85fc4-c2cd-4a9f-8a0a-bb4fe4cdfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s(h_max, h_forecast):\n",
    "    # Среднеквадратическая погрешность прогноза\n",
    "    n = h_max.shape[0]\n",
    "    sqr_diff = np.sum((h_max - h_forecast) ** 2) / (n - 1)\n",
    "    std = sqr_diff ** 0.5\n",
    "    return std    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e98c4-4841-4b3f-b61b-284bc99af53d",
   "metadata": {},
   "source": [
    "#### Среднеквадратическое отклонение sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5079b8f9-3ae5-47a1-a6f3-214bdf69ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigma(h_max):\n",
    "    # Среднеквадратическая погрешность климатическая.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.std(h_max, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10fa8d-d7cd-44f4-b4f2-4f652409383f",
   "metadata": {},
   "source": [
    "#### Допустимая погрешность прогноза delta_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d4648d-8e14-4c63-bcf8-6a3c2cb7a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_dop(sigma):\n",
    "    return 0.674 * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1c7ff-c4e1-4639-a877-d3b78d944dea",
   "metadata": {},
   "source": [
    "#### Критерий критерий применимости и качества методики S/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f92739a9-3b12-44bd-b51e-4ffd73e828a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(s, sigma):\n",
    "    return s / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5a9fa-a43a-4252-864e-5fd59fc6b44e",
   "metadata": {},
   "source": [
    "#### Климатическая обеспеченность Pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7188d628-0546-49d9-8ada-ce04541b8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pk(h_max, h_max_avg, delta_dop):\n",
    "    diff = np.abs(h_max - h_max_avg) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4c686-2946-4ea0-980a-172b2ff4dd64",
   "metadata": {},
   "source": [
    "#### Обеспеченность метода (оправдываемость) Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68ca5adf-fbf3-49a4-8458-06d103bfacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pm(h_max, h_forecast, delta_dop):\n",
    "    diff = np.abs(h_max - h_forecast) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d93f0-e243-4107-b007-cb24efbdc6f6",
   "metadata": {},
   "source": [
    "#### Корреляционное отношение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a501afb2-3736-4023-a488-a19747e54910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_ratio(criterion):\n",
    "    c_1 = (1 - criterion ** 2)\n",
    "    ro = c_1 ** 0.5 if c_1 > 0 else 0\n",
    "    return ro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0cc72-ec19-437b-b6c5-8010c2e3a5bf",
   "metadata": {},
   "source": [
    "#### Функция записи списка моделей с их характеристиками в csv файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c76ad0-d75c-4746-90dc-0d8465218463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_dataset_csv(year, dataset, dataset_name, fieldnames, pr_group, mode='training'):\n",
    "#     if mode == 'estimation':\n",
    "#         dir_path = f'results/Estimation/{year}/{dataset_name}/group-{pr_group}/'\n",
    "#         file_name = f'{dataset_name}-гр{pr_group}-Оценка.csv'\n",
    "#     elif mode == 'training':\n",
    "#         dir_path = f'results/Models/{year}/'\n",
    "#         file_name = f'{dataset_name}-гр{pr_group}-Обучение.csv'\n",
    "#     elif mode == 'forecast':\n",
    "#         dir_path = f'results/Forecast/{year}/'\n",
    "#         file_name = f'{dataset_name}-гр{pr_group}-Прогноз.csv'\n",
    "#     else:\n",
    "#         ...\n",
    "    \n",
    "#     with open(\n",
    "#         f'{dir_path}'\n",
    "#         f'{file_name}', \n",
    "#         'w', newline='', encoding='utf-8'\n",
    "#     ) as csvfile:\n",
    "        \n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames, \n",
    "#                                 delimiter=';', extrasaction='ignore')\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2d2a0-f9da-46e5-a0ae-45e8fe0c2482",
   "metadata": {},
   "source": [
    "#### Функция разделения набора данных на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82db9e50-fadc-4c7f-9151-58c2b74df8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, n_test, split=True):\n",
    "    if split:   \n",
    "        X_train = X[:-n_test].copy()\n",
    "        y_train = y[:-n_test].copy()\n",
    "        X_test = X[-n_test:].copy()\n",
    "        y_test = y[-n_test:].copy()\n",
    "    else:\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "        X_test = X.copy()\n",
    "        y_test = y.copy()\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05933c74-63f1-4a9e-bc5b-35842087b385",
   "metadata": {},
   "source": [
    "#### Функция перемешивания данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14522f32-b14c-4dfd-8286-b8cf15b960c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_xy(X, y, shuffle=True):\n",
    "    if shuffle:\n",
    "        # Перемешивание данных\n",
    "        Xy = np.column_stack((X, y))\n",
    "        rng = np.random.default_rng(42)\n",
    "        rng.shuffle(Xy)\n",
    "        y_sh = Xy[:, -1]\n",
    "        X_sh = Xy[:,:-1]\n",
    "    else:\n",
    "        y_sh = y.copy()\n",
    "        X_sh = X.copy()\n",
    "    return X_sh, y_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719a5f9-176e-473d-8ce7-b7744d9c6e19",
   "metadata": {},
   "source": [
    "#### Функция формирования тестового набора данных с подстановкой нормированных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b464d45-843c-4792-ab02-2819edddee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_norm(x, pr_list, norms):\n",
    "    x_norm = np.copy(x)\n",
    "    for col, pr in enumerate(pr_list):\n",
    "        if pr in norms:\n",
    "            x_norm[:, col:col+1] = norms[pr]\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae178d5d-9f28-48c4-b523-5753c0c17ea6",
   "metadata": {},
   "source": [
    "#### Функция получения словаря гидропостов из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ad62b1f-b176-472b-abd6-b37195882fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_db():\n",
    "    conn, cursor = connect_db()\n",
    "    sql = \"SELECT station, station_id FROM maxlevel.stations\"\n",
    "    cursor.execute(sql)\n",
    "    result = cursor.fetchall()\n",
    "    close_db(conn, cursor)\n",
    "\n",
    "    return {station: id for station, id in result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02cfb179-af76-4a88-88f6-558dcb12e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_datasets_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b437911-310c-4a00-a63a-c80fecb21d9f",
   "metadata": {},
   "source": [
    "#### Функция получения списка предикторов по названию гидропоста из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea19577c-12a4-49b9-b3af-8af4631a2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictors_db(station_id, group=None):\n",
    "    conn, cursor = connect_db()\n",
    "    sql = \"\"\"\n",
    "    SELECT p.predictors_id, p.predictors\n",
    "    FROM maxlevel.predictors_groups p\n",
    "    INNER JOIN maxlevel.stations s on p.method_id = s.method_id \n",
    "    WHERE s.station_id = %s\n",
    "    ORDER BY p.group_n\n",
    "    \"\"\"\n",
    "    cursor.execute(sql, (station_id,))\n",
    "    groups = cursor.fetchall()\n",
    "    # print(groups)\n",
    "    # Преобразование строк в списки\n",
    "    groups_list = []\n",
    "    for s in groups:\n",
    "        sg = s[1].replace(' ', '')\n",
    "        groups_list.append((s[0], sg.split(',')))\n",
    "    close_db(conn, cursor)\n",
    "    \n",
    "    return groups_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35465897-4655-40b1-a5cc-547912e0a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_predictors_db(74021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790ff38-4843-40b6-ac40-a9efec555315",
   "metadata": {},
   "source": [
    "#### Функция получения нормированных значений предикторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a741f9dd-31c9-4ea6-a1a8-b2b684b05013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norms(dataset_name):\n",
    "    norms_list = {\n",
    "        'Белица': {'Smax': 59.89, 'X':96.16, 'X1': 46.0, 'X2':35.0},\n",
    "        'Гродно': {'Smax': 51.70, 'X':80.27, 'X1': 36.0, 'X2':26.0},\n",
    "        'Мосты': {'Smax': 53.62, 'X':88.51, 'X1': 40.0, 'X2':31.0},\n",
    "        'Столбцы': {'Smax': 73.68, 'X':101.68, 'X1': 43.0, 'X2':34.0},\n",
    "\n",
    "        'Стешицы': {'Smax': 67.0, 'X': 112.0, 'X1': 40.0, 'X2': 33.0, 'L_max': 60.0},\n",
    "        'Михалишки': {'Smax': 60.0, 'X': 116.0, 'X1': 46.0, 'X2': 37.0, 'L_max': 57.0},\n",
    "\n",
    "        'Сураж': {'S_max': 89.0, 'X':126.0, 'X1': 50.0, 'X2':43.0},\n",
    "        'Верхнедвинск': {'S_max': 62.0, 'X':122.0, 'X1': 68.0, 'X2':56.0},\n",
    "        'Витебск': {'S_max': 80.0, 'X': 134.0, 'X1': 65.0, 'X2': 53.0, 'Y_sum': 46.4},\n",
    "        'Полоцк': {'S_max': 66.0, 'X': 122.0, 'X1': 61.0, 'X2': 53.0, 'Y_sum': 46.5},\n",
    "    }\n",
    "    return norms_list[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83f925-d3c5-46ea-bbba-cb3cf9605113",
   "metadata": {},
   "source": [
    "#### Функция получения аугментированных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb9c7743-9f84-4dbb-8cbb-c0db0b952414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(x_data, y_data, aug_mpl, aug_pow=2, mirror=True, s=None, clear=True):\n",
    "    #print(x_data)\n",
    "    data_len = len(y_data)\n",
    "    \n",
    "    x_points = np.linspace(0, data_len, data_len)\n",
    "    \n",
    "    x_splitted = np.hsplit(x_data, x_data.shape[1])\n",
    "    #print(x_splitted)\n",
    "\n",
    "    aug_n = round(data_len * (aug_mpl - 1) / (data_len - 1)) * (data_len - 1) + data_len\n",
    "    \n",
    "    x_list = []\n",
    "    for arr in x_splitted:\n",
    "        x_spl = splrep(x_points, arr, k=aug_pow, s=s)\n",
    "        x_points_n = np.linspace(0, data_len, aug_n)\n",
    "        x_col_augmented = splev(x_points_n, x_spl)\n",
    "        x_list.append(x_col_augmented)\n",
    "    x_augmented = np.array(x_list).T\n",
    "\n",
    "    y_points = np.linspace(0, data_len, data_len)\n",
    "    y_spl = splrep(y_points, y_data, k=aug_pow, s=s)\n",
    "    y_points_n = np.linspace(0, data_len, aug_n)\n",
    "    y_augmented = splev(y_points_n, y_spl)\n",
    "\n",
    "    if clear:\n",
    "        x_aug_round = np.round(x_augmented, decimals=-1)\n",
    "        y_aug_round = np.round(y_augmented, decimals=1)\n",
    "    \n",
    "        x_data_round = np.round(x_data, decimals=-1)\n",
    "        y_data_round = np.round(y_data, decimals=1)   \n",
    "        \n",
    "        mx = (x_aug_round[:, None] == x_data_round).all(-1).any(1)\n",
    "        x_aug_clear = x_augmented[~mx].copy()\n",
    "        y_aug_clear = y_augmented[~mx].copy()\n",
    "        points_aug_clear = y_points_n[~mx]\n",
    "    else:\n",
    "        x_aug_clear = x_augmented\n",
    "        y_aug_clear = y_augmented\n",
    "\n",
    "    \n",
    "    # print('x_aug_clear.shape', x_aug_clear.shape)\n",
    "    # print('x_augmented.shape', x_augmented.shape)\n",
    "\n",
    "    if mirror:\n",
    "        x_mirror = np.mean(x_augmented) - x_augmented + np.mean(x_augmented)\n",
    "        y_mirror = np.mean(y_augmented) - y_augmented + np.mean(y_augmented)\n",
    "    \n",
    "        x_result = np.vstack((x_aug_clear, x_mirror))\n",
    "        y_result = np.hstack((y_aug_clear, y_mirror))\n",
    "\n",
    "    else:\n",
    "        x_result = x_aug_clear\n",
    "        y_result = y_aug_clear\n",
    "    \n",
    "    if mirror:\n",
    "        ...\n",
    "    #     plt.plot(y_points, y_data, 'o', y_points_n, y_mirror)\n",
    "    #     plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_mirror[:, 0])\n",
    "   \n",
    "    # plt.plot(y_points, y_data, 'o', points_aug_clear, y_aug_clear)\n",
    "    # plt.plot(x_points, x_data[:, 0], 'x', points_aug_clear, x_aug_clear[:, 0])\n",
    "    # plt.show()\n",
    "\n",
    "    x_result[x_result < 0] = 0\n",
    "    y_result[y_result < 0] = 0\n",
    "    \n",
    "    return x_result, y_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519297c-5520-4427-972f-4f3acda51145",
   "metadata": {},
   "source": [
    "#### Функция получения трансформеров входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d17b776-77e2-4677-a8cb-40e33404248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer(transformer, n_samples=10_000):\n",
    "    scaler = (\n",
    "        StandardScaler() if transformer == 'standard' else \\\n",
    "        MinMaxScaler() if transformer == 'minmax' else \\\n",
    "        MaxAbsScaler() if transformer == 'maxabs' else \\\n",
    "        RobustScaler() if transformer == 'robust' else \\\n",
    "        QuantileTransformer(output_distribution='uniform', n_quantiles=n_samples, random_state=0) if transformer == 'uniform' else \\\n",
    "        QuantileTransformer(output_distribution='normal', n_quantiles=n_samples, random_state=0) if transformer == 'normal' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=False) if transformer == 'normal-bc' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=False) if transformer == 'normal-yj' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=True) if transformer == 'normal-bc-st' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=True) if transformer == 'normal-yj-st' else \\\n",
    "        None\n",
    "    )\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c487c28-3c96-40f8-8206-ba78a57cad4d",
   "metadata": {},
   "source": [
    "#### Функция получения списка моделей регрессоров из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48ca65e9-ad82-4b01-9981-ee7ae85de30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressors_list_db():\n",
    "    conn, cursor = connect_db()\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT algorithm_id, algorithm FROM maxlevel.algorithms\n",
    "    WHERE active is true\n",
    "    ORDER BY algorithm_id\n",
    "    \"\"\"\n",
    "    cursor.execute(sql)\n",
    "    algorithms = cursor.fetchall()\n",
    "    \n",
    "    close_db(conn, cursor)\n",
    "\n",
    "    return algorithms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b78bb91-e15e-4ffa-8ef6-f4a49bdf5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_regressors_list_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad1b1f-6076-45b5-b9f1-689dd88981bf",
   "metadata": {},
   "source": [
    "#### Функция получения объектов моделей регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f061ca6e-ca23-41aa-9f72-add0e4a9cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressors_objects(grid_search=False):\n",
    "    # Инициализация генератора случайных чисел для\n",
    "    # для обеспечения воспроизводимости результатов\n",
    "    rng = np.random.RandomState(0)\n",
    "\n",
    "    # Наборы гиперпараметров моделей для алгоритма кроссвалидации\n",
    "    # Гиперпараметры для Ridge, Lasso, ElasticNet, LassoLars, HuberRegressor\n",
    "    alphas = np.logspace(-4, 3, num=100)\n",
    "    \n",
    "    # Гиперпараметры для ElasticNet\n",
    "    l1_ratio = np.linspace(0.01, 1.0, num=50)\n",
    "    \n",
    "    # Гиперпараметры для BayesianRidge\n",
    "    alphas_init = np.linspace(0.5, 2, 5)\n",
    "    lambdas_init = np.logspace(-3, 1, num=5)\n",
    "    \n",
    "    # Гиперпараметры для ARDRegression\n",
    "    alphas_lambdas = np.logspace(-7, -4, num=4)\n",
    "    \n",
    "    # Гиперпараметры для SGDRegressor\n",
    "    losses = ['squared_error', 'huber', \n",
    "              'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "    sgd_alphas = np.logspace(-4, 1, num=100)\n",
    "   \n",
    "    # Гиперпараметры для PassiveAggressiveRegressor\n",
    "    cc = np.linspace(0.1, 1.5, 50)\n",
    "    \n",
    "    # Гиперпараметры для HuberRegressor\n",
    "    epsilons = np.append(np.linspace(1.1, 2.0, 10), [1.35])\n",
    "    \n",
    "    # Гиперпараметры для TheilSenRegressor\n",
    "    # n_subsamples = np.arange(15, 24)\n",
    "    n_subsamples = (16, 24, 32)\n",
    "    \n",
    "    # Гиперпараметры для QuantileRegressor\n",
    "    # q_alphas = np.linspace(0, 1, 5)\n",
    "    q_alphas = (0.1, 1, 2)    \n",
    "    \n",
    "    regressors = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        \n",
    "        # Ridge(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=Ridge(random_state=0), \n",
    "        #     param_grid={\"alpha\": alphas}\n",
    "        # ),\n",
    "        \n",
    "        'RidgeCV': RidgeCV(),\n",
    "\n",
    "        'ElasticNetCV': ElasticNetCV(random_state=0),\n",
    "\n",
    "        'LassoCV': LassoCV(max_iter=10000, n_alphas=300, random_state=0),  \n",
    "\n",
    "        'LarsCV': LarsCV(),\n",
    "        \n",
    "        'Lars1': Lars(n_nonzero_coefs=1, random_state=0),\n",
    "        'Lars2': Lars(n_nonzero_coefs=2, random_state=0),\n",
    "        'Lars3': Lars(n_nonzero_coefs=3, random_state=0),\n",
    "        'Lars4': Lars(n_nonzero_coefs=4, random_state=0),\n",
    "        'Lars5': Lars(n_nonzero_coefs=5, random_state=0),\n",
    "        'Lars6': Lars(n_nonzero_coefs=6, random_state=0),\n",
    "        'Lars7': Lars(n_nonzero_coefs=7, random_state=0),\n",
    "        'Lars8': Lars(n_nonzero_coefs=8, random_state=0),\n",
    "        'Lars9': Lars(n_nonzero_coefs=9, random_state=0),\n",
    "        'Lars10': Lars(n_nonzero_coefs=10, random_state=0),\n",
    "        'Lars11': Lars(n_nonzero_coefs=11, random_state=0),\n",
    "        'Lars12': Lars(n_nonzero_coefs=12, random_state=0),\n",
    "        'Lars13': Lars(n_nonzero_coefs=13, random_state=0),\n",
    "        'Lars14': Lars(n_nonzero_coefs=14, random_state=0),\n",
    "\n",
    "        'LassoLarsCV': LassoLarsCV(max_iter=500, max_n_alphas=1000),\n",
    "\n",
    "        'OMPCV': OrthogonalMatchingPursuitCV(n_jobs=-1),\n",
    "        \n",
    "        'OMP1': OrthogonalMatchingPursuit(n_nonzero_coefs=1),\n",
    "        'OMP2': OrthogonalMatchingPursuit(n_nonzero_coefs=2),\n",
    "        'OMP3': OrthogonalMatchingPursuit(n_nonzero_coefs=3),\n",
    "        'OMP4': OrthogonalMatchingPursuit(n_nonzero_coefs=4),\n",
    "        'OMP5': OrthogonalMatchingPursuit(n_nonzero_coefs=5),\n",
    "        'OMP6': OrthogonalMatchingPursuit(n_nonzero_coefs=6),\n",
    "        'OMP7': OrthogonalMatchingPursuit(n_nonzero_coefs=7),\n",
    "        'OMP8': OrthogonalMatchingPursuit(n_nonzero_coefs=8),\n",
    "        'OMP9': OrthogonalMatchingPursuit(n_nonzero_coefs=9),\n",
    "        'OMP10': OrthogonalMatchingPursuit(n_nonzero_coefs=10),\n",
    "        'OMP11': OrthogonalMatchingPursuit(n_nonzero_coefs=11),\n",
    "        'OMP12': OrthogonalMatchingPursuit(n_nonzero_coefs=12),\n",
    "        'OMP13': OrthogonalMatchingPursuit(n_nonzero_coefs=13),\n",
    "        'OMP14': OrthogonalMatchingPursuit(n_nonzero_coefs=14),\n",
    "        \n",
    "        'BayesianRidge': BayesianRidge(),\n",
    "        # BayesianRidge() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=BayesianRidge(),\n",
    "        #     param_grid={\"alpha_init\": alphas_init, \"lambda_init\": lambdas_init}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # ARDRegression(),\n",
    "        \n",
    "        # ARDRegression() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=ARDRegression(), \n",
    "        #     param_grid={\"alpha_1\": alphas_lambdas, \"alpha_2\": alphas_lambdas,\n",
    "        #                 \"lambda_1\": alphas_lambdas,\"lambda_2\": alphas_lambdas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # SGDRegressor(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=SGDRegressor(random_state=0), \n",
    "        #     param_grid={\"loss\": losses, \"alpha\": sgd_alphas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        'PassiveAggressiveRegressor': PassiveAggressiveRegressor(random_state=0) if not grid_search else \\\n",
    "        GridSearchCV(\n",
    "            estimator=PassiveAggressiveRegressor(random_state=0), \n",
    "            param_grid={\"C\": cc}, \n",
    "            n_jobs=-1, \n",
    "            cv=3\n",
    "        ),\n",
    "\n",
    "        'HuberRegressor': HuberRegressor(max_iter=1000),\n",
    "        # HuberRegressor(max_iter=1000) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=HuberRegressor(), \n",
    "        #     param_grid={\"epsilon\": epsilons, \"alpha\": alphas}, \n",
    "        #     n_jobs=-1 \n",
    "        # ),\n",
    "\n",
    "        # TheilSenRegressor(random_state=0, n_jobs=-1),\n",
    "        # TheilSenRegressor(random_state=0, n_jobs=-1) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=TheilSenRegressor(random_state=0, n_jobs=-1), \n",
    "        #     param_grid={\"n_subsamples\": n_subsamples}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        'QuantileRegressor': QuantileRegressor(),\n",
    "        \n",
    "        # QuantileRegressor() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=QuantileRegressor(), \n",
    "        #     param_grid={\"alpha\": q_alphas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "        \n",
    "        \n",
    "        \n",
    "        'KNeighborsRegressor': KNeighborsRegressor(n_neighbors=10, metric='euclidean'),\n",
    "        # NuSVR(C=5.0, nu=0.9, kernel='poly', degree=3),\n",
    "        # SVR(C=5.0, epsilon=0.2, kernel='poly', degree=3),\n",
    "        \n",
    "        \n",
    "        # MLPRegressor(\n",
    "        #     hidden_layer_sizes=(3, ), \n",
    "        #     activation='identity', \n",
    "        #     max_iter=100000, \n",
    "        #     early_stopping=True, \n",
    "        #     learning_rate='constant',\n",
    "        #     learning_rate_init=0.00025,\n",
    "        #     batch_size=75,\n",
    "        #     solver='adam',\n",
    "        #     random_state=0\n",
    "        # ),\n",
    "       \n",
    "        \n",
    "        \n",
    "        'RandomForestRegressor': RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0),\n",
    "        \n",
    "        'ExtraTreesRegressor': ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0),\n",
    "        \n",
    "        'HistGradientBoostingRegressor':HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0),\n",
    "        \n",
    "        'BaggingRegressor': BaggingRegressor(\n",
    "            #KNeighborsRegressor(n_neighbors=20, metric='euclidean'),\n",
    "            estimator=ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0), \n",
    "            max_samples=0.75, max_features=0.75, n_estimators=10, random_state=0\n",
    "        ),\n",
    "\n",
    "        'VotingRegressor': VotingRegressor(\n",
    "            estimators=[\n",
    "                ('hgbr', HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0)), \n",
    "                ('omp', ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0)), \n",
    "                ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean')),\n",
    "                ('rfr', RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)),\n",
    "            ]\n",
    "        ),\n",
    "\n",
    "\n",
    "        'StackingRegressorRidgeCV': StackingRegressor( # RidgeCV - final estimator\n",
    "            estimators=[\n",
    "                ('knr', KNeighborsRegressor(n_neighbors=10, metric='euclidean')),\n",
    "                ('rfr', RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)),\n",
    "                ('hgbr', HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0)), \n",
    "                ('etr', ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0)),\n",
    "                ('omp', OrthogonalMatchingPursuit(n_nonzero_coefs=5)),\n",
    "            ],\n",
    "        ),\n",
    "\n",
    "        'AdaBoostRegressor': AdaBoostRegressor(estimator=KNeighborsRegressor(n_neighbors=5, metric='euclidean'), n_estimators=100, loss='linear', random_state=0),\n",
    "        \n",
    "    }\n",
    "    return regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08312297-9ca3-4e7d-be60-26969a4611e7",
   "metadata": {},
   "source": [
    "#### Функция обучения моделей на полном наборе данных с записью на диск и в БД (таблица models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7dff9414-8a26-4393-b480-446307aafc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(year, pr_group, n_test=None,\n",
    "                 norms=True, aug_n=0, aug_mpl=30,\n",
    "                 aug_pow=2, aug_mirror=False, grid_search=False,\n",
    "                 scaler_x=None, scaler_y=None, shuffle=True,\n",
    "                 serial=True, top_best=None,\n",
    "                 stations_id=None):\n",
    "    \n",
    "    # ds_dir = f'data/{year}/Train'\n",
    "    \n",
    "    # {station: id, ...}\n",
    "    if stations_id:\n",
    "        datasets = {st: stid for st, stid in get_datasets_db().items() if stid in stations_id}\n",
    "    else:\n",
    "        datasets = get_datasets_db()\n",
    "                     \n",
    "    fieldnames = [\n",
    "        'Predictors', \n",
    "        'Equations', \n",
    "        'Algorithm', \n",
    "        'Criterion', \n",
    "        'Correlation', \n",
    "        'Pm',\n",
    "        'R2',\n",
    "\n",
    "        'Criterion_t', \n",
    "        'Correlation_t', \n",
    "        'Pm_t',\n",
    "        'R2_t',\n",
    "\n",
    "        'Criterion_f', \n",
    "        'Correlation_f', \n",
    "        'Pm_f',\n",
    "        'R2_f',\n",
    "\n",
    "        # 'Group',\n",
    "        # 'Augmentation',\n",
    "        # 'Data size',\n",
    "        # 'Normalization',\n",
    "        # 'Equations',\n",
    "    ]\n",
    "\n",
    "    # Описание структуры данных переменной datasets_result\n",
    "    # datasets_result = {\n",
    "    #     \"hydropost_0\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    #     ...,\n",
    "    #     \"hydropost_n\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    # Итерация по датасетам\n",
    "    datasets_result = dict()\n",
    "    for ds in datasets:\n",
    "\n",
    "        # Получить все группы по датасету\n",
    "        ds_groups = get_predictors_db(datasets[ds])\n",
    "        \n",
    "        # Итерация по группам предикторов\n",
    "        for group, (predictors_id, pr_list) in enumerate(ds_groups):\n",
    "            if group == 0:\n",
    "                # По группе 0 модели не обучаются - это все предикторы + год + максимальный уровень и его дата\n",
    "                continue\n",
    "            if pr_group is not None:\n",
    "                if group != pr_group:\n",
    "                    continue\n",
    "        \n",
    "            result_list = []\n",
    "            \n",
    "            X, y, obs_years = get_river_dataset_db(datasets[ds], pr_list=pr_list)\n",
    "    \n",
    "            # Проверочный набор данных (исходный)\n",
    "            X_prior = X.copy()\n",
    "            y_prior = y.copy()\n",
    "            \n",
    "            # Полный набор данных\n",
    "            X_full = X.copy()\n",
    "            y_full = y.copy()\n",
    "\n",
    "            if aug_mpl > 1:\n",
    "                X_full, y_full = augment_data(X_full, y_full, aug_mpl, aug_pow=aug_pow, mirror=aug_mirror, clear=True)\n",
    "            \n",
    "            if shuffle:\n",
    "                X_full, y_full = shuffle_xy(X_full, y_full, shuffle=True)\n",
    "            \n",
    "            if n_test:\n",
    "                X_train, y_train, X_test, y_test = train_test_split(X_full, y_full, n_test, split=True)\n",
    "            \n",
    "            norms_data = None\n",
    "            if norms:\n",
    "                norms_data = get_norms(ds) # сделать для БД\n",
    "                # Подстановка норм в исходный набор данных (пессимистичный сценарий)\n",
    "                X_prior = test_norm(X_prior, pr_list, norms_data)\n",
    "                # Подстановка норм в тестовый набор данных\n",
    "                X_test = test_norm(X_test, pr_list, norms_data)\n",
    "                # Подстановка норм в полный набор данных не требуется\n",
    "  \n",
    "            transformer_y = get_transformer(scaler_y, n_samples=y_train.shape[0]) # !!!\n",
    "            transformer_x = get_transformer(scaler_x, n_samples=y_train.shape[0]) # !!!\n",
    "            transformer_y_full = get_transformer(scaler_y, n_samples=y_full.shape[0]) # !!!\n",
    "            transformer_x_full = get_transformer(scaler_x, n_samples=y_full.shape[0]) # !!!\n",
    "\n",
    "            # Список оцениваемых ререссионных моделей !!!!!\n",
    "            algorithms = get_regressors_list_db()\n",
    "            \n",
    "            regressors = get_regressors_objects(grid_search=grid_search)\n",
    "            regressors_full = get_regressors_objects(grid_search=grid_search)\n",
    "                \n",
    "            # Итерация по моделям регрессии\n",
    "            for alg_id, alg in algorithms:\n",
    "                model = regressors[alg]\n",
    "                model_full = regressors_full[alg]\n",
    "\n",
    "                # Препроцессинг - трансформация целевых значений y\n",
    "                if scaler_y: \n",
    "                    regressor = TransformedTargetRegressor(regressor=model, transformer=transformer_y)\n",
    "                    regressor_full = TransformedTargetRegressor(regressor=model_full, transformer=transformer_y_full)\n",
    "                else:\n",
    "                    regressor = model\n",
    "                    regressor_full = model_full\n",
    "                \n",
    "                one_model_row = dict()                \n",
    "                \n",
    "                # Препроцессинг - трансформация признаков X\n",
    "                regr = make_pipeline(transformer_x, regressor) if transformer_x else regressor\n",
    "                regr_full = make_pipeline(transformer_x_full, regressor_full) if transformer_x_full else regressor_full\n",
    "                \n",
    "                # Обучение на тренировочном наборе\n",
    "                # regr = regr.fit(X_train, y_train)\n",
    "                try:\n",
    "                    regr.fit(X_train, y_train)\n",
    "                except ValueError as error:\n",
    "                    print(f'{ds}, гр. {group}, {alg} - Ошибка обучения на тренировочном наборе данных:')\n",
    "                    print(error)\n",
    "                    continue\n",
    "                if serial:\n",
    "                    serial_model = pickle.dumps(regr)\n",
    "                    regr = pickle.loads(serial_model)\n",
    "                \n",
    "                # Обученная на тренировочном наборе данных модель\n",
    "                one_model_row['Model_train'] = regr\n",
    "                    \n",
    "                # Прогноз по исходному набору на тренировочной модели\n",
    "                y_predicted_prior = np.ravel(regr.predict(X_prior))\n",
    "                \n",
    "                # Прогноз по тестовому набору на тренировочной модели \n",
    "                y_predicted_test = np.ravel(regr.predict(X_test))\n",
    "    \n",
    "                \n",
    "                # Очистка значений строк предикторов и уравнений перед переходом к следующей модели\n",
    "                coef = None\n",
    "                intercept = None\n",
    "                \n",
    "                try:\n",
    "                    coef = regr.best_estimator_.coef_\n",
    "                    intercept = regr.best_estimator_.intercept_\n",
    "                    \n",
    "                    if isinstance(intercept, np.ndarray):\n",
    "                        intercept = intercept[0]\n",
    "                except Exception as error:\n",
    "                                    \n",
    "                    try:\n",
    "                        coef = regr.coef_\n",
    "                        intercept = regr.intercept_\n",
    "                    \n",
    "                        if isinstance(intercept, np.ndarray):\n",
    "                            intercept = intercept[0]\n",
    "                        # print(\"ERROR1 START\")\n",
    "                        # print(error)\n",
    "                        # print(\"ERROR1 FINISH\")\n",
    "                    except Exception as error:\n",
    "                        ...\n",
    "                        # print(\"ERROR2 START\")\n",
    "                        # print(error)\n",
    "                        # print(\"ERROR2 FINISH\")\n",
    "                    \n",
    "                \n",
    "                try:\n",
    "                    # Коэффициенты уравнения (если есть)\n",
    "                    coef = np.around(np.ravel(coef), 3)\n",
    "                    intercept = round(intercept, 3)\n",
    "                    \n",
    "                    predictors_coef = {f: c for f, c \n",
    "                                       in zip(pr_list, coef) if c != 0.0}\n",
    "                    \n",
    "                    predictors = \", \".join(predictors_coef.keys())\n",
    "                    \n",
    "                    equation = (\n",
    "                        str(intercept) \n",
    "                        + ' ' \n",
    "                        + ' '.join(str(c) + '*' \n",
    "                                   + f for f, c in predictors_coef.items())\n",
    "                    )\n",
    "                    \n",
    "                    equation = equation.replace(\" -\", \"-\")\n",
    "                    equation = equation.replace(\" \", \" + \")\n",
    "                    equation = equation.replace(\"-\", \" - \")\n",
    "        \n",
    "                    one_model_row['Predictors'] = predictors\n",
    "                    one_model_row['Equations'] = equation\n",
    "                except Exception as error:\n",
    "                    # print(\"ERROR3 START\")\n",
    "                    # print(error)\n",
    "                    # print(\"ERROR3 FINISH\")\n",
    "                    one_model_row['Predictors'] = \", \".join(pr_list)\n",
    "                    one_model_row['Equations'] = \"Непараметрическая (нелинейная) модель\"\n",
    "    \n",
    "                \n",
    "                \n",
    "                # Код алгоритма\n",
    "                one_model_row['Algorithm_id'] = alg_id\n",
    "\n",
    "                # Код гидропоста\n",
    "                one_model_row['Station_id'] = datasets[ds]\n",
    "\n",
    "                # Код списка предикторов\n",
    "                one_model_row['Predictors_id'] = predictors_id\n",
    "                    \n",
    "                # Год прогноза\n",
    "                one_model_row['Forecast_year'] = year\n",
    "\n",
    "                # Название датасета (гидропоста)\n",
    "                one_model_row['Dataset_name'] = ds\n",
    "\n",
    "                # Исходный список предикторов\n",
    "                one_model_row['Predictors_list'] = pr_list\n",
    "\n",
    "                # Нормы\n",
    "                one_model_row['Norms_data'] = norms_data\n",
    "    \n",
    "                # Группа предикторов\n",
    "                one_model_row['Group'] = group\n",
    "                    \n",
    "                # Название метода\n",
    "                one_model_row['Algorithm'] = alg\n",
    "\n",
    "                # Путь к файлу модели на бэкенде\n",
    "                model_file = (\n",
    "                    f'results/Models/{one_model_row[\"Forecast_year\"]}/'\n",
    "                    f'{one_model_row[\"Dataset_name\"]}_'\n",
    "                    f'{one_model_row[\"Forecast_year\"]}_'\n",
    "                    f'гр{one_model_row[\"Group\"]}_'\n",
    "                    f'{one_model_row[\"Algorithm\"]}.pickle'\n",
    "                )\n",
    "                one_model_row['Model_file'] = model_file\n",
    "\n",
    "                # Сумма, максимум, минимум, среднее максимальных уровней\n",
    "                # по исходному набору:\n",
    "                one_model_row['H_sum'] = get_sum(y_prior)\n",
    "                one_model_row['H_max'] = get_max(y_prior)\n",
    "                one_model_row['H_min'] = get_min(y_prior)\n",
    "                h_max_avg = get_hmax_avg(y_prior)\n",
    "                one_model_row['H_avg'] = h_max_avg\n",
    "                # по тестовому набору:\n",
    "                if n_test:\n",
    "                    one_model_row['H_sum_t'] = get_sum(y_test)\n",
    "                    one_model_row['H_max_t'] = get_max(y_test)\n",
    "                    one_model_row['H_min_t'] = get_min(y_test)\n",
    "                    h_max_avg_t = get_hmax_avg(y_test)\n",
    "                    one_model_row['H_avg_t'] = h_max_avg_t\n",
    "                \n",
    "                # Среднеквадратическое отклонение\n",
    "                # по исходному набору:\n",
    "                sigma = get_sigma(y_prior) #!!!\n",
    "                one_model_row['Sigma'] = sigma\n",
    "                # по тестовому набору:\n",
    "                sigma_t = get_sigma(y_test) #!!!\n",
    "                one_model_row['Sigma_t'] = sigma_t\n",
    "                \n",
    "                # Допустимая погрешность прогноза\n",
    "                # по исходному набору:\n",
    "                delta_dop = get_delta_dop(sigma) #!!!\n",
    "                one_model_row['Delta_dop'] = delta_dop\n",
    "                # по тестовому набору:\n",
    "                delta_dop_t = get_delta_dop(sigma_t) #!!!\n",
    "                one_model_row['Delta_dop_t'] = delta_dop_t\n",
    "\n",
    "                # Обеспеченность климатическая Pk \n",
    "                # по исходному набору:\n",
    "                pk = get_pk(y_prior, h_max_avg, delta_dop)\n",
    "                one_model_row['Pk'] = pk\n",
    "                # по тестовому набору:\n",
    "                pk_t = get_pk(y_test, h_max_avg_t, delta_dop_t)\n",
    "                one_model_row['Pk_t'] = pk_t\n",
    "    \n",
    "                # Обеспеченность метода (оправдываемость) Pm\n",
    "                # по исходному набору:\n",
    "                pm = get_pm(y_prior, y_predicted_prior, delta_dop) #!!!\n",
    "                one_model_row['Pm'] = pm\n",
    "                # по тестовому набору:\n",
    "                pm_t = get_pm(y_test, y_predicted_test, delta_dop_t) #!!!\n",
    "                one_model_row['Pm_t'] = pm_t\n",
    "    \n",
    "                # Среднеквадратическая погрешность прогноза\n",
    "                # по исходному набору:\n",
    "                s_forecast = get_s(y_prior, y_predicted_prior) #!!!\n",
    "                one_model_row['S'] = s_forecast if s_forecast < 10e8 else 99_999_999\n",
    "                # по тестовому набору:\n",
    "                s_forecast_t = get_s(y_test, y_predicted_test) #!!!\n",
    "                one_model_row['S_t'] = s_forecast_t if s_forecast_t < 10e8 else 99_999_999\n",
    "                \n",
    "                # Критерий эффективности метода прогнозирования \n",
    "                # климатический S/sigma\n",
    "                # по исходному набору:\n",
    "                criterion_forecast = get_criterion(s_forecast, sigma) #!!!\n",
    "                # print('!!!!!!!!!!!------criterion_forecast------!!!!!!!!!!!!!')\n",
    "                # print(criterion_forecast)\n",
    "                one_model_row['Criterion'] = criterion_forecast if criterion_forecast < 10e6 else 999_999\n",
    "                criterion_sqr = criterion_forecast ** 2.0\n",
    "                one_model_row['Criterion_sqr'] = criterion_sqr\n",
    "                \n",
    "                # по тестовому набору:\n",
    "                criterion_forecast_t = get_criterion(s_forecast_t, sigma_t) #!!!\n",
    "                one_model_row['Criterion_t'] = criterion_forecast_t if criterion_forecast_t < 10e6 else 999_999\n",
    "                criterion_sqr_t = criterion_forecast_t ** 2.0\n",
    "                one_model_row['Criterion_sqr_t'] = criterion_sqr_t\n",
    "                \n",
    "                # Корреляционное отношение ro\n",
    "                # по исходному набору:\n",
    "                correlation_forecast = get_correlation_ratio(criterion_forecast)\n",
    "                one_model_row['Correlation'] = correlation_forecast\n",
    "                # print('!!!!!!!!!!!------correlation_forecast------!!!!!!!!!!!!!')\n",
    "                # print(correlation_forecast)\n",
    "                # по тестовому набору:\n",
    "                correlation_forecast_t = get_correlation_ratio(criterion_forecast_t)\n",
    "                one_model_row['Correlation_t'] = correlation_forecast_t\n",
    "                \n",
    "                \n",
    "                # Коэффициент детерминации R2\n",
    "                # по исходному набору:\n",
    "                r2 = regr.score(X_prior, y_prior)\n",
    "                one_model_row['R2'] = r2 if r2 > -10e6 else -999_999\n",
    "                # по тестовому набору:\n",
    "                r2_t = regr.score(X_test, y_test)\n",
    "                one_model_row['R2_t'] = r2_t if r2_t > -10e6 else -999_999\n",
    "                \n",
    "    \n",
    "                # Обучение на полном наборе данных\n",
    "                try:\n",
    "                    regr_full = regr_full.fit(X_full, y_full)\n",
    "                except ValueError as error:\n",
    "                    print('Ошибка обучения на полном наборе данных:')\n",
    "                    print(error)\n",
    "                    continue\n",
    "                if serial:\n",
    "                    serial_model_full = pickle.dumps(regr_full)\n",
    "                    regr_full = pickle.loads(serial_model_full)\n",
    "\n",
    "                # Обученная на полных данных модель\n",
    "                one_model_row['Model_full'] = regr_full\n",
    "    \n",
    "                # Прогноз по полному набору (производится на \"тестовых\" данных) \n",
    "                y_predicted_full = np.ravel(regr_full.predict(X_test))         \n",
    "    \n",
    "                # по полному набору:\n",
    "                sigma_f = get_sigma(y_test) #!!!\n",
    "                one_model_row['Sigma_f'] = sigma_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                delta_dop_f = get_delta_dop(sigma_f) #!!!\n",
    "                one_model_row['Delta_dop_f'] = delta_dop_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                pm_f = get_pm(y_test, y_predicted_full, delta_dop_f) #!!!\n",
    "                one_model_row['Pm_f'] = pm_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                s_forecast_f = get_s(y_test, y_predicted_full) #!!!\n",
    "                one_model_row['S_f'] = s_forecast_f if s_forecast_f < 10e8 else 99_999_999\n",
    "    \n",
    "                # по полному набору:\n",
    "                criterion_forecast_f = get_criterion(s_forecast_f, sigma_f) #!!!\n",
    "                one_model_row['Criterion_f'] = criterion_forecast_f if criterion_forecast_f < 10e6 else 999_999\n",
    "               \n",
    "                # по полному набору:\n",
    "                correlation_forecast_f = get_correlation_ratio(criterion_forecast_f)\n",
    "                one_model_row['Correlation_f'] = correlation_forecast_f\n",
    "                \n",
    "                # Коэффициент детерминации R2\n",
    "                # по полному набору:\n",
    "                r2_f = regr_full.score(X_test, y_test)\n",
    "                one_model_row['R2_f'] = r2_f if r2_f > -10e6 else -999_999\n",
    "    \n",
    "                # print(one_model_row)\n",
    "    \n",
    "                \n",
    "                # Добавление результатов модели в результирующий список по датасету\n",
    "                result_list.append(one_model_row)\n",
    "    \n",
    "                # Запись сериализованного объекта {модель, статистика} в файл\n",
    "                write_model(one_model_row)\n",
    "                # Запись данных модели в БД (таблица models)\n",
    "                model_id = write_model_db(one_model_row)\n",
    "\n",
    "                # Создание проверочного прогноза и запись в БД (таблица test_forecasts)\n",
    "                # X_prior передается уже с нормами\n",
    "                verify_forecast(model_id, model_info=one_model_row, xy=(X_prior, y_prior), obs_years=obs_years)\n",
    "\n",
    "                \n",
    "                # #----------------------------------------------------------------------------------------------\n",
    "                # smodel = pickle.dumps(one_model_row)\n",
    "                # # with open(f'results/Models/{year}/Вилия-Стешицы_2024_гр0_OMP7.pickle', 'rb') as f:\n",
    "                # #     model_info = pickle.load(f, encoding=\"latin1\")\n",
    "                # model_info = pickle.loads(smodel)\n",
    "                # model_full = model_info['Model_full']\n",
    "                # # model_train = model_info['Model_train']\n",
    "                # # Прогноз по исходному набору\n",
    "                # pickled_y_predicted_prior = np.ravel(model_full.predict(X_prior))\n",
    "\n",
    "                # Конец итерации по модели\n",
    "\n",
    "                # print(f'Рассчитано: {ds}, {alg}, гр: {group}')\n",
    "    \n",
    "            # Сортировка результатов по каждому датасету\n",
    "            result_list.sort(\n",
    "                key=lambda row: (row['Criterion'], \n",
    "                                 -row['Correlation'], \n",
    "                                 -row['Pm'])\n",
    "            )\n",
    "    \n",
    "            datasets_result[ds] = result_list\n",
    "    \n",
    "            # Запись в .csv файл\n",
    "            # write_dataset_csv(year, result_list, ds, fieldnames, pr_group=group, mode='training')\n",
    "        \n",
    "            # Конец итерации по группе\n",
    "        \n",
    "        # Конец итерации по датасету\n",
    "       \n",
    "    return datasets_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b9338-086a-4d3b-aff9-c583952daeac",
   "metadata": {},
   "source": [
    "### Запись сериализованного объекта {модель, статистика} в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2375fa5b-c336-4244-89f3-af0612ebf93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model(model_row):\n",
    "    current_dir = os.getcwd()\n",
    "    dir_path = f'{current_dir}/results/Models/{model_row[\"Forecast_year\"]}'\n",
    "    \n",
    "    with open(f'{dir_path}/'\n",
    "              f'{model_row[\"Dataset_name\"]}_'\n",
    "              f'{model_row[\"Forecast_year\"]}_'\n",
    "              f'гр{model_row[\"Group\"]}_'\n",
    "              f'{model_row[\"Algorithm\"]}.pickle', 'wb') as pf:\n",
    "        pickle.dump(model_row, pf) #, pickle.HIGHEST_PROTOCOL\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4b324-3aa4-4349-9f86-455acbe7ad00",
   "metadata": {},
   "source": [
    "### Запись данных о модели в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b0d99cc-8c34-42fa-b6e5-3630121937a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_db(model_row):\n",
    "    conn, cursor = connect_db()\n",
    "\n",
    "    # Удалить данные модели из таблицы models\n",
    "    sql_delete = \"\"\"\n",
    "    DELETE FROM maxlevel.models\n",
    "    WHERE algorithm_id = %s\n",
    "    AND   station_id = %s\n",
    "    AND   predictors_id = %s\n",
    "    AND   forecast_year = %s\n",
    "    \"\"\"\n",
    "    arg_delete = (\n",
    "        model_row['Algorithm_id'],\n",
    "        model_row['Station_id'],\n",
    "        model_row['Predictors_id'],\n",
    "        model_row['Forecast_year'],\n",
    "    )\n",
    "    cursor.execute(sql_delete, arg_delete)\n",
    "    conn.commit()\n",
    "\n",
    "    # Записать данные модели в таблицу models\n",
    "    sql_insert = \"\"\"\n",
    "    INSERT INTO \n",
    "    maxlevel.models (\n",
    "    algorithm_id, station_id, predictors_id, forecast_year,\n",
    "    model_file, group_n, predictors, equations, dataset_name,\n",
    "    algorithm,\n",
    "\n",
    "    sigma,\n",
    "    sigma_t,\n",
    "    sigma_f,\n",
    "    delta_dop,\n",
    "    delta_dop_t,\n",
    "    delta_dop_f,\n",
    "    pm,\n",
    "    pm_t,\n",
    "    pm_f,\n",
    "    s,\n",
    "    s_t,\n",
    "    s_f,\n",
    "    criterion,\n",
    "    criterion_t,\n",
    "    criterion_f,\n",
    "    correlation,\n",
    "    correlation_t,\n",
    "    correlation_f,\n",
    "    r2,\n",
    "    r2_t,\n",
    "    r2_f,\n",
    "\n",
    "    pk,\n",
    "    criterion_sqr,\n",
    "    h_sum,\n",
    "    h_avg,\n",
    "    h_max,\n",
    "    h_min\n",
    "    )\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
    "            %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    arg_insert = (\n",
    "        model_row['Algorithm_id'],\n",
    "        model_row['Station_id'],\n",
    "        model_row['Predictors_id'],\n",
    "        model_row['Forecast_year'],\n",
    "        model_row['Model_file'],\n",
    "        model_row['Group'],\n",
    "        model_row['Predictors'],\n",
    "        model_row['Equations'],\n",
    "        model_row['Dataset_name'],\n",
    "        model_row['Algorithm'],\n",
    "\n",
    "        model_row['Sigma'],\n",
    "        model_row['Sigma_t'],\n",
    "        model_row['Sigma_f'],\n",
    "        model_row['Delta_dop'],\n",
    "        model_row['Delta_dop_t'],\n",
    "        model_row['Delta_dop_f'],\n",
    "        model_row['Pm'],\n",
    "        model_row['Pm_t'],\n",
    "        model_row['Pm_f'],\n",
    "        model_row['S'],\n",
    "        model_row['S_t'],\n",
    "        model_row['S_f'],\n",
    "        model_row['Criterion'],\n",
    "        model_row['Criterion_t'],\n",
    "        model_row['Criterion_f'],\n",
    "        model_row['Correlation'],\n",
    "        model_row['Correlation_t'],\n",
    "        model_row['Correlation_f'],\n",
    "        model_row['R2'],\n",
    "        model_row['R2_t'],\n",
    "        model_row['R2_f'],\n",
    "\n",
    "        model_row['Pk'],\n",
    "        model_row['Criterion_sqr'],\n",
    "        model_row['H_sum'],\n",
    "        model_row['H_avg'],\n",
    "        model_row['H_max'],\n",
    "        model_row['H_min'],\n",
    "        \n",
    "        \n",
    "    )\n",
    "    cursor.execute(sql_insert, arg_insert)\n",
    "    conn.commit()\n",
    "\n",
    "    # Получить model_id записанной модели\n",
    "    sql_model_id = \"\"\"\n",
    "    SELECT model_id FROM maxlevel.models\n",
    "    WHERE algorithm_id=%s and station_id=%s and predictors_id=%s and forecast_year=%s\n",
    "    \"\"\"\n",
    "    arg_model_id = (\n",
    "        model_row['Algorithm_id'],\n",
    "        model_row['Station_id'],\n",
    "        model_row['Predictors_id'],\n",
    "        model_row['Forecast_year']\n",
    "    )\n",
    "    cursor.execute(sql_model_id, arg_model_id)\n",
    "    model_id = cursor.fetchone()[0]\n",
    "    close_db(conn, cursor)\n",
    "\n",
    "    return model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0688e-fdc2-46cc-8299-b242fdb69763",
   "metadata": {},
   "source": [
    "### Функция вычисления проверочных прогнозов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5db027e-7c57-44c3-a918-0759ada85cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_forecast(model_id, model_info, xy, obs_years):\n",
    "    \n",
    "    # ds_dir = f\"data/{model_info['Forecast_year']}/Train\"\n",
    "    \n",
    "    fieldnames = [\n",
    "        '№', \n",
    "        'Год',\n",
    "        'Hmax фактический', \n",
    "        'Hф-Hср', \n",
    "        '(Hф-Hср)^2', \n",
    "        \n",
    "        'δ50% Погрешность климатических прогнозов '\n",
    "        'в долях от допустимой погрешности',\n",
    "        \n",
    "        'Hmax прогнозный', \n",
    "        'Hф-Hп', \n",
    "        '(Hф-Hп)^2', \n",
    "        \n",
    "        'δ50% Погрешность проверочных прогнозов '\n",
    "        'в долях от допустимой погрешности',\n",
    "    ]\n",
    "\n",
    "    X, y = xy\n",
    "    X_test = X.copy()\n",
    "    y_test = y.copy()\n",
    "    \n",
    "    # Forecast\n",
    "    h_max_forecast = np.ravel(model_info['Model_full'].predict(X_test))\n",
    "\n",
    "    # print('verify_forecast')\n",
    "    \n",
    "    # Hсредний\n",
    "    h_max_avg = np.mean(y)\n",
    "\n",
    "    # H - Hсредний\n",
    "    diff_fact = y_test - h_max_avg\n",
    "\n",
    "    # (H - Hсредний) в квадрате\n",
    "    diff_fact_sqr = diff_fact ** 2\n",
    "\n",
    "    # Погрешность климатических прогнозов в долях от допустимой погрешности\n",
    "    delta_dop = get_delta_dop(get_sigma(y))\n",
    "    error_climate = get_delta50(y_test, delta_dop, h_max_avg=h_max_avg)\n",
    "\n",
    "    # H - Hпрогнозный\n",
    "    diff_forecast = y_test - h_max_forecast\n",
    "\n",
    "    # (H - Hпрогнозный) в квадрате\n",
    "    diff_forecast_sqr = diff_forecast ** 2       \n",
    "\n",
    "    # Погрешность проверочных прогнозов в долях от допустимой погрешности\n",
    "    error_forecast = get_delta50(\n",
    "        y_test, delta_dop, h_max_forecast=h_max_forecast\n",
    "    )\n",
    "\n",
    "    # Номер по порядку\n",
    "    rows_num = y_test.shape[0]\n",
    "    npp = np.arange(1, rows_num + 1, 1)\n",
    "\n",
    "    # Конкатенация массивов\n",
    "    att_tuple = (\n",
    "        npp, \n",
    "        obs_years, \n",
    "        y_test, \n",
    "        diff_fact, \n",
    "        diff_fact_sqr, \n",
    "        error_climate, \n",
    "        h_max_forecast, \n",
    "        diff_forecast, \n",
    "        diff_forecast_sqr, \n",
    "        error_forecast\n",
    "    )\n",
    "    \n",
    "    arr = np.column_stack(att_tuple)\n",
    "    arr_db = arr.copy()\n",
    "    arr = arr.tolist()    \n",
    "    arr_db[:, 0] = model_id\n",
    "    \n",
    "    # Обеспеченность метода (оправдываемость) Pm\n",
    "    pm = get_pm(y_test, h_max_forecast, delta_dop)\n",
    "\n",
    "    # Запись проверочного прогноза в БД\n",
    "    write_test_forecast_db(model_id, values=arr_db)\n",
    "    \n",
    "    # # Запись проверочного прогноза в csv файл\n",
    "    # with open(\n",
    "    #     f\"results/Estimation/{model_info['Forecast_year']}/{model_info['Dataset_name']}/group-{model_info['Group']}/{model_info['Dataset_name']}\"\n",
    "    #     f\"-проверочный-гр{model_info['Group']}-{model_id}.csv\", \n",
    "    #     'w', \n",
    "    #     newline='', \n",
    "    #     encoding='utf-8'\n",
    "    # ) as csvfile:\n",
    "        \n",
    "    #     stat_header = (\n",
    "    #         f\"Таблица  - \"\n",
    "    #         f\"Проверочные прогнозы максимумов весеннего половодья\\n\"\n",
    "    #         f\"р.{model_info['Dataset_name']}\\n\"\n",
    "    #         f\"Предикторы:;; {model_info['Predictors']}\\n\"\n",
    "    #         f\"Уравнение:;; {model_info['Equations']}\\n\"\n",
    "    #         f\"Модель:;; {model_info['Algorithm']}\\n\\n\"\n",
    "    #     )\n",
    "        \n",
    "    #     csvfile.write(stat_header)\n",
    "    #     writer = csv.writer(csvfile, delimiter=';')\n",
    "    #     writer.writerow(fieldnames)\n",
    "    #     writer.writerows(arr)\n",
    "      \n",
    "    #     stat_footer = (\n",
    "    #         f\"Сумма;;{model_info['H_sum']}\\n\"  \n",
    "    #         f\"Средний;;{model_info['H_avg']}\\n\" \n",
    "    #         f\"Высший;;{model_info['H_max']}\\n\"\n",
    "    #         f\"Низший;;{model_info['H_min']}\\n\\n\"\n",
    "            \n",
    "    #         f\"σ = ;;{model_info['Sigma']};;σ -;\"\n",
    "    #         f\"среднеквадратическое отклонение (см)\\n\" \n",
    "            \n",
    "    #         f\"δдоп =;;{model_info['Delta_dop']};;δдоп -;\"\n",
    "    #         f\"допустимая погрешность прогноза (см)\\n\" \n",
    "            \n",
    "    #         f\"Pк =;;{model_info['Pk']};;Pк -;\"\n",
    "    #         f\"климатическая обеспеченность в %\\n\"\n",
    "            \n",
    "    #         f\"Pм =;;{model_info['Pm']};;Pм -;\"\n",
    "    #         f\"обеспеченность метода в %\\n\"\n",
    "            \n",
    "    #         f\"S =;;{model_info['S']};;;\"\n",
    "    #         f\"(допустимой погрешности проверочных прогнозов)\\n\"\n",
    "            \n",
    "    #         f\"S/σ =;;{model_info['Criterion']};;S -;\"\n",
    "    #         f\"среднеквадратическая погрешность (см)\\n\" \n",
    "            \n",
    "    #         f\"(S/σ)^2 =;;{model_info['Criterion_sqr']};;S/σ -;\"\n",
    "    #         f\"критерий эффективности метода прогнозирования\\n\"\n",
    "            \n",
    "    #         f\"ρ =;;{model_info['Correlation']};;ρ -;\"\n",
    "    #         f\"корреляционное отношение\\n\"\n",
    "            \n",
    "    #         f\";;;;;(оценка эффективности метода прогнозирования)\\n\"\n",
    "    #         f\";;;;δ50% -;погрешность (ошибка) прогнозов (см)\\n\"\n",
    "    #     )\n",
    "        \n",
    "    #     csvfile.write(stat_footer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea388f5-34df-4d8d-a652-32bbfecc85da",
   "metadata": {},
   "source": [
    "#### Функция записи проверочного прогноза в БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7042f8b8-a7da-4a7d-9a93-855f4151683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test_forecast_db(model_id, values):\n",
    "    conn, cursor = connect_db()\n",
    "\n",
    "    sql = \"\"\"\n",
    "    INSERT INTO maxlevel.test_forecasts (\n",
    "        model_id, \n",
    "        obs_year, \n",
    "        h_max_fact, \n",
    "        diff_fact, \n",
    "        diff_fact_sqr,\n",
    "        error_climate,\n",
    "        h_max_forecast, \n",
    "        diff_forecast, \n",
    "        diff_forecast_sqr, \n",
    "        error_forecast\n",
    "    )\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.executemany(sql, values)\n",
    "    conn.commit()\n",
    "\n",
    "    close_db(conn, cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421c981-0172-402c-988e-779028c37738",
   "metadata": {},
   "source": [
    "### Функция удаления моделей из БД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0e06f6f-35cb-47bd-9484-1a3429fffee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_models_db(stations_id=None):\n",
    "    conn, cursor = connect_db()\n",
    "\n",
    "    if stations_id:\n",
    "        # Удалить модели для указанных гидропостов из таблицы models\n",
    "        sql = \"\"\"\n",
    "        DELETE FROM maxlevel.models\n",
    "        WHERE station_id = ANY(%s)\n",
    "        \"\"\"\n",
    "        arg = (stations_id,)\n",
    "        cursor.execute(sql, arg)\n",
    "    else:\n",
    "        # Удалить все модели из таблицы models\n",
    "        sql = \"\"\"\n",
    "        DELETE FROM maxlevel.models\n",
    "        \"\"\"\n",
    "        cursor.execute(sql)\n",
    "    conn.commit()\n",
    "    close_db(conn, cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142eeb2-bb04-4690-ad10-21c38696f8ef",
   "metadata": {},
   "source": [
    "#### Запуск процесса удаления моделей множественной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff826ece-f1e2-4be1-946b-6bf1f833940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_models_db(stations_id=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae6e8e-07c7-4de1-b8cc-0ae599007b06",
   "metadata": {},
   "source": [
    "#### Запуск процесса обучения моделей множественной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c99dc269-0381-4cf0-b186-3381a52b6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Верхнедвинск, гр. 1, StackingRegressorRidgeCV - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n",
      "Верхнедвинск, гр. 2, StackingRegressorRidgeCV - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n",
      "Сураж, гр. 1, StackingRegressorRidgeCV - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n",
      "Сураж, гр. 2, StackingRegressorRidgeCV - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n"
     ]
    }
   ],
   "source": [
    "_ = train_models(2024, pr_group=None, n_test=100, norms=True, aug_n=1000, aug_mpl=30, aug_pow=2, \n",
    "                 aug_mirror=False, grid_search=False, scaler_x=None, scaler_y=None, \n",
    "                 shuffle=True, serial=True,\n",
    "                 stations_id=[73131, 73111])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5f2bc-fc27-4c4c-9e7e-e2d0c0c88a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b42197-8de4-45dd-93ca-c9cc147fb67f",
   "metadata": {},
   "source": [
    "#### Альтернативная функция обучения моделей на полном наборе данных с записью на диск и в БД (таблица models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d10cf72-66c7-4b15-804c-eb601ff39fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models2(year, pr_group, n_test=None,\n",
    "                 norms=True, aug_n=0, aug_mpl=30,\n",
    "                 aug_pow=2, aug_mirror=False, grid_search=False,\n",
    "                 scaler_x=None, scaler_y=None, shuffle=True,\n",
    "                 serial=True, top_best=None,\n",
    "                 stations_id=None):\n",
    "    \n",
    "    # ds_dir = f'data/{year}/Train'\n",
    "    \n",
    "    # {station: id, ...}\n",
    "    if stations_id:\n",
    "        datasets = {st: stid for st, stid in get_datasets_db().items() if stid in stations_id}\n",
    "    else:\n",
    "        datasets = get_datasets_db()\n",
    "                     \n",
    "    fieldnames = [\n",
    "        'Predictors', \n",
    "        'Equations', \n",
    "        'Algorithm', \n",
    "        'Criterion', \n",
    "        'Correlation', \n",
    "        'Pm',\n",
    "        'R2',\n",
    "\n",
    "        'Criterion_t', \n",
    "        'Correlation_t', \n",
    "        'Pm_t',\n",
    "        'R2_t',\n",
    "\n",
    "        'Criterion_f', \n",
    "        'Correlation_f', \n",
    "        'Pm_f',\n",
    "        'R2_f',\n",
    "\n",
    "        # 'Group',\n",
    "        # 'Augmentation',\n",
    "        # 'Data size',\n",
    "        # 'Normalization',\n",
    "        # 'Equations',\n",
    "    ]\n",
    "\n",
    "    # Описание структуры данных переменной datasets_result\n",
    "    # datasets_result = {\n",
    "    #     \"hydropost_0\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    #     ...,\n",
    "    #     \"hydropost_n\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    # Итерация по датасетам\n",
    "    datasets_result = dict()\n",
    "    for ds in datasets:\n",
    "\n",
    "        # Получить все группы по датасету\n",
    "        ds_groups = get_predictors_db(datasets[ds])\n",
    "        \n",
    "        # Итерация по группам предикторов\n",
    "        for group, (predictors_id, pr_list) in enumerate(ds_groups):\n",
    "            if group == 0:\n",
    "                # По группе 0 модели не обучаются - это все предикторы + год + максимальный уровень и его дата\n",
    "                continue\n",
    "            if pr_group is not None:\n",
    "                if group != pr_group:\n",
    "                    continue\n",
    "        \n",
    "            result_list = []\n",
    "            \n",
    "            X, y, obs_years = get_river_dataset_db(datasets[ds], pr_list=pr_list)\n",
    "    \n",
    "            # Проверочный набор данных (исходный)\n",
    "            X_prior = X.copy()\n",
    "            y_prior = y.copy()\n",
    "            \n",
    "            # Полный набор данных\n",
    "            X_full = X.copy()\n",
    "            y_full = y.copy()\n",
    "\n",
    "            if aug_mpl > 1:\n",
    "                X_full, y_full = augment_data(X_full, y_full, aug_mpl, aug_pow=aug_pow, mirror=aug_mirror, clear=False)\n",
    "            \n",
    "            if shuffle:\n",
    "                X_full, y_full = shuffle_xy(X_full, y_full, shuffle=True)\n",
    "            \n",
    "            if n_test:\n",
    "                X_train, y_train, X_test, y_test = train_test_split(X_full, y_full, n_test, split=True)\n",
    "            \n",
    "            norms_data = None\n",
    "            if norms:\n",
    "                norms_data = get_norms(ds) # сделать для БД\n",
    "                # Подстановка норм в исходный набор данных (пессимистичный сценарий)\n",
    "                X_prior = test_norm(X_prior, pr_list, norms_data)\n",
    "                # Подстановка норм в тестовый набор данных\n",
    "                X_test = test_norm(X_test, pr_list, norms_data)\n",
    "                # Подстановка норм в полный набор данных не требуется\n",
    "  \n",
    "            transformer_y = get_transformer(scaler_y, n_samples=y_train.shape[0]) # !!!\n",
    "            transformer_x = get_transformer(scaler_x, n_samples=y_train.shape[0]) # !!!\n",
    "            transformer_y_full = get_transformer(scaler_y, n_samples=y_full.shape[0]) # !!!\n",
    "            transformer_x_full = get_transformer(scaler_x, n_samples=y_full.shape[0]) # !!!\n",
    "\n",
    "            # Список оцениваемых ререссионных моделей !!!!!\n",
    "            algorithms = get_regressors_list_db()\n",
    "            \n",
    "            regressors = get_regressors_objects(grid_search=grid_search)\n",
    "            regressors_full = get_regressors_objects(grid_search=grid_search)\n",
    "                \n",
    "            # Итерация по моделям регрессии\n",
    "            for alg_id, alg in algorithms:\n",
    "                model = regressors[alg]\n",
    "                model_full = regressors_full[alg]\n",
    "\n",
    "                # Препроцессинг - трансформация целевых значений y\n",
    "                if scaler_y: \n",
    "                    regressor = TransformedTargetRegressor(regressor=model, transformer=transformer_y)\n",
    "                    regressor_full = TransformedTargetRegressor(regressor=model_full, transformer=transformer_y_full)\n",
    "                else:\n",
    "                    regressor = model\n",
    "                    regressor_full = model_full\n",
    "                \n",
    "                one_model_row = dict()                \n",
    "                \n",
    "                # Препроцессинг - трансформация признаков X\n",
    "                regr = make_pipeline(transformer_x, regressor) if transformer_x else regressor\n",
    "                regr_full = make_pipeline(transformer_x_full, regressor_full) if transformer_x_full else regressor_full\n",
    "                \n",
    "                # Обучение на тренировочном наборе\n",
    "                # regr = regr.fit(X_train, y_train)\n",
    "                try:\n",
    "                    regr.fit(X_train, y_train)\n",
    "                except ValueError as error:\n",
    "                    print(f'{ds}, гр. {group}, {alg} - Ошибка обучения на тренировочном наборе данных:')\n",
    "                    print(error)\n",
    "                    continue\n",
    "                if serial:\n",
    "                    serial_model = pickle.dumps(regr)\n",
    "                    regr = pickle.loads(serial_model)\n",
    "                \n",
    "                # Обученная на тренировочном наборе данных модель\n",
    "                one_model_row['Model_train'] = regr\n",
    "                    \n",
    "                # Прогноз по исходному набору на тренировочной модели\n",
    "                y_predicted_prior = np.ravel(regr.predict(X_prior))\n",
    "                \n",
    "                # Прогноз по тестовому набору на тренировочной модели \n",
    "                y_predicted_test = np.ravel(regr.predict(X_test))\n",
    "    \n",
    "                \n",
    "                # Очистка значений строк предикторов и уравнений перед переходом к следующей модели\n",
    "                coef = None\n",
    "                intercept = None\n",
    "                \n",
    "                try:\n",
    "                    coef = regr.best_estimator_.coef_\n",
    "                    intercept = regr.best_estimator_.intercept_\n",
    "                    \n",
    "                    if isinstance(intercept, np.ndarray):\n",
    "                        intercept = intercept[0]\n",
    "                except Exception as error:\n",
    "                                    \n",
    "                    try:\n",
    "                        coef = regr.coef_\n",
    "                        intercept = regr.intercept_\n",
    "                    \n",
    "                        if isinstance(intercept, np.ndarray):\n",
    "                            intercept = intercept[0]\n",
    "                        # print(\"ERROR1 START\")\n",
    "                        # print(error)\n",
    "                        # print(\"ERROR1 FINISH\")\n",
    "                    except Exception as error:\n",
    "                        ...\n",
    "                        # print(\"ERROR2 START\")\n",
    "                        # print(error)\n",
    "                        # print(\"ERROR2 FINISH\")\n",
    "                    \n",
    "                \n",
    "                try:\n",
    "                    # Коэффициенты уравнения (если есть)\n",
    "                    coef = np.around(np.ravel(coef), 3)\n",
    "                    intercept = round(intercept, 3)\n",
    "                    \n",
    "                    predictors_coef = {f: c for f, c \n",
    "                                       in zip(pr_list, coef) if c != 0.0}\n",
    "                    \n",
    "                    predictors = \", \".join(predictors_coef.keys())\n",
    "                    \n",
    "                    equation = (\n",
    "                        str(intercept) \n",
    "                        + ' ' \n",
    "                        + ' '.join(str(c) + '*' \n",
    "                                   + f for f, c in predictors_coef.items())\n",
    "                    )\n",
    "                    \n",
    "                    equation = equation.replace(\" -\", \"-\")\n",
    "                    equation = equation.replace(\" \", \" + \")\n",
    "                    equation = equation.replace(\"-\", \" - \")\n",
    "        \n",
    "                    one_model_row['Predictors'] = predictors\n",
    "                    one_model_row['Equations'] = equation\n",
    "                except Exception as error:\n",
    "                    # print(\"ERROR3 START\")\n",
    "                    # print(error)\n",
    "                    # print(\"ERROR3 FINISH\")\n",
    "                    one_model_row['Predictors'] = \", \".join(pr_list)\n",
    "                    one_model_row['Equations'] = \"Непараметрическая (нелинейная) модель\"\n",
    "    \n",
    "                \n",
    "                \n",
    "                # Код алгоритма\n",
    "                one_model_row['Algorithm_id'] = alg_id\n",
    "\n",
    "                # Код гидропоста\n",
    "                one_model_row['Station_id'] = datasets[ds]\n",
    "\n",
    "                # Код списка предикторов\n",
    "                one_model_row['Predictors_id'] = predictors_id\n",
    "                    \n",
    "                # Год прогноза\n",
    "                one_model_row['Forecast_year'] = year\n",
    "\n",
    "                # Название датасета (гидропоста)\n",
    "                one_model_row['Dataset_name'] = ds\n",
    "\n",
    "                # Исходный список предикторов\n",
    "                one_model_row['Predictors_list'] = pr_list\n",
    "\n",
    "                # Нормы\n",
    "                one_model_row['Norms_data'] = norms_data\n",
    "    \n",
    "                # Группа предикторов\n",
    "                one_model_row['Group'] = group\n",
    "                    \n",
    "                # Название метода\n",
    "                one_model_row['Algorithm'] = alg\n",
    "\n",
    "                # Путь к файлу модели на бэкенде\n",
    "                model_file = (\n",
    "                    f'results/Models/{one_model_row[\"Forecast_year\"]}/'\n",
    "                    f'{one_model_row[\"Dataset_name\"]}_'\n",
    "                    f'{one_model_row[\"Forecast_year\"]}_'\n",
    "                    f'гр{one_model_row[\"Group\"]}_'\n",
    "                    f'{one_model_row[\"Algorithm\"]}.pickle'\n",
    "                )\n",
    "                one_model_row['Model_file'] = model_file\n",
    "\n",
    "                # Сумма, максимум, минимум, среднее максимальных уровней\n",
    "                # по исходному набору:\n",
    "                one_model_row['H_sum'] = get_sum(y_prior)\n",
    "                one_model_row['H_max'] = get_max(y_prior)\n",
    "                one_model_row['H_min'] = get_min(y_prior)\n",
    "                h_max_avg = get_hmax_avg(y_prior)\n",
    "                one_model_row['H_avg'] = h_max_avg\n",
    "                # по тестовому набору:\n",
    "                if n_test:\n",
    "                    one_model_row['H_sum_t'] = get_sum(y_test)\n",
    "                    one_model_row['H_max_t'] = get_max(y_test)\n",
    "                    one_model_row['H_min_t'] = get_min(y_test)\n",
    "                    h_max_avg_t = get_hmax_avg(y_test)\n",
    "                    one_model_row['H_avg_t'] = h_max_avg_t\n",
    "                \n",
    "                # Среднеквадратическое отклонение\n",
    "                # по исходному набору:\n",
    "                sigma = get_sigma(y_prior) #!!!\n",
    "                one_model_row['Sigma'] = sigma\n",
    "                # по тестовому набору:\n",
    "                sigma_t = get_sigma(y_test) #!!!\n",
    "                one_model_row['Sigma_t'] = sigma_t\n",
    "                \n",
    "                # Допустимая погрешность прогноза\n",
    "                # по исходному набору:\n",
    "                delta_dop = get_delta_dop(sigma) #!!!\n",
    "                one_model_row['Delta_dop'] = delta_dop\n",
    "                # по тестовому набору:\n",
    "                delta_dop_t = get_delta_dop(sigma_t) #!!!\n",
    "                one_model_row['Delta_dop_t'] = delta_dop_t\n",
    "\n",
    "                # Обеспеченность климатическая Pk \n",
    "                # по исходному набору:\n",
    "                pk = get_pk(y_prior, h_max_avg, delta_dop)\n",
    "                one_model_row['Pk'] = pk\n",
    "                # по тестовому набору:\n",
    "                pk_t = get_pk(y_test, h_max_avg_t, delta_dop_t)\n",
    "                one_model_row['Pk_t'] = pk_t\n",
    "    \n",
    "                # Обеспеченность метода (оправдываемость) Pm\n",
    "                # по исходному набору:\n",
    "                pm = get_pm(y_prior, y_predicted_prior, delta_dop) #!!!\n",
    "                one_model_row['Pm'] = pm\n",
    "                # по тестовому набору:\n",
    "                pm_t = get_pm(y_test, y_predicted_test, delta_dop_t) #!!!\n",
    "                one_model_row['Pm_t'] = pm_t\n",
    "    \n",
    "                # Среднеквадратическая погрешность прогноза\n",
    "                # по исходному набору:\n",
    "                s_forecast = get_s(y_prior, y_predicted_prior) #!!!\n",
    "                one_model_row['S'] = s_forecast if s_forecast < 10e8 else 99_999_999\n",
    "                # по тестовому набору:\n",
    "                s_forecast_t = get_s(y_test, y_predicted_test) #!!!\n",
    "                one_model_row['S_t'] = s_forecast_t if s_forecast_t < 10e8 else 99_999_999\n",
    "                \n",
    "                # Критерий эффективности метода прогнозирования \n",
    "                # климатический S/sigma\n",
    "                # по исходному набору:\n",
    "                criterion_forecast = get_criterion(s_forecast, sigma) #!!!\n",
    "                # print('!!!!!!!!!!!------criterion_forecast------!!!!!!!!!!!!!')\n",
    "                # print(criterion_forecast)\n",
    "                one_model_row['Criterion'] = criterion_forecast if criterion_forecast < 10e6 else 999_999\n",
    "                criterion_sqr = criterion_forecast ** 2.0\n",
    "                one_model_row['Criterion_sqr'] = criterion_sqr\n",
    "                \n",
    "                # по тестовому набору:\n",
    "                criterion_forecast_t = get_criterion(s_forecast_t, sigma_t) #!!!\n",
    "                one_model_row['Criterion_t'] = criterion_forecast_t if criterion_forecast_t < 10e6 else 999_999\n",
    "                criterion_sqr_t = criterion_forecast_t ** 2.0\n",
    "                one_model_row['Criterion_sqr_t'] = criterion_sqr_t\n",
    "                \n",
    "                # Корреляционное отношение ro\n",
    "                # по исходному набору:\n",
    "                correlation_forecast = get_correlation_ratio(criterion_forecast)\n",
    "                one_model_row['Correlation'] = correlation_forecast\n",
    "                # print('!!!!!!!!!!!------correlation_forecast------!!!!!!!!!!!!!')\n",
    "                # print(correlation_forecast)\n",
    "                # по тестовому набору:\n",
    "                correlation_forecast_t = get_correlation_ratio(criterion_forecast_t)\n",
    "                one_model_row['Correlation_t'] = correlation_forecast_t\n",
    "                \n",
    "                \n",
    "                # Коэффициент детерминации R2\n",
    "                # по исходному набору:\n",
    "                r2 = regr.score(X_prior, y_prior)\n",
    "                one_model_row['R2'] = r2 if r2 > -10e6 else -999_999\n",
    "                # по тестовому набору:\n",
    "                r2_t = regr.score(X_test, y_test)\n",
    "                one_model_row['R2_t'] = r2_t if r2_t > -10e6 else -999_999\n",
    "                \n",
    "    \n",
    "                # Обучение на полном наборе данных\n",
    "                try:\n",
    "                    regr_full = regr_full.fit(X_full, y_full)\n",
    "                except ValueError as error:\n",
    "                    print('Ошибка обучения на полном наборе данных:')\n",
    "                    print(error)\n",
    "                    continue\n",
    "                if serial:\n",
    "                    serial_model_full = pickle.dumps(regr_full)\n",
    "                    regr_full = pickle.loads(serial_model_full)\n",
    "\n",
    "                # Обученная на полных данных модель\n",
    "                one_model_row['Model_full'] = regr_full\n",
    "    \n",
    "                # Прогноз по полному набору (производится на \"тестовых\" данных) \n",
    "                y_predicted_full = np.ravel(regr_full.predict(X_test))         \n",
    "    \n",
    "                # по полному набору:\n",
    "                sigma_f = get_sigma(y_test) #!!!\n",
    "                one_model_row['Sigma_f'] = sigma_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                delta_dop_f = get_delta_dop(sigma_f) #!!!\n",
    "                one_model_row['Delta_dop_f'] = delta_dop_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                pm_f = get_pm(y_test, y_predicted_full, delta_dop_f) #!!!\n",
    "                one_model_row['Pm_f'] = pm_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                s_forecast_f = get_s(y_test, y_predicted_full) #!!!\n",
    "                one_model_row['S_f'] = s_forecast_f if s_forecast_f < 10e8 else 99_999_999\n",
    "    \n",
    "                # по полному набору:\n",
    "                criterion_forecast_f = get_criterion(s_forecast_f, sigma_f) #!!!\n",
    "                one_model_row['Criterion_f'] = criterion_forecast_f if criterion_forecast_f < 10e6 else 999_999\n",
    "               \n",
    "                # по полному набору:\n",
    "                correlation_forecast_f = get_correlation_ratio(criterion_forecast_f)\n",
    "                one_model_row['Correlation_f'] = correlation_forecast_f\n",
    "                \n",
    "                # Коэффициент детерминации R2\n",
    "                # по полному набору:\n",
    "                r2_f = regr_full.score(X_test, y_test)\n",
    "                one_model_row['R2_f'] = r2_f if r2_f > -10e6 else -999_999\n",
    "    \n",
    "                # print(one_model_row)\n",
    "    \n",
    "                \n",
    "                # Добавление результатов модели в результирующий список по датасету\n",
    "                result_list.append(one_model_row)\n",
    "    \n",
    "                # Запись сериализованного объекта {модель, статистика} в файл\n",
    "                write_model(one_model_row)\n",
    "                # Запись данных модели в БД (таблица models)\n",
    "                model_id = write_model_db(one_model_row)\n",
    "\n",
    "                # Создание проверочного прогноза и запись в БД (таблица test_forecasts)\n",
    "                # X_prior передается уже с нормами\n",
    "                verify_forecast(model_id, model_info=one_model_row, xy=(X_prior, y_prior), obs_years=obs_years)\n",
    "\n",
    "                \n",
    "                # #----------------------------------------------------------------------------------------------\n",
    "                # smodel = pickle.dumps(one_model_row)\n",
    "                # # with open(f'results/Models/{year}/Вилия-Стешицы_2024_гр0_OMP7.pickle', 'rb') as f:\n",
    "                # #     model_info = pickle.load(f, encoding=\"latin1\")\n",
    "                # model_info = pickle.loads(smodel)\n",
    "                # model_full = model_info['Model_full']\n",
    "                # # model_train = model_info['Model_train']\n",
    "                # # Прогноз по исходному набору\n",
    "                # pickled_y_predicted_prior = np.ravel(model_full.predict(X_prior))\n",
    "\n",
    "                # Конец итерации по модели\n",
    "\n",
    "                # print(f'Рассчитано: {ds}, {alg}, гр: {group}')\n",
    "    \n",
    "            # Сортировка результатов по каждому датасету\n",
    "            result_list.sort(\n",
    "                key=lambda row: (row['Criterion'], \n",
    "                                 -row['Correlation'], \n",
    "                                 -row['Pm'])\n",
    "            )\n",
    "    \n",
    "            datasets_result[ds] = result_list\n",
    "    \n",
    "            # Запись в .csv файл\n",
    "            # write_dataset_csv(year, result_list, ds, fieldnames, pr_group=group, mode='training')\n",
    "        \n",
    "            # Конец итерации по группе\n",
    "        \n",
    "        # Конец итерации по датасету\n",
    "       \n",
    "    return datasets_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784483c-b393-4a7a-995b-76d925cae75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18eeeda7-7510-4c6c-8445-11fa3e57fb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Верхнедвинск, гр. 1, OMP6 - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n",
      "Верхнедвинск, гр. 2, OMP6 - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n",
      "Сураж, гр. 1, OMP6 - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n",
      "Сураж, гр. 2, OMP6 - Ошибка обучения на тренировочном наборе данных:\n",
      "The number of atoms cannot be more than the number of features\n"
     ]
    }
   ],
   "source": [
    "_ = train_models2(2024, pr_group=None, n_test=100, norms=True, aug_n=1000, aug_mpl=30, aug_pow=2, \n",
    "                 aug_mirror=False, grid_search=False, scaler_x=None, scaler_y=None, \n",
    "                 shuffle=True, serial=True,\n",
    "                 stations_id=[73131, 73111])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f2536-dfb3-49a9-bd50-5092b945d706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
