{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75541273-d8a0-40fe-b676-e024f18f6b41",
   "metadata": {},
   "source": [
    "#### Импорт необходимых объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d31a67-e1f4-4f73-b386-b54e9da7cea7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.interpolate import splrep, splev\n",
    "import pickle\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import Lars, LarsCV\n",
    "from sklearn.linear_model import LassoLars, LassoLarsCV\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, LassoLarsCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c8264-7ddd-469d-9581-8eefb3f13322",
   "metadata": {},
   "source": [
    "#### Функция чтения набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cff566a-8886-47b1-a651-f3df2c864e39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !!!\n",
    "def get_river_dataset(fname, pr_list=None, y_name='H_max'):\n",
    "    pr_arr = []\n",
    "    y_arr = []\n",
    "    with open(fname, newline='') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            pr_arr_row = []\n",
    "            for pr in pr_list:\n",
    "                pr_arr_row.append(row[pr])\n",
    "\n",
    "            pr_arr.append(pr_arr_row)\n",
    "            y_arr.append(row[y_name])\n",
    "    X = np.asarray(pr_arr, dtype=np.float64)\n",
    "    y = np.asarray(y_arr, dtype=np.float64)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96dafa4-e1f7-4c26-b28d-3dfd6a2b1b14",
   "metadata": {},
   "source": [
    "#### Среднеквадратическая погрешность прогноза S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad85fc4-c2cd-4a9f-8a0a-bb4fe4cdfe0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_s(h_max, h_forecast):\n",
    "    # Среднеквадратическая погрешность прогноза\n",
    "    n = h_max.shape[0]\n",
    "    sqr_diff = np.sum((h_max - h_forecast) ** 2) / (n - 1)\n",
    "    std = sqr_diff ** 0.5\n",
    "    return std    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e98c4-4841-4b3f-b61b-284bc99af53d",
   "metadata": {},
   "source": [
    "#### Среднеквадратическое отклонение sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5079b8f9-3ae5-47a1-a6f3-214bdf69ad77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_sigma(h_max):\n",
    "    # Среднеквадратическая погрешность климатическая.\n",
    "    # Рассчитывается только по всей совокупности данных.\n",
    "    return np.std(h_max, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10fa8d-d7cd-44f4-b4f2-4f652409383f",
   "metadata": {},
   "source": [
    "#### Допустимая погрешность прогноза delta_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d4648d-8e14-4c63-bcf8-6a3c2cb7a829",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_delta_dop(sigma):\n",
    "    return 0.674 * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1c7ff-c4e1-4639-a877-d3b78d944dea",
   "metadata": {},
   "source": [
    "#### Критерий критерий применимости и качества методики S/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f92739a9-3b12-44bd-b51e-4ffd73e828a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_criterion(s, sigma):\n",
    "    return s / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4c686-2946-4ea0-980a-172b2ff4dd64",
   "metadata": {},
   "source": [
    "#### Обеспеченность метода (оправдываемость) Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ca5adf-fbf3-49a4-8458-06d103bfacdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_pm(h_max, h_forecast, delta_dop):\n",
    "    diff = np.abs(h_max - h_forecast) / delta_dop\n",
    "    trusted_values = diff[diff <= 1.0]\n",
    "    m = trusted_values.shape[0]\n",
    "    n = h_max.shape[0]\n",
    "    return m / n * 100.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d93f0-e243-4107-b007-cb24efbdc6f6",
   "metadata": {},
   "source": [
    "#### Корреляционное отношение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a501afb2-3736-4023-a488-a19747e54910",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_correlation_ratio(criterion):\n",
    "    c_1 = (1 - criterion ** 2)\n",
    "    ro = c_1 ** 0.5 if c_1 > 0 else 0\n",
    "    return ro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0cc72-ec19-437b-b6c5-8010c2e3a5bf",
   "metadata": {},
   "source": [
    "#### Функция записи списка моделей с их характеристиками в csv файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c76ad0-d75c-4746-90dc-0d8465218463",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def write_dataset_csv(year, dataset, dataset_name, fieldnames, pr_group, mode='training'):\n",
    "    if mode == 'estimation':\n",
    "        dir_path = f'results/Estimation/{year}/{dataset_name}/group-{pr_group}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Оценка.csv'\n",
    "    elif mode == 'training':\n",
    "        dir_path = f'results/Models/{year}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Обучение.csv'\n",
    "    elif mode == 'forecast':\n",
    "        dir_path = f'results/Forecast/{year}/'\n",
    "        file_name = f'{dataset_name}-гр{pr_group}-Прогноз.csv'\n",
    "    else:\n",
    "        ...\n",
    "    \n",
    "    with open(\n",
    "        f'{dir_path}'\n",
    "        f'{file_name}', \n",
    "        'w', newline='', encoding='utf-8'\n",
    "    ) as csvfile:\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, \n",
    "                                delimiter=';', extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2d2a0-f9da-46e5-a0ae-45e8fe0c2482",
   "metadata": {},
   "source": [
    "#### Функция разделения набора данных на тренировочный и тестовый"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82db9e50-fadc-4c7f-9151-58c2b74df8c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!! -------->\n",
    "def train_test_split(X, y, n_test, split=True):\n",
    "    if split:   \n",
    "        X_train = X[:-n_test].copy()\n",
    "        y_train = y[:-n_test].copy()\n",
    "        X_test = X[-n_test:].copy()\n",
    "        y_test = y[-n_test:].copy()\n",
    "    else:\n",
    "        X_train = X.copy()\n",
    "        y_train = y.copy()\n",
    "        X_test = X.copy()\n",
    "        y_test = y.copy()\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05933c74-63f1-4a9e-bc5b-35842087b385",
   "metadata": {},
   "source": [
    "#### Функция перемешивания данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14522f32-b14c-4dfd-8286-b8cf15b960c2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!! ------->\n",
    "def shuffle_xy(X, y, shuffle=True):\n",
    "    if shuffle:\n",
    "        # Перемешивание данных\n",
    "        Xy = np.column_stack((X, y))\n",
    "        rng = np.random.default_rng(42)\n",
    "        rng.shuffle(Xy)\n",
    "        y_sh = Xy[:, -1]\n",
    "        X_sh = Xy[:,:-1]\n",
    "    else:\n",
    "        y_sh = y.copy()\n",
    "        X_sh = X.copy()\n",
    "    return X_sh, y_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719a5f9-176e-473d-8ce7-b7744d9c6e19",
   "metadata": {},
   "source": [
    "#### Функция формирования тестового набора данных с подстановкой нормированных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b464d45-843c-4792-ab02-2819edddee65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def test_norm(x, pr_list, norms):\n",
    "    x_norm = np.copy(x)\n",
    "    for col, pr in enumerate(pr_list):\n",
    "        if pr in norms:\n",
    "            x_norm[:, col:col+1] = norms[pr]\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3a942-95d0-4f7f-aa39-82f61624b070",
   "metadata": {},
   "source": [
    "#### Функция получения датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7628a977-9d54-4129-928c-625b4e80d107",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_datasets():\n",
    "    datasets = {\n",
    "        #'Неман-Белица': 'Неман',\n",
    "        #'Неман-Гродно': 'Неман',\n",
    "        #'Неман-Мосты': 'Неман',\n",
    "        #'Неман-Столбцы': 'Неман',\n",
    "\n",
    "        'Вилия-Стешицы': 'Вилия',\n",
    "        #'Вилия-Михалишки': 'Вилия',\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83168b0-64e2-4e58-a47b-f49f677a9eb4",
   "metadata": {},
   "source": [
    "#### Функция получения списка предикторов по названию датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca1eea5-a699-4b0e-91fa-9fe6f4451ea8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !!!\n",
    "def get_predictors(dataset_name, group=None):\n",
    "\n",
    "    datasets = get_datasets()   \n",
    "    predictors_lists = {\n",
    "        'Неман': (\n",
    "            ['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'X3', 'Xs'],\n",
    "            ['Smax', 'H_2802', 'X', 'X1', 'X3'],\n",
    "            ['S_2802', 'H_2802', 'X2', 'X3', 'Xs'],\n",
    "        ),\n",
    "        'Вилия': (\n",
    "            ['S_2802', 'Smax', 'H_2802', 'X', 'X1', 'X2', 'X3', 'Xs', 'L_max', 'L_2802', 'Q12', 'Q01', 'Q02', 'Y_sum'],\n",
    "            ['Smax', 'H_2802', 'X', 'X1', 'X3', 'L_max', 'Y_sum'],\n",
    "            ['S_2802', 'H_2802', 'X2', 'X3', 'Xs', 'L_2802', 'Y_sum'],\n",
    "        )\n",
    "    }\n",
    "    result = predictors_lists[datasets[dataset_name]] if group is None else \\\n",
    "             predictors_lists[datasets[dataset_name]][group]\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790ff38-4843-40b6-ac40-a9efec555315",
   "metadata": {},
   "source": [
    "#### Функция получения нормированных значений предикторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a741f9dd-31c9-4ea6-a1a8-b2b684b05013",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!!!\n",
    "def get_norms(dataset_name):\n",
    "    norms_list = {\n",
    "        'Неман-Белица': {'X1': 46.0, 'X2':35.0},\n",
    "        'Неман-Гродно': {'X1': 36.0, 'X2':26.0},\n",
    "        'Неман-Мосты': {'X1': 40.0, 'X2':31.0},\n",
    "        'Неман-Столбцы': {'X1': 43.0, 'X2':34.0},\n",
    "\n",
    "        'Вилия-Стешицы': {'S_max': 67.0, 'X': 112.0, 'X1': 40.0, 'X2': 33.0, 'L_max': 60.0},\n",
    "        'Вилия-Михалишки': {'S_max': 60.0, 'X': 116.0, 'X1': 46.0, 'X2': 37.0, 'L_max': 57.0},\n",
    "    }\n",
    "    return norms_list[dataset_name]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83f925-d3c5-46ea-bbba-cb3cf9605113",
   "metadata": {},
   "source": [
    "#### Функция получения аугментированных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc8e198a-5d6a-4815-bbbc-94ae2d2c11d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def augment_data(x_data, y_data, aug_n, aug_pow=2, mirror=True, s=None):\n",
    "    #print(x_data)\n",
    "    data_len = len(y_data)\n",
    "    \n",
    "    x_points = np.linspace(0, data_len, data_len)\n",
    "    \n",
    "    x_splitted = np.hsplit(x_data, x_data.shape[1])\n",
    "    #print(x_splitted)\n",
    "\n",
    "    x_list = []\n",
    "    for arr in x_splitted:\n",
    "        x_spl = splrep(x_points, arr, k=aug_pow, s=s)\n",
    "        x_points_n = np.linspace(0, data_len, aug_n)\n",
    "        x_col_augmented = splev(x_points_n, x_spl)\n",
    "        x_list.append(x_col_augmented)\n",
    "    x_augmented = np.array(x_list).T\n",
    "\n",
    "    y_points = np.linspace(0, data_len, data_len)\n",
    "    y_spl = splrep(y_points, y_data, k=aug_pow, s=s)\n",
    "    y_points_n = np.linspace(0, data_len, aug_n)\n",
    "    y_augmented = splev(y_points_n, y_spl)\n",
    "        \n",
    "    \n",
    "    x_aug_round = np.round(x_augmented, decimals=-1)\n",
    "    y_aug_round = np.round(y_augmented, decimals=1)\n",
    "\n",
    "    x_data_round = np.round(x_data, decimals=-1)\n",
    "    y_data_round = np.round(y_data, decimals=1)\n",
    "    \n",
    "    \n",
    "    mx = (x_aug_round[:, None] == x_data_round).all(-1).any(1)\n",
    "    x_aug_clear = x_augmented[~mx].copy()\n",
    "    y_aug_clear = y_augmented[~mx].copy()\n",
    "    \n",
    "    # my = np.in1d(y_aug_round, y_data_round)\n",
    "    # x_aug_clear = x_aug_clear[~my]\n",
    "    # y_aug_clear = y_aug_clear[~my]\n",
    "    \n",
    "    print('x_aug_clear.shape', x_aug_clear.shape)\n",
    "    print('x_augmented.shape', x_augmented.shape)\n",
    "\n",
    "    if mirror:\n",
    "        x_mirror = np.mean(x_augmented) - x_augmented + np.mean(x_augmented)\n",
    "        y_mirror = np.mean(y_augmented) - y_augmented + np.mean(y_augmented)\n",
    "    \n",
    "        x_result = np.vstack((x_aug_clear, x_mirror))\n",
    "        y_result = np.hstack((y_aug_clear, y_mirror))\n",
    "    else:\n",
    "        x_result = x_aug_clear\n",
    "        y_result = y_aug_clear\n",
    "    \n",
    "    if mirror:\n",
    "        ...\n",
    "        plt.plot(y_points, y_data, 'o', y_points_n, y_mirror)\n",
    "        plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_mirror[:, 0])\n",
    "\n",
    "    \n",
    "    plt.plot(y_points, y_data, 'o', y_points_n, y_augmented)\n",
    "    plt.plot(x_points, x_data[:, 0], 'x', x_points_n, x_augmented[:, 0])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    points_aug_clear = np.linspace(0, y_points, len(y_aug_clear))\n",
    "\n",
    "    \n",
    "    \n",
    "    # plt.plot(y_points, y_data, 'o', points_aug_clear, y_aug_clear)\n",
    "    # plt.plot(x_points, x_data[:, 0], 'x', points_aug_clear, x_aug_clear[:, 0])\n",
    "    # plt.show()\n",
    "\n",
    "    x_result[x_result < 0] = 0\n",
    "    y_result[y_result < 0] = 0\n",
    "    \n",
    "    return x_result, y_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519297c-5520-4427-972f-4f3acda51145",
   "metadata": {},
   "source": [
    "#### Функция получения трансформеров входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d17b776-77e2-4677-a8cb-40e33404248d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_transformer(transformer, n_samples=10_000):\n",
    "    scaler = (\n",
    "        StandardScaler() if transformer == 'standard' else \\\n",
    "        MinMaxScaler() if transformer == 'minmax' else \\\n",
    "        MaxAbsScaler() if transformer == 'maxabs' else \\\n",
    "        RobustScaler() if transformer == 'robust' else \\\n",
    "        QuantileTransformer(output_distribution='uniform', n_quantiles=n_samples, random_state=0) if transformer == 'uniform' else \\\n",
    "        QuantileTransformer(output_distribution='normal', n_quantiles=n_samples, random_state=0) if transformer == 'normal' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=False) if transformer == 'normal-bc' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=False) if transformer == 'normal-yj' else \\\n",
    "        PowerTransformer(method='box-cox', standardize=True) if transformer == 'normal-bc-st' else \\\n",
    "        PowerTransformer(method='yeo-johnson', standardize=True) if transformer == 'normal-yj-st' else \\\n",
    "        None\n",
    "    )\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ad14e-9c64-40cd-a9e2-c1ac61c65a93",
   "metadata": {},
   "source": [
    "#### Функция получения списка моделей регрессоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d36d9b6c-03df-45c0-891a-6633a80265b2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_regressors_list():\n",
    "    names = [\n",
    "        # 'LinearRegression',\n",
    "        \n",
    "        # 'Ridge',\n",
    "        # 'RidgeCV',\n",
    "        \n",
    "        # 'ElasticNetCV',\n",
    "        \n",
    "        # 'LassoCV',\n",
    "\n",
    "        # 'LarsCV',\n",
    "        \n",
    "        # 'Lars1',\n",
    "        # 'Lars2',\n",
    "        # 'Lars3',\n",
    "        # 'Lars4',\n",
    "        # 'Lars5',\n",
    "        # 'Lars6',\n",
    "        # 'Lars7',\n",
    "        # 'Lars8',\n",
    "\n",
    "        # 'LassoLarsCV',\n",
    "        \n",
    "        # 'OMPCV',\n",
    "        # 'OMP1',\n",
    "        # 'OMP2',\n",
    "        # 'OMP3',\n",
    "        # 'OMP4',\n",
    "        # 'OMP5',\n",
    "        # 'OMP6',\n",
    "        # 'OMP7',\n",
    "        # 'OMP8',\n",
    "        \n",
    "        # 'BayesianRidge',\n",
    "        # 'BayesianRidgeCV',\n",
    "        # 'ARDRegression',\n",
    "        #'ARDRegressionCV',\n",
    "        # 'SGDRegressor', \n",
    "        # 'PassiveAggressiveRegressor',\n",
    "        # # 'HuberRegressor',\n",
    "        # 'HuberRegressorCV',\n",
    "        # 'TheilSenRegressor',\n",
    "        # # 'TheilSenRegressorCV',\n",
    "        # 'QuantileRegressor',\n",
    "        # # 'QuantileRegressorCV',\n",
    "        \n",
    "        \n",
    "        'KNeighborsRegressor',\n",
    "        # # # 'NuSVR',\n",
    "        # # # 'SVR',\n",
    "        # # # 'MLPRegressor',\n",
    "        \n",
    "        'RandomForestRegressor',\n",
    "        'ExtraTreesRegressor',\n",
    "        'HistGradientBoostingRegressor',\n",
    "        'BaggingRegressor',\n",
    "        'VotingRegressor',\n",
    "        # 'StackingRegressorRidge',\n",
    "        'AdaBoostRegressor',\n",
    "    ]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad1b1f-6076-45b5-b9f1-689dd88981bf",
   "metadata": {},
   "source": [
    "#### Функция получения объектов моделей регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f061ca6e-ca23-41aa-9f72-add0e4a9cd24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_regressors_objects():\n",
    "    # Инициализация генератора случайных чисел для\n",
    "    # для обеспечения воспроизводимости результатов\n",
    "    rng = np.random.RandomState(0)\n",
    "\n",
    "    # Наборы гиперпараметров моделей для алгоритма кроссвалидации\n",
    "    # Гиперпараметры для Ridge, Lasso, ElasticNet, LassoLars, HuberRegressor\n",
    "    alphas = np.logspace(-4, 3, num=100)\n",
    "    \n",
    "    # Гиперпараметры для ElasticNet\n",
    "    l1_ratio = np.linspace(0.01, 1.0, num=50)\n",
    "    \n",
    "    # Гиперпараметры для BayesianRidge\n",
    "    alphas_init = np.linspace(0.5, 2, 5)\n",
    "    lambdas_init = np.logspace(-3, 1, num=5)\n",
    "    \n",
    "    # Гиперпараметры для ARDRegression\n",
    "    alphas_lambdas = np.logspace(-7, -4, num=4)\n",
    "    \n",
    "    # Гиперпараметры для SGDRegressor\n",
    "    losses = ['squared_error', 'huber', \n",
    "              'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "    sgd_alphas = np.logspace(-4, 1, num=100)\n",
    "   \n",
    "    # Гиперпараметры для PassiveAggressiveRegressor\n",
    "    cc = np.linspace(0.1, 1.5, 50)\n",
    "    \n",
    "    # Гиперпараметры для HuberRegressor\n",
    "    epsilons = np.append(np.linspace(1.1, 2.0, 10), [1.35])\n",
    "    \n",
    "    # Гиперпараметры для TheilSenRegressor\n",
    "    # n_subsamples = np.arange(15, 24)\n",
    "    n_subsamples = (16, 24, 32)\n",
    "    \n",
    "    # Гиперпараметры для QuantileRegressor\n",
    "    # q_alphas = np.linspace(0, 1, 5)\n",
    "    q_alphas = (0.1, 1, 2)    \n",
    "    \n",
    "    regressors = [\n",
    "        # LinearRegression(),\n",
    "        \n",
    "        # Ridge(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=Ridge(random_state=0), \n",
    "        #     param_grid={\"alpha\": alphas}\n",
    "        # ),\n",
    "        # RidgeCV(),\n",
    "\n",
    "        # ElasticNetCV(random_state=0),\n",
    "        \n",
    "        # LassoCV(max_iter=10000, n_alphas=300, random_state=0),  \n",
    "        \n",
    "        # LarsCV(),\n",
    "        \n",
    "        # Lars(n_nonzero_coefs=1, random_state=0),\n",
    "        # Lars(n_nonzero_coefs=2, random_state=0),\n",
    "        # Lars(n_nonzero_coefs=3, random_state=0),\n",
    "        # Lars(n_nonzero_coefs=4, random_state=0),\n",
    "        # Lars(n_nonzero_coefs=5, random_state=0),\n",
    "        # Lars(n_nonzero_coefs=6, random_state=0),\n",
    "        # Lars(n_nonzero_coefs=7, random_state=0),\n",
    "        # Lars(n_nonzero_coefs=8, random_state=0),\n",
    "\n",
    "        # LassoLarsCV(max_iter=500, max_n_alphas=1000),\n",
    "\n",
    "        # OrthogonalMatchingPursuitCV(n_jobs=-1),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=1),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=2),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=3),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=4),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=5),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=6),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=7),\n",
    "        # OrthogonalMatchingPursuit(n_nonzero_coefs=8),\n",
    "        \n",
    "        # BayesianRidge(),\n",
    "        # BayesianRidge() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=BayesianRidge(),\n",
    "        #     param_grid={\"alpha_init\": alphas_init, \"lambda_init\": lambdas_init}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # ARDRegression(),\n",
    "        # ARDRegression() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=ARDRegression(), \n",
    "        #     param_grid={\"alpha_1\": alphas_lambdas, \"alpha_2\": alphas_lambdas,\n",
    "        #                 \"lambda_1\": alphas_lambdas,\"lambda_2\": alphas_lambdas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # SGDRegressor(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=SGDRegressor(random_state=0), \n",
    "        #     param_grid={\"loss\": losses, \"alpha\": sgd_alphas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # PassiveAggressiveRegressor(random_state=0) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=PassiveAggressiveRegressor(random_state=0), \n",
    "        #     param_grid={\"C\": cc}, \n",
    "        #     n_jobs=-1, \n",
    "        #     cv=3\n",
    "        # ),\n",
    "\n",
    "        # # HuberRegressor(max_iter=1000),\n",
    "        # HuberRegressor(max_iter=1000) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=HuberRegressor(), \n",
    "        #     param_grid={\"epsilon\": epsilons, \"alpha\": alphas}, \n",
    "        #     n_jobs=-1 \n",
    "        # ),\n",
    "\n",
    "        # TheilSenRegressor(random_state=0, n_jobs=-1),\n",
    "        # TheilSenRegressor(random_state=0, n_jobs=-1) if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=TheilSenRegressor(random_state=0, n_jobs=-1), \n",
    "        #     param_grid={\"n_subsamples\": n_subsamples}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "\n",
    "        # QuantileRegressor(),\n",
    "        # QuantileRegressor() if not grid_search else \\\n",
    "        # GridSearchCV(\n",
    "        #     estimator=QuantileRegressor(), \n",
    "        #     param_grid={\"alpha\": q_alphas}, \n",
    "        #     n_jobs=-1\n",
    "        # ),\n",
    "        \n",
    "        \n",
    "        \n",
    "        KNeighborsRegressor(n_neighbors=10, metric='euclidean'),\n",
    "        # # NuSVR(C=5.0, nu=0.9, kernel='poly', degree=3),\n",
    "        # # SVR(C=5.0, epsilon=0.2, kernel='poly', degree=3),\n",
    "        \n",
    "        \n",
    "        # # MLPRegressor(\n",
    "        # #     hidden_layer_sizes=(3, ), \n",
    "        # #     activation='identity', \n",
    "        # #     max_iter=100000, \n",
    "        # #     early_stopping=True, \n",
    "        # #     learning_rate='constant',\n",
    "        # #     learning_rate_init=0.00025,\n",
    "        # #     batch_size=75,\n",
    "        # #     solver='adam',\n",
    "        # #     random_state=0\n",
    "        # # ),\n",
    "       \n",
    "        \n",
    "        \n",
    "        RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0),\n",
    "        ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0),\n",
    "        HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0),\n",
    "        BaggingRegressor(\n",
    "            #KNeighborsRegressor(n_neighbors=20, metric='euclidean'),\n",
    "            estimator=ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0), \n",
    "            max_samples=0.75, max_features=0.75, n_estimators=10, random_state=0\n",
    "        ),\n",
    "\n",
    "        VotingRegressor(\n",
    "            estimators=[\n",
    "                ('hgbr', HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0)), \n",
    "                ('omp', ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0)), \n",
    "                ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean')),\n",
    "                ('rfr', RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)),\n",
    "            ]\n",
    "        ),\n",
    "\n",
    "\n",
    "        # StackingRegressor( # RidgeCV - final estimator\n",
    "        #     estimators=[\n",
    "        #         ('knr', KNeighborsRegressor(n_neighbors=10, metric='euclidean')),\n",
    "        #         ('rfr', RandomForestRegressor(n_estimators=100, criterion='absolute_error', random_state=0)),\n",
    "        #         ('hgbr', HistGradientBoostingRegressor(max_iter=100, loss='absolute_error', max_leaf_nodes=None, min_samples_leaf=10, random_state=0)), \n",
    "        #         ('etr', ExtraTreesRegressor(n_estimators=100, criterion='squared_error', random_state=0)),\n",
    "        #         ('omp', OrthogonalMatchingPursuit(n_nonzero_coefs=5)),\n",
    "        #     ],\n",
    "        # ),\n",
    "\n",
    "        AdaBoostRegressor(estimator=KNeighborsRegressor(n_neighbors=5, metric='euclidean'), n_estimators=100, loss='linear', random_state=0),\n",
    "        \n",
    "    ]\n",
    "    return regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08312297-9ca3-4e7d-be60-26969a4611e7",
   "metadata": {},
   "source": [
    "#### Функция обучения моделей на полном наборе данных с записью на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dff9414-8a26-4393-b480-446307aafc2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train_models(year, pr_group, n_test=None, norms=True, aug_n=0, aug_pow=2, aug_mirror=False, grid_search=False, scaler_x=None, scaler_y=None, shuffle=True, serial=True, top_best=None):\n",
    "    \n",
    "    ds_dir = f'data/{year}/Train'\n",
    "    \n",
    "    \n",
    "\n",
    "    datasets = get_datasets()\n",
    "\n",
    "    fieldnames = [\n",
    "        'Predictors', \n",
    "        'Equations', \n",
    "        'Method', \n",
    "        'Criterion', \n",
    "        'Correlation', \n",
    "        'Pm',\n",
    "        'R2',\n",
    "\n",
    "        'Criterion_t', \n",
    "        'Correlation_t', \n",
    "        'Pm_t',\n",
    "        'R2_t',\n",
    "\n",
    "        'Criterion_f', \n",
    "        'Correlation_f', \n",
    "        'Pm_f',\n",
    "        'R2_f',\n",
    "\n",
    "        # 'Group',\n",
    "        # 'Augmentation',\n",
    "        # 'Data size',\n",
    "        # 'Normalization',\n",
    "        # 'Equations',\n",
    "    ]\n",
    "\n",
    "    # Описание структуры данных переменной datasets_result\n",
    "    # datasets_result = {\n",
    "    #     \"hydropost_0\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    #     ...,\n",
    "    #     \"hydropost_n\": [\n",
    "    #         { model_row }\n",
    "    #         { model_row }\n",
    "    #     ],\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    # Итерация по датасетам\n",
    "    datasets_result = dict()\n",
    "    for ds in datasets:\n",
    "\n",
    "        # Получить все группы по датасету\n",
    "        ds_groups = get_predictors(ds)\n",
    "        \n",
    "        # Итерация по группам предикторов\n",
    "        for group, pr_list in enumerate(ds_groups):\n",
    "\n",
    "            if pr_group is not None:\n",
    "                if group != pr_group:\n",
    "                    continue\n",
    "        \n",
    "            result_list = []\n",
    "            \n",
    "            X, y = get_river_dataset(f'{ds_dir}/{ds}.csv', pr_list=pr_list)\n",
    "    \n",
    "            # Проверочный набор данных (исходный)\n",
    "            X_prior = X.copy()\n",
    "            y_prior = y.copy()\n",
    "            \n",
    "            # Полный набор данных\n",
    "            X_full = X.copy()\n",
    "            y_full = y.copy()\n",
    "\n",
    "            if aug_n:\n",
    "                X_full, y_full = augment_data(X_full, y_full, aug_n, aug_pow=aug_pow, mirror=aug_mirror)\n",
    "            \n",
    "            if shuffle:\n",
    "                X_full, y_full = shuffle_xy(X_full, y_full, shuffle=True)\n",
    "            \n",
    "            if n_test:\n",
    "                X_train, y_train, X_test, y_test = train_test_split(X_full, y_full, n_test, split=True)\n",
    "    \n",
    "            # print(\"SHAPES:\")\n",
    "            # print(\"X_train.shape, y_train.shape\", X_train.shape, y_train.shape)\n",
    "            # print(\"X_test.shape, y_test.shape\", X_test.shape, y_test.shape)\n",
    "            \n",
    "            norms_data = None\n",
    "            if norms:\n",
    "                norms_data = get_norms(ds)\n",
    "                # Подстановка норм в исходный набор данных (пессимистичный сценарий)\n",
    "                X_prior = test_norm(X_prior, pr_list, norms_data)\n",
    "                # Подстановка норм в тестовый набор данных\n",
    "                X_test = test_norm(X_test, pr_list, norms_data)\n",
    "                # Подстановка норм в полный набор данных не требуется\n",
    "  \n",
    "            # print(\"X_test:\")\n",
    "            # print(X_test)\n",
    "            # print(\"X_train:\")\n",
    "            # print(X_train)\n",
    "\n",
    "            transformer_y = get_transformer(scaler_y, n_samples=y_train.shape[0]) # !!!\n",
    "            transformer_x = get_transformer(scaler_x, n_samples=y_train.shape[0]) # !!!\n",
    "            transformer_y_full = get_transformer(scaler_y, n_samples=y_full.shape[0]) # !!!\n",
    "            transformer_x_full = get_transformer(scaler_x, n_samples=y_full.shape[0]) # !!!\n",
    "            print(transformer_y)\n",
    "            print(transformer_x)\n",
    "\n",
    "            # Список оцениваемых ререссионных моделей !!!!!\n",
    "            names = get_regressors_list()\n",
    "            regressors = get_regressors_objects()\n",
    "            regressors_full = get_regressors_objects()\n",
    "                \n",
    "            # Итерация по моделям регрессии\n",
    "            for name, model, model_full in zip(names, regressors, regressors_full):\n",
    "\n",
    "                # Препроцессинг - трансформация целевых значений y\n",
    "                if scaler_y: \n",
    "                    regressor = TransformedTargetRegressor(regressor=model, transformer=transformer_y)\n",
    "                    regressor_full = TransformedTargetRegressor(regressor=model_full, transformer=transformer_y_full)\n",
    "                else:\n",
    "                    regressor = model\n",
    "                    regressor_full = model_full\n",
    "                \n",
    "                one_model_row = dict()\n",
    "                print('X_full.shape', X_full.shape)\n",
    "                print('y_full.shape', y_full.shape)\n",
    "                print('X_train.shape', X_train.shape)\n",
    "                print('y_train.shape', y_train.shape)\n",
    "                print('X_test.shape', X_test.shape)\n",
    "                print('y_test.shape', y_test.shape)\n",
    "                # n_samples = min(10000, y_train.shape[0])\n",
    "                \n",
    "                \n",
    "                # Препроцессинг - трансформация признаков X\n",
    "                regr = make_pipeline(transformer_x, regressor) if transformer_x else regressor\n",
    "                regr_full = make_pipeline(transformer_x_full, regressor_full) if transformer_x_full else regressor_full\n",
    "                \n",
    "                # Обучение на тренировочном наборе\n",
    "                regr = regr.fit(X_train, y_train)\n",
    "                try:\n",
    "                    regr.fit(X_train, y_train)\n",
    "                except ValueError as error:\n",
    "                    print('Ошибка обучения на тренировочном наборе данных:')\n",
    "                    print(error)\n",
    "                    continue\n",
    "                if serial:\n",
    "                    serial_model = pickle.dumps(regr)\n",
    "                    regr = pickle.loads(serial_model)\n",
    "                \n",
    "                # Обученная на тренировочном наборе данных модель\n",
    "                one_model_row['Model_train'] = regr\n",
    "                    \n",
    "                # Прогноз по исходному набору на тренировочной модели\n",
    "                y_predicted_prior = np.ravel(regr.predict(X_prior))\n",
    "                \n",
    "                # Прогноз по тестовому набору на тренировочной модели \n",
    "                y_predicted_test = np.ravel(regr.predict(X_test))\n",
    "    \n",
    "                \n",
    "                # Очистка значений строк предикторов и уравнений перед переходом к следующей модели\n",
    "                coef = None\n",
    "                intercept = None\n",
    "                \n",
    "                try:\n",
    "                    coef = regr.best_estimator_.coef_\n",
    "                    intercept = regr.best_estimator_.intercept_\n",
    "                    \n",
    "                    if isinstance(intercept, np.ndarray):\n",
    "                        intercept = intercept[0]\n",
    "                except Exception as error:\n",
    "                                    \n",
    "                    try:\n",
    "                        coef = regr.coef_\n",
    "                        intercept = regr.intercept_\n",
    "                    \n",
    "                        if isinstance(intercept, np.ndarray):\n",
    "                            intercept = intercept[0]\n",
    "                        print(\"ERROR1 START\")\n",
    "                        print(error)\n",
    "                        print(\"ERROR1 FINISH\")\n",
    "                    except Exception as error:\n",
    "                        print(\"ERROR2 START\")\n",
    "                        print(error)\n",
    "                        print(\"ERROR2 FINISH\")\n",
    "                    \n",
    "                \n",
    "                try:\n",
    "                    # Коэффициенты уравнения (если есть)\n",
    "                    coef = np.around(np.ravel(coef), 3)\n",
    "                    intercept = round(intercept, 3)\n",
    "                    \n",
    "                    predictors_coef = {f: c for f, c \n",
    "                                       in zip(pr_list, coef) if c != 0.0}\n",
    "                    \n",
    "                    predictors = \", \".join(predictors_coef.keys())\n",
    "                    \n",
    "                    equation = (\n",
    "                        str(intercept) \n",
    "                        + ' ' \n",
    "                        + ' '.join(str(c) + '*' \n",
    "                                   + f for f, c in predictors_coef.items())\n",
    "                    )\n",
    "                    \n",
    "                    equation = equation.replace(\" -\", \"-\")\n",
    "                    equation = equation.replace(\" \", \" + \")\n",
    "                    equation = equation.replace(\"-\", \" - \")\n",
    "        \n",
    "                    one_model_row['Predictors'] = predictors\n",
    "                    one_model_row['Equations'] = equation\n",
    "                except Exception as error:\n",
    "                    print(\"ERROR3 START\")\n",
    "                    print(error)\n",
    "                    print(\"ERROR3 FINISH\")\n",
    "                    one_model_row['Predictors'] = \"\"\n",
    "                    one_model_row['Equations'] = \"\"\n",
    "    \n",
    "                # Год прогноза\n",
    "                one_model_row['Forecast_year'] = year\n",
    "                    \n",
    "                # Название датасета\n",
    "                one_model_row['Dataset_name'] = ds\n",
    "\n",
    "                # Название датасета\n",
    "                one_model_row['Predictors_list'] = pr_list\n",
    "\n",
    "                # Нормы\n",
    "                one_model_row['Norms_data'] = norms_data\n",
    "    \n",
    "                # Группа предикторов\n",
    "                one_model_row['Group'] = group\n",
    "                    \n",
    "                # Название метода\n",
    "                one_model_row['Method'] = name\n",
    "                \n",
    "                # Среднеквадратическое отклонение\n",
    "                # по исходному набору:\n",
    "                sigma = get_sigma(y_prior) #!!!\n",
    "                one_model_row['Sigma'] = sigma\n",
    "                # по тестовому набору:\n",
    "                sigma_t = get_sigma(y_test) #!!!\n",
    "                one_model_row['Sigma_t'] = sigma_t\n",
    "                \n",
    "                # Допустимая погрешность прогноза\n",
    "                # по исходному набору:\n",
    "                delta_dop = get_delta_dop(sigma) #!!!\n",
    "                one_model_row['Delta_dop'] = delta_dop\n",
    "                # по тестовому набору:\n",
    "                delta_dop_t = get_delta_dop(sigma_t) #!!!\n",
    "                one_model_row['Delta_dop_t'] = delta_dop_t\n",
    "    \n",
    "                # Обеспеченность метода (оправдываемость) Pm\n",
    "                # по исходному набору:\n",
    "                pm = get_pm(y_prior, y_predicted_prior, delta_dop) #!!!\n",
    "                one_model_row['Pm'] = pm\n",
    "                # по тестовому набору:\n",
    "                pm_t = get_pm(y_test, y_predicted_test, delta_dop_t) #!!!\n",
    "                one_model_row['Pm_t'] = pm_t\n",
    "    \n",
    "                # Среднеквадратическая погрешность прогноза\n",
    "                # по исходному набору:\n",
    "                s_forecast = get_s(y_prior, y_predicted_prior) #!!!\n",
    "                one_model_row['S'] = s_forecast\n",
    "                # по тестовому набору:\n",
    "                s_forecast_t = get_s(y_test, y_predicted_test) #!!!\n",
    "                one_model_row['S_t'] = s_forecast_t\n",
    "                \n",
    "                # Критерий эффективности метода прогнозирования \n",
    "                # климатический S/sigma\n",
    "                # по исходному набору:\n",
    "                criterion_forecast = get_criterion(s_forecast, sigma) #!!!\n",
    "                one_model_row['Criterion'] = criterion_forecast\n",
    "                # по тестовому набору:\n",
    "                criterion_forecast_t = get_criterion(s_forecast_t, sigma_t) #!!!\n",
    "                one_model_row['Criterion_t'] = criterion_forecast_t\n",
    "    \n",
    "                \n",
    "                # Корреляционное отношение ro\n",
    "                # по исходному набору:\n",
    "                correlation_forecast = get_correlation_ratio(criterion_forecast)\n",
    "                one_model_row['Correlation'] = correlation_forecast\n",
    "                # по тестовому набору:\n",
    "                correlation_forecast_t = get_correlation_ratio(criterion_forecast_t)\n",
    "                one_model_row['Correlation_t'] = correlation_forecast_t\n",
    "                \n",
    "                \n",
    "                # Коэффициент детерминации R2\n",
    "                # по исходному набору:\n",
    "                one_model_row['R2'] = regr.score(X_prior, y_prior)\n",
    "                # по тестовому набору:\n",
    "                one_model_row['R2_t'] = regr.score(X_test, y_test)\n",
    "                \n",
    "    \n",
    "                # Обучение на полном наборе данных\n",
    "                try:\n",
    "                    regr_full = regr_full.fit(X_full, y_full)\n",
    "                except ValueError as error:\n",
    "                    print('Ошибка обучения на полном наборе данных:')\n",
    "                    print(error)\n",
    "                    continue\n",
    "                if serial:\n",
    "                    serial_model_full = pickle.dumps(regr_full)\n",
    "                    regr_full = pickle.loads(serial_model_full)\n",
    "\n",
    "                # Обученная на полных данных модель\n",
    "                one_model_row['Model_full'] = regr_full\n",
    "    \n",
    "                # Прогноз по полному набору (производится на \"тестовых\" данных) \n",
    "                y_predicted_full = np.ravel(regr_full.predict(X_test))         \n",
    "    \n",
    "                # по полному набору:\n",
    "                sigma_f = get_sigma(y_test) #!!!\n",
    "                one_model_row['Sigma_f'] = sigma_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                delta_dop_f = get_delta_dop(sigma_f) #!!!\n",
    "                one_model_row['Delta_dop_f'] = delta_dop_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                pm_f = get_pm(y_test, y_predicted_full, delta_dop_f) #!!!\n",
    "                one_model_row['Pm_f'] = pm_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                s_forecast_f = get_s(y_test, y_predicted_full) #!!!\n",
    "                one_model_row['S_f'] = s_forecast_f\n",
    "    \n",
    "                # по полному набору:\n",
    "                criterion_forecast_f = get_criterion(s_forecast_f, sigma_f) #!!!\n",
    "                one_model_row['Criterion_f'] = criterion_forecast_f\n",
    "               \n",
    "                # по полному набору:\n",
    "                correlation_forecast_f = get_correlation_ratio(criterion_forecast_f)\n",
    "                one_model_row['Correlation_f'] = correlation_forecast_f\n",
    "                \n",
    "                # Коэффициент детерминации R2\n",
    "                # по полному набору:\n",
    "                one_model_row['R2_f'] = regr_full.score(X_test, y_test)\n",
    "    \n",
    "                print(one_model_row)\n",
    "    \n",
    "                \n",
    "                # Добавление результатов модели в результирующий список по датасету\n",
    "                result_list.append(one_model_row)\n",
    "    \n",
    "                # Запись сериализованного объекта {модель, статистика} в файл\n",
    "                write_model(one_model_row)\n",
    "\n",
    "                \n",
    "                #----------------------------------------------------------------------------------------------\n",
    "                smodel = pickle.dumps(one_model_row)\n",
    "                # with open(f'results/Models/{year}/Вилия-Стешицы_2024_гр0_OMP7.pickle', 'rb') as f:\n",
    "                #     model_info = pickle.load(f, encoding=\"latin1\")\n",
    "                model_info = pickle.loads(smodel)\n",
    "                model_full = model_info['Model_full']\n",
    "                # model_train = model_info['Model_train']\n",
    "                # Прогноз по исходному набору\n",
    "                pickled_y_predicted_prior = np.ravel(model_full.predict(X_prior))\n",
    "\n",
    "                print('------------------------------------------------------------------------------')\n",
    "                print('predicted_prior train model')\n",
    "                print(y_predicted_prior)\n",
    "                \n",
    "                \n",
    "                print(name)\n",
    "                print(y_prior)\n",
    "                print('predicted_prior full model')\n",
    "                print(pickled_y_predicted_prior)\n",
    "                print('------------------------------------------------------------------------------')\n",
    "                #----------------------------------------------------------------------------------------------\n",
    "                # Конец итерации по модели\n",
    "    \n",
    "            # Сортировка результатов по каждому датасету\n",
    "            result_list.sort(\n",
    "                key=lambda row: (row['Criterion'], \n",
    "                                 -row['Correlation'], \n",
    "                                 -row['Pm'])\n",
    "            )\n",
    "    \n",
    "            datasets_result[ds] = result_list\n",
    "    \n",
    "            # Запись в .csv файл\n",
    "            write_dataset_csv(year, result_list, ds, fieldnames, pr_group=group, mode='training')\n",
    "        \n",
    "            # Конец итерации по группе\n",
    "        \n",
    "        # Конец итерации по датасету\n",
    "       \n",
    "    return datasets_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b9338-086a-4d3b-aff9-c583952daeac",
   "metadata": {},
   "source": [
    "### Запись сериализованного объекта {модель, статистика} в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2375fa5b-c336-4244-89f3-af0612ebf93f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def write_model(model_row):\n",
    "    dir_path = f'results/Models/{model_row[\"Forecast_year\"]}'\n",
    "    \n",
    "    with open(f'{dir_path}/'\n",
    "              f'{model_row[\"Dataset_name\"]}_'\n",
    "              f'{model_row[\"Forecast_year\"]}_'\n",
    "              f'гр{model_row[\"Group\"]}_'\n",
    "              f'{model_row[\"Method\"]}.pickle', 'wb') as pf:\n",
    "        pickle.dump(model_row, pf) #, pickle.HIGHEST_PROTOCOL\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "62ef4a5d-540a-417d-9ef9-9b37afd6dfd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Поля в таблице сравнения:\n",
    "\n",
    "Группа предикторов (название группы)\n",
    "Расширение данных (вид расширения -линейная, нелинейная интерпол)\n",
    "Размер данных (количество точек)\n",
    "!Нелинейное преобразование входных данных (степень полинома)\n",
    "Нормализация данных (вид нормализации)\n",
    "Уравнение\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae6e8e-07c7-4de1-b8cc-0ae599007b06",
   "metadata": {},
   "source": [
    "#### Запуск процесса обучения моделей множественной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df2181-e85a-449e-bbd2-41e64908cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_models(2024, pr_group=0, n_test=100, norms=True, aug_n=1000, aug_pow=2, \n",
    "                 aug_mirror=False, grid_search=True, scaler_x=None, scaler_y=None, \n",
    "                 shuffle=True, serial=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
